Epoch 1/150
 73/122 [================>.............] - ETA: 0s - loss: 0.0352 - mae: 0.0547 - mse: 0.0075
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 9ms/step - loss: 0.0221 - mae: 0.0482 - mse: 0.0053 - val_loss: 0.0022 - val_mae: 0.0379 - val_mse: 0.0019 - lr: 0.0947
Epoch 2/150
122/122 [==============================] - 1s 6ms/step - loss: 0.0022 - mae: 0.0374 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0366 - val_mse: 0.0018 - lr: 0.0947
Epoch 3/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0022 - mae: 0.0367 - mse: 0.0019 - val_loss: 0.0022 - val_mae: 0.0355 - val_mse: 0.0018 - lr: 0.0947
Epoch 4/150
 80/122 [==================>...........] - ETA: 0s - loss: 0.0020 - mae: 0.0349 - mse: 0.0018
122/122 [==============================] - 1s 6ms/step - loss: 0.0019 - mae: 0.0342 - mse: 0.0017 - val_loss: 0.0016 - val_mae: 0.0317 - val_mse: 0.0015 - lr: 0.0947
Epoch 5/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0017 - mae: 0.0316 - mse: 0.0015 - val_loss: 0.0017 - val_mae: 0.0306 - val_mse: 0.0014 - lr: 0.0947
Epoch 6/150
122/122 [==============================] - 0s 983us/step - loss: 0.0015 - mae: 0.0286 - mse: 0.0013 - val_loss: 0.0017 - val_mae: 0.0307 - val_mse: 0.0014 - lr: 0.0947
Epoch 7/150
 83/122 [===================>..........] - ETA: 0s - loss: 0.0014 - mae: 0.0267 - mse: 0.0011
122/122 [==============================] - 1s 6ms/step - loss: 0.0014 - mae: 0.0267 - mse: 0.0011 - val_loss: 0.0015 - val_mae: 0.0289 - val_mse: 0.0013 - lr: 0.0947
Epoch 8/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0014 - mae: 0.0263 - mse: 0.0011 - val_loss: 0.0016 - val_mae: 0.0296 - val_mse: 0.0013 - lr: 0.0947
Epoch 9/150
 84/122 [===================>..........] - ETA: 0s - loss: 0.0013 - mae: 0.0259 - mse: 0.0011
122/122 [==============================] - 1s 6ms/step - loss: 0.0014 - mae: 0.0262 - mse: 0.0011 - val_loss: 0.0013 - val_mae: 0.0257 - val_mse: 0.0011 - lr: 0.0947
Epoch 10/150
 53/122 [============>.................] - ETA: 0s - loss: 0.0016 - mae: 0.0282 - mse: 0.0013
122/122 [==============================] - 1s 6ms/step - loss: 0.0015 - mae: 0.0269 - mse: 0.0012 - val_loss: 0.0012 - val_mae: 0.0244 - val_mse: 9.7307e-04 - lr: 0.0947
Epoch 11/150
 76/122 [=================>............] - ETA: 0s - loss: 0.0013 - mae: 0.0250 - mse: 0.0010
122/122 [==============================] - 1s 6ms/step - loss: 0.0013 - mae: 0.0251 - mse: 0.0010 - val_loss: 0.0011 - val_mae: 0.0228 - val_mse: 8.4886e-04 - lr: 0.0947
Epoch 12/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0012 - mae: 0.0248 - mse: 0.0010 - val_loss: 0.0014 - val_mae: 0.0269 - val_mse: 0.0011 - lr: 0.0947
Epoch 13/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0013 - mae: 0.0255 - mse: 0.0010 - val_loss: 0.0023 - val_mae: 0.0370 - val_mse: 0.0020 - lr: 0.0947
Epoch 14/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0012 - mae: 0.0245 - mse: 9.7087e-04 - val_loss: 0.0011 - val_mae: 0.0230 - val_mse: 8.6136e-04 - lr: 0.0947
Epoch 15/150
 72/122 [================>.............] - ETA: 0s - loss: 0.0011 - mae: 0.0237 - mse: 9.0348e-04
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0011 - mae: 0.0237 - mse: 9.1361e-04 - val_loss: 0.0010 - val_mae: 0.0223 - val_mse: 8.1435e-04 - lr: 0.0947
Epoch 16/150
 75/122 [=================>............] - ETA: 0s - loss: 0.0012 - mae: 0.0243 - mse: 9.6735e-04
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.047330185770988464.
122/122 [==============================] - 0s 1ms/step - loss: 0.0011 - mae: 0.0239 - mse: 9.4661e-04 - val_loss: 0.0014 - val_mae: 0.0274 - val_mse: 0.0012 - lr: 0.0947
Epoch 17/150
122/122 [==============================] - 1s 6ms/step - loss: 0.0010 - mae: 0.0231 - mse: 8.8230e-04 - val_loss: 9.5550e-04 - val_mae: 0.0227 - val_mse: 8.4122e-04 - lr: 0.0473
Epoch 18/150
122/122 [==============================] - 0s 1ms/step - loss: 9.9066e-04 - mae: 0.0227 - mse: 8.5107e-04 - val_loss: 9.6454e-04 - val_mae: 0.0227 - val_mse: 8.4137e-04 - lr: 0.0473
Epoch 19/150
122/122 [==============================] - 0s 1ms/step - loss: 9.8522e-04 - mae: 0.0228 - mse: 8.4405e-04 - val_loss: 9.7916e-04 - val_mae: 0.0231 - val_mse: 8.6579e-04 - lr: 0.0473
Epoch 20/150
122/122 [==============================] - 0s 978us/step - loss: 0.0011 - mae: 0.0239 - mse: 9.2251e-04 - val_loss: 0.0011 - val_mae: 0.0252 - val_mse: 9.9412e-04 - lr: 0.0473
Epoch 21/150
 81/122 [==================>...........] - ETA: 0s - loss: 9.5440e-04 - mae: 0.0222 - mse: 8.2110e-04
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.023665092885494232.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 9.5196e-04 - mae: 0.0223 - mse: 8.2525e-04 - val_loss: 8.9338e-04 - val_mae: 0.0221 - val_mse: 7.8845e-04 - lr: 0.0473
Epoch 22/150
122/122 [==============================] - 0s 1ms/step - loss: 9.0912e-04 - mae: 0.0220 - mse: 7.9591e-04 - val_loss: 0.0010 - val_mae: 0.0239 - val_mse: 9.1639e-04 - lr: 0.0237
Epoch 23/150
122/122 [==============================] - 0s 1ms/step - loss: 8.8261e-04 - mae: 0.0217 - mse: 7.8057e-04 - val_loss: 9.1529e-04 - val_mae: 0.0222 - val_mse: 8.0516e-04 - lr: 0.0237
Epoch 24/150
 83/122 [===================>..........] - ETA: 0s - loss: 8.8510e-04 - mae: 0.0218 - mse: 7.7902e-04
122/122 [==============================] - 1s 6ms/step - loss: 8.9611e-04 - mae: 0.0219 - mse: 7.9087e-04 - val_loss: 8.7576e-04 - val_mae: 0.0221 - val_mse: 7.7809e-04 - lr: 0.0237
Epoch 25/150
122/122 [==============================] - 0s 1ms/step - loss: 9.0946e-04 - mae: 0.0221 - mse: 8.0153e-04 - val_loss: 0.0012 - val_mae: 0.0263 - val_mse: 0.0011 - lr: 0.0237
Epoch 26/150
 80/122 [==================>...........] - ETA: 0s - loss: 8.7226e-04 - mae: 0.0216 - mse: 7.6654e-04
Epoch 26: ReduceLROnPlateau reducing learning rate to 0.011832546442747116.
122/122 [==============================] - 0s 1ms/step - loss: 8.6985e-04 - mae: 0.0216 - mse: 7.6970e-04 - val_loss: 9.0068e-04 - val_mae: 0.0217 - val_mse: 7.6361e-04 - lr: 0.0237
Epoch 27/150
 82/122 [===================>..........] - ETA: 0s - loss: 8.8894e-04 - mae: 0.0219 - mse: 7.8622e-04
122/122 [==============================] - 1s 8ms/step - loss: 8.8337e-04 - mae: 0.0217 - mse: 7.7903e-04 - val_loss: 8.6177e-04 - val_mae: 0.0221 - val_mse: 7.8009e-04 - lr: 0.0118
Epoch 28/150
122/122 [==============================] - 0s 1ms/step - loss: 8.5292e-04 - mae: 0.0214 - mse: 7.5843e-04 - val_loss: 8.7675e-04 - val_mae: 0.0218 - val_mse: 7.7616e-04 - lr: 0.0118
Epoch 29/150
122/122 [==============================] - 0s 1ms/step - loss: 8.4429e-04 - mae: 0.0214 - mse: 7.5464e-04 - val_loss: 8.7162e-04 - val_mae: 0.0216 - val_mse: 7.6322e-04 - lr: 0.0118
Epoch 30/150
 69/122 [===============>..............] - ETA: 0s - loss: 8.4461e-04 - mae: 0.0212 - mse: 7.5557e-04
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 8.6926e-04 - mae: 0.0216 - mse: 7.7334e-04 - val_loss: 8.6049e-04 - val_mae: 0.0222 - val_mse: 7.8122e-04 - lr: 0.0118
Epoch 31/150
 80/122 [==================>...........] - ETA: 0s - loss: 8.3169e-04 - mae: 0.0212 - mse: 7.4428e-04
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.005916273221373558.
122/122 [==============================] - 0s 990us/step - loss: 8.3633e-04 - mae: 0.0212 - mse: 7.4970e-04 - val_loss: 8.6245e-04 - val_mae: 0.0218 - val_mse: 7.7573e-04 - lr: 0.0118
Epoch 32/150
122/122 [==============================] - 0s 1ms/step - loss: 8.2709e-04 - mae: 0.0211 - mse: 7.4314e-04 - val_loss: 8.6405e-04 - val_mae: 0.0217 - val_mse: 7.7064e-04 - lr: 0.0059
Epoch 33/150
122/122 [==============================] - 0s 1ms/step - loss: 8.3211e-04 - mae: 0.0212 - mse: 7.4319e-04 - val_loss: 8.7392e-04 - val_mae: 0.0219 - val_mse: 7.8786e-04 - lr: 0.0059
Epoch 34/150
122/122 [==============================] - 1s 6ms/step - loss: 8.1597e-04 - mae: 0.0209 - mse: 7.3339e-04 - val_loss: 8.4737e-04 - val_mae: 0.0216 - val_mse: 7.6220e-04 - lr: 0.0059
Epoch 35/150
 81/122 [==================>...........] - ETA: 0s - loss: 8.2866e-04 - mae: 0.0212 - mse: 7.4118e-04
122/122 [==============================] - 1s 6ms/step - loss: 8.2949e-04 - mae: 0.0212 - mse: 7.4306e-04 - val_loss: 8.3687e-04 - val_mae: 0.0216 - val_mse: 7.6212e-04 - lr: 0.0059
Epoch 36/150
 74/122 [=================>............] - ETA: 0s - loss: 8.1596e-04 - mae: 0.0212 - mse: 7.3783e-04
122/122 [==============================] - 1s 6ms/step - loss: 8.3272e-04 - mae: 0.0212 - mse: 7.5043e-04 - val_loss: 8.2380e-04 - val_mae: 0.0212 - val_mse: 7.3093e-04 - lr: 0.0059
Epoch 37/150
122/122 [==============================] - 0s 1ms/step - loss: 8.1171e-04 - mae: 0.0209 - mse: 7.2902e-04 - val_loss: 8.4474e-04 - val_mae: 0.0216 - val_mse: 7.6450e-04 - lr: 0.0059
Epoch 38/150
122/122 [==============================] - 0s 1ms/step - loss: 8.1239e-04 - mae: 0.0209 - mse: 7.3057e-04 - val_loss: 8.7107e-04 - val_mae: 0.0219 - val_mse: 7.8281e-04 - lr: 0.0059
Epoch 39/150
 80/122 [==================>...........] - ETA: 0s - loss: 8.3155e-04 - mae: 0.0212 - mse: 7.4806e-04
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 8.0949e-04 - mae: 0.0209 - mse: 7.2797e-04 - val_loss: 8.1797e-04 - val_mae: 0.0215 - val_mse: 7.4903e-04 - lr: 0.0059
Epoch 40/150
122/122 [==============================] - 0s 1ms/step - loss: 8.0974e-04 - mae: 0.0209 - mse: 7.3416e-04 - val_loss: 9.2277e-04 - val_mae: 0.0226 - val_mse: 8.2899e-04 - lr: 0.0059
Epoch 41/150
 77/122 [=================>............] - ETA: 0s - loss: 8.3199e-04 - mae: 0.0215 - mse: 7.4306e-04
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.002958136610686779.
122/122 [==============================] - 0s 1ms/step - loss: 8.3119e-04 - mae: 0.0213 - mse: 7.4148e-04 - val_loss: 8.9151e-04 - val_mae: 0.0221 - val_mse: 7.9855e-04 - lr: 0.0059
Epoch 42/150
 80/122 [==================>...........] - ETA: 0s - loss: 8.0452e-04 - mae: 0.0208 - mse: 7.2354e-04
122/122 [==============================] - 1s 6ms/step - loss: 8.0377e-04 - mae: 0.0208 - mse: 7.2573e-04 - val_loss: 8.1294e-04 - val_mae: 0.0213 - val_mse: 7.3634e-04 - lr: 0.0030
Epoch 43/150
122/122 [==============================] - 0s 1ms/step - loss: 7.9593e-04 - mae: 0.0207 - mse: 7.1992e-04 - val_loss: 8.7551e-04 - val_mae: 0.0221 - val_mse: 7.9751e-04 - lr: 0.0030
Epoch 44/150
122/122 [==============================] - 0s 1ms/step - loss: 8.0174e-04 - mae: 0.0209 - mse: 7.2573e-04 - val_loss: 8.4515e-04 - val_mae: 0.0216 - val_mse: 7.5867e-04 - lr: 0.0030
Epoch 45/150
122/122 [==============================] - 1s 6ms/step - loss: 7.9971e-04 - mae: 0.0208 - mse: 7.2125e-04 - val_loss: 8.0771e-04 - val_mae: 0.0213 - val_mse: 7.3607e-04 - lr: 0.0030
Epoch 46/150
 60/122 [=============>................] - ETA: 0s - loss: 8.1972e-04 - mae: 0.0212 - mse: 7.4361e-04
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0014790683053433895.
122/122 [==============================] - 0s 1ms/step - loss: 8.0138e-04 - mae: 0.0209 - mse: 7.2459e-04 - val_loss: 8.0814e-04 - val_mae: 0.0211 - val_mse: 7.2240e-04 - lr: 0.0030
Epoch 47/150
122/122 [==============================] - 0s 1ms/step - loss: 7.9285e-04 - mae: 0.0206 - mse: 7.1243e-04 - val_loss: 8.0853e-04 - val_mae: 0.0212 - val_mse: 7.3351e-04 - lr: 0.0015
Epoch 48/150
122/122 [==============================] - 0s 1ms/step - loss: 7.9076e-04 - mae: 0.0207 - mse: 7.1796e-04 - val_loss: 8.0879e-04 - val_mae: 0.0212 - val_mse: 7.3282e-04 - lr: 0.0015
Epoch 49/150
122/122 [==============================] - 0s 1ms/step - loss: 7.8997e-04 - mae: 0.0207 - mse: 7.1677e-04 - val_loss: 8.1069e-04 - val_mae: 0.0213 - val_mse: 7.3643e-04 - lr: 0.0015
Epoch 50/150
 80/122 [==================>...........] - ETA: 0s - loss: 7.9165e-04 - mae: 0.0207 - mse: 7.1770e-04
122/122 [==============================] - 1s 7ms/step - loss: 7.8903e-04 - mae: 0.0207 - mse: 7.1710e-04 - val_loss: 8.0126e-04 - val_mae: 0.0213 - val_mse: 7.3315e-04 - lr: 0.0015
Epoch 51/150
 95/122 [======================>.......] - ETA: 0s - loss: 7.8761e-04 - mae: 0.0207 - mse: 7.1509e-04
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0007395341526716948.
122/122 [==============================] - 1s 7ms/step - loss: 7.8903e-04 - mae: 0.0207 - mse: 7.1710e-04 - val_loss: 8.0126e-04 - val_mae: 0.0213 - val_mse: 7.3315e-04 - lr: 0.0015
122/122 [==============================] - 0s 1ms/step - loss: 7.8118e-04 - mae: 0.0206 - mse: 7.1021e-04 - val_loss: 8.0692e-04 - val_mae: 0.0212 - val_mse: 7.3408e-04 - lr: 3.6977e-0404
Epoch 56/150
 82/122 [===================>..........] - ETA: 0s - loss: 7.6661e-04 - mae: 0.0205 - mse: 6.9590e-04
Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0003697670763358474.
122/122 [==============================] - 0s 982us/step - loss: 7.8209e-04 - mae: 0.0207 - mse: 7.1019e-04 - val_loss: 8.0737e-04 - val_mae: 0.0212 - val_mse: 7.3326e-04 - lr: 7.3953e-04
Epoch 57/150
122/122 [==============================] - 0s 985us/step - loss: 7.8242e-04 - mae: 0.0206 - mse: 7.1008e-04 - val_loss: 8.0269e-04 - val_mae: 0.0212 - val_mse: 7.3013e-04 - lr: 3.6977e-04
Epoch 58/150
122/122 [==============================] - 0s 981us/step - loss: 7.8199e-04 - mae: 0.0206 - mse: 7.1053e-04 - val_loss: 8.0301e-04 - val_mae: 0.0212 - val_mse: 7.3084e-04 - lr: 3.6977e-04
Epoch 59/150
122/122 [==============================] - 0s 998us/step - loss: 7.8213e-04 - mae: 0.0206 - mse: 7.1020e-04 - val_loss: 8.0121e-04 - val_mae: 0.0212 - val_mse: 7.2923e-04 - lr: 3.6977e-04
Epoch 60/150
122/122 [==============================] - 0s 1ms/step - loss: 7.8118e-04 - mae: 0.0206 - mse: 7.1021e-04 - val_loss: 8.0692e-04 - val_mae: 0.0212 - val_mse: 7.3408e-04 - lr: 3.6977e-0404
Epoch 61/150
 78/122 [==================>...........] - ETA: 0s - loss: 7.6577e-04 - mae: 0.0204 - mse: 6.9514e-04
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0001848835381679237.
122/122 [==============================] - 0s 1ms/step - loss: 7.8182e-04 - mae: 0.0206 - mse: 7.1101e-04 - val_loss: 8.0505e-04 - val_mae: 0.0212 - val_mse: 7.3320e-04 - lr: 3.6977e-04
Epoch 62/150
122/122 [==============================] - 0s 1000us/step - loss: 7.8104e-04 - mae: 0.0206 - mse: 7.1039e-04 - val_loss: 8.0329e-04 - val_mae: 0.0212 - val_mse: 7.3171e-04 - lr: 1.8488e-04
Epoch 63/150
122/122 [==============================] - 0s 1ms/step - loss: 7.8079e-04 - mae: 0.0206 - mse: 7.1000e-04 - val_loss: 8.0134e-04 - val_mae: 0.0212 - val_mse: 7.2977e-04 - lr: 1.8488e-04
Epoch 64/150
122/122 [==============================] - 0s 1ms/step - loss: 7.8104e-04 - mae: 0.0206 - mse: 7.0968e-04 - val_loss: 7.9949e-04 - val_mae: 0.0212 - val_mse: 7.2850e-04 - lr: 1.8488e-04
Epoch 65/150
 84/122 [===================>..........] - ETA: 0s - loss: 7.8252e-04 - mae: 0.0206 - mse: 7.1129e-04
122/122 [==============================] - 1s 6ms/step - loss: 7.8066e-04 - mae: 0.0206 - mse: 7.0954e-04 - val_loss: 7.9784e-04 - val_mae: 0.0212 - val_mse: 7.2757e-04 - lr: 1.8488e-04
Epoch 66/150
 60/122 [=============>................] - ETA: 0s - loss: 7.9094e-04 - mae: 0.0208 - mse: 7.2017e-04
Epoch 66: ReduceLROnPlateau reducing learning rate to 9.244176908396184e-05.
122/122 [==============================] - 0s 1ms/step - loss: 7.8053e-04 - mae: 0.0206 - mse: 7.0965e-04 - val_loss: 8.0141e-04 - val_mae: 0.0212 - val_mse: 7.3019e-04 - lr: 1.8488e-04
Epoch 67/150
122/122 [==============================] - 0s 999us/step - loss: 7.7989e-04 - mae: 0.0206 - mse: 7.0870e-04 - val_loss: 7.9809e-04 - val_mae: 0.0212 - val_mse: 7.2761e-04 - lr: 9.2442e-05
Epoch 68/150
122/122 [==============================] - 0s 991us/step - loss: 7.8005e-04 - mae: 0.0206 - mse: 7.0983e-04 - val_loss: 7.9904e-04 - val_mae: 0.0212 - val_mse: 7.2837e-04 - lr: 9.2442e-05
Epoch 69/150
122/122 [==============================] - 0s 987us/step - loss: 7.7980e-04 - mae: 0.0206 - mse: 7.0910e-04 - val_loss: 8.0073e-04 - val_mae: 0.0212 - val_mse: 7.2949e-04 - lr: 9.2442e-05
Epoch 70/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7988e-04 - mae: 0.0206 - mse: 7.0903e-04 - val_loss: 7.9870e-04 - val_mae: 0.0212 - val_mse: 7.2798e-04 - lr: 9.2442e-05
Epoch 71/150
 82/122 [===================>..........] - ETA: 0s - loss: 7.9012e-04 - mae: 0.0208 - mse: 7.1940e-04
Epoch 71: ReduceLROnPlateau reducing learning rate to 4.622088454198092e-05.
122/122 [==============================] - 1s 6ms/step - loss: 7.8066e-04 - mae: 0.0206 - mse: 7.0954e-04 - val_loss: 7.9784e-04 - val_mae: 0.0212 - val_mse: 7.2757e-04 - lr: 1.8488e-04
122/122 [==============================] - 0s 993us/step - loss: 7.7926e-04 - mae: 0.0206 - mse: 7.0881e-04 - val_loss: 7.9832e-04 - val_mae: 0.0212 - val_mse: 7.2786e-04 - lr: 1.1555e-05
Epoch 76/150
 85/122 [===================>..........] - ETA: 0s - loss: 7.9162e-04 - mae: 0.0207 - mse: 7.2098e-04
Epoch 76: ReduceLROnPlateau reducing learning rate to 2.311044227099046e-05.
122/122 [==============================] - 0s 993us/step - loss: 7.7952e-04 - mae: 0.0206 - mse: 7.0886e-04 - val_loss: 7.9844e-04 - val_mae: 0.0212 - val_mse: 7.2803e-04 - lr: 4.6221e-05
Epoch 77/150
122/122 [==============================] - 0s 969us/step - loss: 7.7946e-04 - mae: 0.0206 - mse: 7.0895e-04 - val_loss: 7.9815e-04 - val_mae: 0.0212 - val_mse: 7.2780e-04 - lr: 2.3110e-05
Epoch 78/150
122/122 [==============================] - 0s 983us/step - loss: 7.7956e-04 - mae: 0.0206 - mse: 7.0919e-04 - val_loss: 7.9772e-04 - val_mae: 0.0212 - val_mse: 7.2749e-04 - lr: 2.3110e-05
Epoch 79/150
122/122 [==============================] - 0s 974us/step - loss: 7.7943e-04 - mae: 0.0206 - mse: 7.0919e-04 - val_loss: 7.9807e-04 - val_mae: 0.0212 - val_mse: 7.2772e-04 - lr: 2.3110e-05
Epoch 80/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7945e-04 - mae: 0.0206 - mse: 7.0900e-04 - val_loss: 7.9829e-04 - val_mae: 0.0212 - val_mse: 7.2785e-04 - lr: 2.3110e-05
Epoch 81/150
 74/122 [=================>............] - ETA: 0s - loss: 7.8582e-04 - mae: 0.0207 - mse: 7.1551e-04
Epoch 81: ReduceLROnPlateau reducing learning rate to 1.155522113549523e-05.
122/122 [==============================] - 0s 1ms/step - loss: 7.7935e-04 - mae: 0.0206 - mse: 7.0901e-04 - val_loss: 7.9842e-04 - val_mae: 0.0212 - val_mse: 7.2787e-04 - lr: 2.3110e-05
Epoch 82/150
122/122 [==============================] - 0s 974us/step - loss: 7.7930e-04 - mae: 0.0206 - mse: 7.0880e-04 - val_loss: 7.9831e-04 - val_mae: 0.0212 - val_mse: 7.2779e-04 - lr: 1.1555e-05
Epoch 83/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7931e-04 - mae: 0.0206 - mse: 7.0876e-04 - val_loss: 7.9840e-04 - val_mae: 0.0212 - val_mse: 7.2787e-04 - lr: 1.1555e-05
Epoch 84/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7927e-04 - mae: 0.0206 - mse: 7.0875e-04 - val_loss: 7.9839e-04 - val_mae: 0.0212 - val_mse: 7.2786e-04 - lr: 1.1555e-05
Epoch 85/150
122/122 [==============================] - 0s 993us/step - loss: 7.7926e-04 - mae: 0.0206 - mse: 7.0881e-04 - val_loss: 7.9832e-04 - val_mae: 0.0212 - val_mse: 7.2786e-04 - lr: 1.1555e-05
Epoch 86/150
 84/122 [===================>..........] - ETA: 0s - loss: 7.6317e-04 - mae: 0.0204 - mse: 6.9257e-04
Epoch 86: ReduceLROnPlateau reducing learning rate to 1e-05.
122/122 [==============================] - 0s 998us/step - loss: 7.7927e-04 - mae: 0.0206 - mse: 7.0866e-04 - val_loss: 7.9855e-04 - val_mae: 0.0212 - val_mse: 7.2801e-04 - lr: 1.1555e-05
Epoch 87/150
122/122 [==============================] - 0s 999us/step - loss: 7.7925e-04 - mae: 0.0206 - mse: 7.0877e-04 - val_loss: 7.9816e-04 - val_mae: 0.0212 - val_mse: 7.2773e-04 - lr: 1.0000e-05
Epoch 88/150
122/122 [==============================] - 0s 971us/step - loss: 7.7924e-04 - mae: 0.0206 - mse: 7.0878e-04 - val_loss: 7.9833e-04 - val_mae: 0.0212 - val_mse: 7.2784e-04 - lr: 1.0000e-05
Epoch 89/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7921e-04 - mae: 0.0206 - mse: 7.0872e-04 - val_loss: 7.9824e-04 - val_mae: 0.0212 - val_mse: 7.2777e-04 - lr: 1.0000e-05
Epoch 90/150
122/122 [==============================] - 0s 980us/step - loss: 7.7926e-04 - mae: 0.0206 - mse: 7.0875e-04 - val_loss: 7.9842e-04 - val_mae: 0.0212 - val_mse: 7.2789e-04 - lr: 1.0000e-05
Epoch 91/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7924e-04 - mae: 0.0206 - mse: 7.0878e-04 - val_loss: 7.9827e-04 - val_mae: 0.0212 - val_mse: 7.2780e-04 - lr: 1.0000e-05
Epoch 92/150
122/122 [==============================] - 0s 980us/step - loss: 7.7920e-04 - mae: 0.0206 - mse: 7.0870e-04 - val_loss: 7.9815e-04 - val_mae: 0.0212 - val_mse: 7.2770e-04 - lr: 1.0000e-05
Epoch 93/150
122/122 [==============================] - 0s 974us/step - loss: 7.7921e-04 - mae: 0.0206 - mse: 7.0868e-04 - val_loss: 7.9821e-04 - val_mae: 0.0212 - val_mse: 7.2775e-04 - lr: 1.0000e-05
Epoch 94/150
122/122 [==============================] - 0s 973us/step - loss: 7.7921e-04 - mae: 0.0206 - mse: 7.0876e-04 - val_loss: 7.9824e-04 - val_mae: 0.0212 - val_mse: 7.2780e-04 - lr: 1.0000e-05
Epoch 95/150
122/122 [==============================] - 0s 971us/step - loss: 7.7919e-04 - mae: 0.0206 - mse: 7.0869e-04 - val_loss: 7.9830e-04 - val_mae: 0.0212 - val_mse: 7.2781e-04 - lr: 1.0000e-05
Epoch 96/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7925e-04 - mae: 0.0206 - mse: 7.0883e-04 - val_loss: 7.9824e-04 - val_mae: 0.0212 - val_mse: 7.2775e-04 - lr: 1.0000e-05
Epoch 97/150
122/122 [==============================] - 0s 980us/step - loss: 7.7927e-04 - mae: 0.0206 - mse: 7.0883e-04 - val_loss: 7.9832e-04 - val_mae: 0.0212 - val_mse: 7.2781e-04 - lr: 1.0000e-05
Epoch 98/150
122/122 [==============================] - 0s 975us/step - loss: 7.7917e-04 - mae: 0.0206 - mse: 7.0866e-04 - val_loss: 7.9833e-04 - val_mae: 0.0212 - val_mse: 7.2782e-04 - lr: 1.0000e-05
Epoch 99/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7919e-04 - mae: 0.0206 - mse: 7.0866e-04 - val_loss: 7.9825e-04 - val_mae: 0.0212 - val_mse: 7.2776e-04 - lr: 1.0000e-05
Epoch 100/150
122/122 [==============================] - 0s 971us/step - loss: 7.7918e-04 - mae: 0.0206 - mse: 7.0867e-04 - val_loss: 7.9827e-04 - val_mae: 0.0212 - val_mse: 7.2777e-04 - lr: 1.0000e-05
Epoch 101/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7921e-04 - mae: 0.0206 - mse: 7.0867e-04 - val_loss: 7.9822e-04 - val_mae: 0.0212 - val_mse: 7.2774e-04 - lr: 1.0000e-05
Epoch 102/150
122/122 [==============================] - 0s 991us/step - loss: 7.7918e-04 - mae: 0.0206 - mse: 7.0878e-04 - val_loss: 7.9803e-04 - val_mae: 0.0212 - val_mse: 7.2757e-04 - lr: 1.0000e-05
Epoch 103/150
122/122 [==============================] - 0s 994us/step - loss: 7.7914e-04 - mae: 0.0206 - mse: 7.0866e-04 - val_loss: 7.9824e-04 - val_mae: 0.0212 - val_mse: 7.2769e-04 - lr: 1.0000e-05
Epoch 104/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7912e-04 - mae: 0.0206 - mse: 7.0864e-04 - val_loss: 7.9815e-04 - val_mae: 0.0212 - val_mse: 7.2765e-04 - lr: 1.0000e-05
Epoch 105/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7922e-04 - mae: 0.0206 - mse: 7.0879e-04 - val_loss: 7.9826e-04 - val_mae: 0.0212 - val_mse: 7.2770e-04 - lr: 1.0000e-05
Epoch 106/150
122/122 [==============================] - 0s 979us/step - loss: 7.7916e-04 - mae: 0.0206 - mse: 7.0870e-04 - val_loss: 7.9811e-04 - val_mae: 0.0212 - val_mse: 7.2762e-04 - lr: 1.0000e-05
Epoch 107/150
122/122 [==============================] - 0s 991us/step - loss: 7.7913e-04 - mae: 0.0206 - mse: 7.0852e-04 - val_loss: 7.9833e-04 - val_mae: 0.0212 - val_mse: 7.2776e-04 - lr: 1.0000e-05
Epoch 108/150
122/122 [==============================] - 0s 956us/step - loss: 7.7913e-04 - mae: 0.0206 - mse: 7.0853e-04 - val_loss: 7.9824e-04 - val_mae: 0.0212 - val_mse: 7.2768e-04 - lr: 1.0000e-05
Epoch 109/150
122/122 [==============================] - 0s 996us/step - loss: 7.7912e-04 - mae: 0.0206 - mse: 7.0858e-04 - val_loss: 7.9817e-04 - val_mae: 0.0212 - val_mse: 7.2765e-04 - lr: 1.0000e-05
Epoch 110/150
122/122 [==============================] - 0s 971us/step - loss: 7.7911e-04 - mae: 0.0206 - mse: 7.0863e-04 - val_loss: 7.9798e-04 - val_mae: 0.0212 - val_mse: 7.2750e-04 - lr: 1.0000e-05
Epoch 111/150
122/122 [==============================] - 0s 982us/step - loss: 7.7910e-04 - mae: 0.0206 - mse: 7.0857e-04 - val_loss: 7.9809e-04 - val_mae: 0.0212 - val_mse: 7.2759e-04 - lr: 1.0000e-05
Epoch 112/150
122/122 [==============================] - 0s 973us/step - loss: 7.7910e-04 - mae: 0.0206 - mse: 7.0858e-04 - val_loss: 7.9808e-04 - val_mae: 0.0212 - val_mse: 7.2758e-04 - lr: 1.0000e-05
Epoch 113/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7907e-04 - mae: 0.0206 - mse: 7.0853e-04 - val_loss: 7.9804e-04 - val_mae: 0.0212 - val_mse: 7.2753e-04 - lr: 1.0000e-05
Epoch 114/150
122/122 [==============================] - 0s 975us/step - loss: 7.7910e-04 - mae: 0.0206 - mse: 7.0867e-04 - val_loss: 7.9801e-04 - val_mae: 0.0212 - val_mse: 7.2753e-04 - lr: 1.0000e-05
Epoch 115/150
122/122 [==============================] - 0s 991us/step - loss: 7.7906e-04 - mae: 0.0206 - mse: 7.0855e-04 - val_loss: 7.9801e-04 - val_mae: 0.0212 - val_mse: 7.2752e-04 - lr: 1.0000e-05
Epoch 116/150
122/122 [==============================] - 0s 978us/step - loss: 7.7911e-04 - mae: 0.0206 - mse: 7.0867e-04 - val_loss: 7.9794e-04 - val_mae: 0.0212 - val_mse: 7.2746e-04 - lr: 1.0000e-05
Epoch 117/150
122/122 [==============================] - 0s 998us/step - loss: 7.7905e-04 - mae: 0.0206 - mse: 7.0858e-04 - val_loss: 7.9780e-04 - val_mae: 0.0212 - val_mse: 7.2737e-04 - lr: 1.0000e-05
Epoch 118/150
122/122 [==============================] - 0s 990us/step - loss: 7.7908e-04 - mae: 0.0206 - mse: 7.0859e-04 - val_loss: 7.9810e-04 - val_mae: 0.0212 - val_mse: 7.2758e-04 - lr: 1.0000e-05
Epoch 119/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7912e-04 - mae: 0.0206 - mse: 7.0868e-04 - val_loss: 7.9784e-04 - val_mae: 0.0212 - val_mse: 7.2742e-04 - lr: 1.0000e-05
Epoch 120/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7907e-04 - mae: 0.0206 - mse: 7.0852e-04 - val_loss: 7.9802e-04 - val_mae: 0.0212 - val_mse: 7.2752e-04 - lr: 1.0000e-05
Epoch 121/150
122/122 [==============================] - 0s 991us/step - loss: 7.7906e-04 - mae: 0.0206 - mse: 7.0864e-04 - val_loss: 7.9802e-04 - val_mae: 0.0212 - val_mse: 7.2752e-04 - lr: 1.0000e-05
Epoch 122/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7903e-04 - mae: 0.0206 - mse: 7.0850e-04 - val_loss: 7.9811e-04 - val_mae: 0.0212 - val_mse: 7.2758e-04 - lr: 1.0000e-05
Epoch 123/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7903e-04 - mae: 0.0206 - mse: 7.0846e-04 - val_loss: 7.9819e-04 - val_mae: 0.0212 - val_mse: 7.2762e-04 - lr: 1.0000e-05
Epoch 124/150
122/122 [==============================] - 0s 981us/step - loss: 7.7901e-04 - mae: 0.0206 - mse: 7.0841e-04 - val_loss: 7.9818e-04 - val_mae: 0.0212 - val_mse: 7.2762e-04 - lr: 1.0000e-05
Epoch 125/150
122/122 [==============================] - 0s 979us/step - loss: 7.7905e-04 - mae: 0.0206 - mse: 7.0852e-04 - val_loss: 7.9783e-04 - val_mae: 0.0212 - val_mse: 7.2738e-04 - lr: 1.0000e-05
Epoch 126/150
122/122 [==============================] - 0s 974us/step - loss: 7.7906e-04 - mae: 0.0206 - mse: 7.0851e-04 - val_loss: 7.9780e-04 - val_mae: 0.0212 - val_mse: 7.2732e-04 - lr: 1.0000e-05
Epoch 127/150
122/122 [==============================] - 0s 989us/step - loss: 7.7902e-04 - mae: 0.0206 - mse: 7.0847e-04 - val_loss: 7.9800e-04 - val_mae: 0.0212 - val_mse: 7.2747e-04 - lr: 1.0000e-05
Epoch 128/150
122/122 [==============================] - 0s 971us/step - loss: 7.7904e-04 - mae: 0.0206 - mse: 7.0860e-04 - val_loss: 7.9781e-04 - val_mae: 0.0212 - val_mse: 7.2735e-04 - lr: 1.0000e-05
Epoch 129/150
122/122 [==============================] - 0s 982us/step - loss: 7.7900e-04 - mae: 0.0206 - mse: 7.0850e-04 - val_loss: 7.9784e-04 - val_mae: 0.0212 - val_mse: 7.2738e-04 - lr: 1.0000e-05
Epoch 130/150
122/122 [==============================] - 0s 989us/step - loss: 7.7900e-04 - mae: 0.0206 - mse: 7.0857e-04 - val_loss: 7.9809e-04 - val_mae: 0.0212 - val_mse: 7.2754e-04 - lr: 1.0000e-05
Epoch 131/150
122/122 [==============================] - 0s 981us/step - loss: 7.7898e-04 - mae: 0.0206 - mse: 7.0840e-04 - val_loss: 7.9802e-04 - val_mae: 0.0212 - val_mse: 7.2748e-04 - lr: 1.0000e-05
Epoch 132/150
122/122 [==============================] - 0s 976us/step - loss: 7.7897e-04 - mae: 0.0206 - mse: 7.0852e-04 - val_loss: 7.9785e-04 - val_mae: 0.0212 - val_mse: 7.2737e-04 - lr: 1.0000e-05
Epoch 133/150
122/122 [==============================] - 0s 983us/step - loss: 7.7897e-04 - mae: 0.0206 - mse: 7.0848e-04 - val_loss: 7.9799e-04 - val_mae: 0.0212 - val_mse: 7.2747e-04 - lr: 1.0000e-05
Epoch 134/150
122/122 [==============================] - 0s 978us/step - loss: 7.7897e-04 - mae: 0.0206 - mse: 7.0844e-04 - val_loss: 7.9802e-04 - val_mae: 0.0212 - val_mse: 7.2750e-04 - lr: 1.0000e-05
Epoch 135/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7896e-04 - mae: 0.0206 - mse: 7.0847e-04 - val_loss: 7.9805e-04 - val_mae: 0.0212 - val_mse: 7.2749e-04 - lr: 1.0000e-05
Epoch 136/150
122/122 [==============================] - 0s 978us/step - loss: 7.7896e-04 - mae: 0.0206 - mse: 7.0840e-04 - val_loss: 7.9813e-04 - val_mae: 0.0212 - val_mse: 7.2758e-04 - lr: 1.0000e-05
Epoch 137/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7895e-04 - mae: 0.0206 - mse: 7.0846e-04 - val_loss: 7.9799e-04 - val_mae: 0.0212 - val_mse: 7.2747e-04 - lr: 1.0000e-05
Epoch 138/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7892e-04 - mae: 0.0206 - mse: 7.0840e-04 - val_loss: 7.9801e-04 - val_mae: 0.0212 - val_mse: 7.2749e-04 - lr: 1.0000e-05
Epoch 139/150
122/122 [==============================] - 0s 990us/step - loss: 7.7896e-04 - mae: 0.0206 - mse: 7.0848e-04 - val_loss: 7.9782e-04 - val_mae: 0.0212 - val_mse: 7.2732e-04 - lr: 1.0000e-05
Epoch 140/150
122/122 [==============================] - 0s 981us/step - loss: 7.7903e-04 - mae: 0.0206 - mse: 7.0842e-04 - val_loss: 7.9798e-04 - val_mae: 0.0212 - val_mse: 7.2745e-04 - lr: 1.0000e-05
Epoch 141/150
122/122 [==============================] - 0s 986us/step - loss: 7.7894e-04 - mae: 0.0206 - mse: 7.0847e-04 - val_loss: 7.9791e-04 - val_mae: 0.0212 - val_mse: 7.2742e-04 - lr: 1.0000e-05
Epoch 142/150
122/122 [==============================] - 0s 974us/step - loss: 7.7893e-04 - mae: 0.0206 - mse: 7.0836e-04 - val_loss: 7.9789e-04 - val_mae: 0.0212 - val_mse: 7.2739e-04 - lr: 1.0000e-05
Epoch 143/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7893e-04 - mae: 0.0206 - mse: 7.0856e-04 - val_loss: 7.9780e-04 - val_mae: 0.0212 - val_mse: 7.2733e-04 - lr: 1.0000e-05
Epoch 144/150
122/122 [==============================] - 0s 989us/step - loss: 7.7890e-04 - mae: 0.0206 - mse: 7.0834e-04 - val_loss: 7.9799e-04 - val_mae: 0.0212 - val_mse: 7.2745e-04 - lr: 1.0000e-05
Epoch 145/150
122/122 [==============================] - 0s 980us/step - loss: 7.7893e-04 - mae: 0.0206 - mse: 7.0847e-04 - val_loss: 7.9767e-04 - val_mae: 0.0212 - val_mse: 7.2723e-04 - lr: 1.0000e-05
Epoch 146/150
122/122 [==============================] - 0s 1ms/step - loss: 7.7890e-04 - mae: 0.0206 - mse: 7.0840e-04 - val_loss: 7.9769e-04 - val_mae: 0.0212 - val_mse: 7.2722e-04 - lr: 1.0000e-05
Epoch 147/150
122/122 [==============================] - 0s 975us/step - loss: 7.7885e-04 - mae: 0.0206 - mse: 7.0835e-04 - val_loss: 7.9782e-04 - val_mae: 0.0212 - val_mse: 7.2733e-04 - lr: 1.0000e-05
Epoch 148/150
122/122 [==============================] - 0s 978us/step - loss: 7.7888e-04 - mae: 0.0206 - mse: 7.0839e-04 - val_loss: 7.9780e-04 - val_mae: 0.0212 - val_mse: 7.2730e-04 - lr: 1.0000e-05
Epoch 149/150
122/122 [==============================] - 0s 974us/step - loss: 7.7886e-04 - mae: 0.0206 - mse: 7.0834e-04 - val_loss: 7.9781e-04 - val_mae: 0.0212 - val_mse: 7.2731e-04 - lr: 1.0000e-05
Epoch 150/150
122/122 [==============================] - 0s 980us/step - loss: 7.7887e-04 - mae: 0.0206 - mse: 7.0827e-04 - val_loss: 7.9781e-04 - val_mae: 0.0212 - val_mse: 7.2733e-04 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap_adam_0.09466036914606556LR_[20]HN_32BS_5P_val_mseM_150epochs/model_1.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0177 - mae: 0.0576 - mse: 0.0053 - val_loss: 0.0036 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0947
Epoch 2/150
162/162 [==============================] - 0s 934us/step - loss: 0.0035 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0035 - val_mae: 0.0505 - val_mse: 0.0035 - lr: 0.0947
Epoch 3/150
162/162 [==============================] - 0s 949us/step - loss: 0.0035 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0505 - val_mse: 0.0034 - lr: 0.0947
Epoch 4/150
162/162 [==============================] - 0s 940us/step - loss: 0.0035 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0947
Epoch 5/150
162/162 [==============================] - 0s 999us/step - loss: 0.0035 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0039 - val_mae: 0.0509 - val_mse: 0.0035 - lr: 0.0947
Epoch 6/150
158/162 [============================>.] - ETA: 0s - loss: 0.0035 - mae: 0.0507 - mse: 0.0034
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.047330185770988464.
162/162 [==============================] - 0s 969us/step - loss: 0.0035 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0038 - val_mae: 0.0504 - val_mse: 0.0034 - lr: 0.0947
Epoch 7/150
162/162 [==============================] - 0s 931us/step - loss: 0.0035 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0504 - val_mse: 0.0034 - lr: 0.0473
Epoch 8/150
162/162 [==============================] - 0s 951us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0473
Epoch 9/150
162/162 [==============================] - 0s 943us/step - loss: 0.0035 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0473
Epoch 10/150
162/162 [==============================] - 0s 953us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0473
Epoch 11/150
 84/162 [==============>...............] - ETA: 0s - loss: 0.0034 - mae: 0.0509 - mse: 0.0034
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.023665092885494232.
162/162 [==============================] - 0s 948us/step - loss: 0.0034 - mae: 0.0508 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0473
Epoch 12/150
162/162 [==============================] - 0s 943us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0237
Epoch 13/150
162/162 [==============================] - 0s 999us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0505 - val_mse: 0.0034 - lr: 0.0237
Epoch 14/150
162/162 [==============================] - 0s 936us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0505 - val_mse: 0.0034 - lr: 0.0237
Epoch 15/150
162/162 [==============================] - 0s 924us/step - loss: 0.0035 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0036 - val_mae: 0.0508 - val_mse: 0.0035 - lr: 0.0237
Epoch 16/150
161/162 [============================>.] - ETA: 0s - loss: 0.0034 - mae: 0.0507 - mse: 0.0034
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.011832546442747116.
162/162 [==============================] - 0s 957us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0237
Epoch 17/150
162/162 [==============================] - 0s 940us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0504 - val_mse: 0.0034 - lr: 0.0118
Epoch 18/150
162/162 [==============================] - 0s 940us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0118
Epoch 19/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0035 - val_mae: 0.0505 - val_mse: 0.0035 - lr: 0.0118
Epoch 20/150
162/162 [==============================] - 0s 980us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0118
Epoch 21/150
 83/162 [==============>...............] - ETA: 0s - loss: 0.0034 - mae: 0.0510 - mse: 0.0034
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.005916273221373558.
162/162 [==============================] - 0s 968us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0504 - val_mse: 0.0034 - lr: 0.0118
Epoch 22/150
162/162 [==============================] - 0s 967us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0059
Epoch 23/150
162/162 [==============================] - 0s 961us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0059
Epoch 24/150
162/162 [==============================] - 0s 942us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0059
Epoch 25/150
162/162 [==============================] - 0s 946us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0059
Epoch 26/150
 82/162 [==============>...............] - ETA: 0s - loss: 0.0034 - mae: 0.0502 - mse: 0.0034
Epoch 26: ReduceLROnPlateau reducing learning rate to 0.002958136610686779.
162/162 [==============================] - 0s 932us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0059
Epoch 27/150
162/162 [==============================] - 0s 942us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0030
Epoch 28/150
162/162 [==============================] - 0s 938us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0030
Epoch 29/150
162/162 [==============================] - 0s 935us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0504 - val_mse: 0.0034 - lr: 0.0030
Epoch 30/150
162/162 [==============================] - 0s 947us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0030
Epoch 31/150
 84/162 [==============>...............] - ETA: 0s - loss: 0.0034 - mae: 0.0509 - mse: 0.0034
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0014790683053433895.
162/162 [==============================] - 0s 947us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0504 - val_mse: 0.0034 - lr: 0.0030
Epoch 32/150
162/162 [==============================] - 0s 940us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0015
Epoch 33/150
162/162 [==============================] - 0s 949us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0015
Epoch 34/150
162/162 [==============================] - 0s 955us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0015
Epoch 35/150
162/162 [==============================] - 0s 954us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0015
Epoch 36/150
 81/162 [==============>...............] - ETA: 0s - loss: 0.0034 - mae: 0.0505 - mse: 0.0034
Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0007395341526716948.
162/162 [==============================] - 0s 953us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0015
Epoch 37/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 7.3953e-04
Epoch 38/150
162/162 [==============================] - 0s 961us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 7.3953e-04
Epoch 39/150
162/162 [==============================] - 0s 942us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 7.3953e-04
Epoch 40/150
162/162 [==============================] - 0s 948us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 7.3953e-04
Epoch 41/150
 82/162 [==============>...............] - ETA: 0s - loss: 0.0034 - mae: 0.0505 - mse: 0.0034
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0003697670763358474.
162/162 [==============================] - 0s 940us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 7.3953e-04
Epoch 42/150
162/162 [==============================] - 0s 931us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 3.6977e-04
Epoch 43/150
162/162 [==============================] - 0s 946us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 3.6977e-04
Epoch 44/150
162/162 [==============================] - 0s 997us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 3.6977e-04
Epoch 45/150
162/162 [==============================] - 0s 993us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 3.6977e-04
Epoch 46/150
148/162 [==========================>...] - ETA: 0s - loss: 0.0034 - mae: 0.0504 - mse: 0.0034
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001848835381679237.
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 3.6977e-04
Epoch 47/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.8488e-04
Epoch 48/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.8488e-04
Epoch 49/150
162/162 [==============================] - 0s 990us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.8488e-04
Epoch 50/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.8488e-04
Epoch 51/150
138/162 [========================>.....] - ETA: 0s - loss: 0.0034 - mae: 0.0505 - mse: 0.0034
Epoch 51: ReduceLROnPlateau reducing learning rate to 9.244176908396184e-05.
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.8488e-04
Epoch 52/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 9.2442e-05
Epoch 53/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 9.2442e-05
Epoch 54/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 9.2442e-05
Epoch 55/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 9.2442e-05
Epoch 56/150
 81/162 [==============>...............] - ETA: 0s - loss: 0.0034 - mae: 0.0506 - mse: 0.0034
Epoch 56: ReduceLROnPlateau reducing learning rate to 4.622088454198092e-05.
162/162 [==============================] - 0s 964us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 9.2442e-05
Epoch 57/150
162/162 [==============================] - 0s 996us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 4.6221e-05
Epoch 58/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 4.6221e-05
Epoch 59/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 4.6221e-05
Epoch 60/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 4.6221e-05
Epoch 61/150
155/162 [===========================>..] - ETA: 0s - loss: 0.0034 - mae: 0.0505 - mse: 0.0034
Epoch 61: ReduceLROnPlateau reducing learning rate to 2.311044227099046e-05.
162/162 [==============================] - 0s 999us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 4.6221e-05
Epoch 62/150
162/162 [==============================] - 0s 977us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 2.3110e-05
Epoch 63/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 2.3110e-05
Epoch 64/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 2.3110e-05
Epoch 65/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 2.3110e-05
Epoch 66/150
162/162 [==============================] - ETA: 0s - loss: 0.0034 - mae: 0.0506 - mse: 0.0034
Epoch 66: ReduceLROnPlateau reducing learning rate to 1.155522113549523e-05.
162/162 [==============================] - 0s 999us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 2.3110e-05
Epoch 67/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.1555e-05
Epoch 68/150
162/162 [==============================] - 0s 988us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.1555e-05
Epoch 69/150
162/162 [==============================] - 0s 986us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.1555e-05
Epoch 70/150
162/162 [==============================] - 0s 937us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.1555e-05
Epoch 71/150
161/162 [============================>.] - ETA: 0s - loss: 0.0034 - mae: 0.0506 - mse: 0.0034
Epoch 71: ReduceLROnPlateau reducing learning rate to 1e-05.
162/162 [==============================] - 0s 964us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.1555e-05
Epoch 72/150
162/162 [==============================] - 0s 956us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 73/150
162/162 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 74/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 75/150
162/162 [==============================] - 0s 923us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 76/150
162/162 [==============================] - 0s 949us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 77/150
162/162 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 78/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 79/150
162/162 [==============================] - 0s 941us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 80/150
162/162 [==============================] - 0s 938us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 81/150
162/162 [==============================] - 0s 964us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 82/150
162/162 [==============================] - 0s 956us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 83/150
162/162 [==============================] - 0s 949us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 84/150
162/162 [==============================] - 0s 965us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 85/150
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 86/150
162/162 [==============================] - 0s 937us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 87/150
162/162 [==============================] - 0s 958us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 88/150
162/162 [==============================] - 0s 960us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 89/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 90/150
162/162 [==============================] - 0s 970us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 91/150
162/162 [==============================] - 0s 929us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 92/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 93/150
162/162 [==============================] - 0s 946us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 94/150
162/162 [==============================] - 0s 966us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 95/150
162/162 [==============================] - 0s 952us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 96/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 97/150
162/162 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 98/150
162/162 [==============================] - 0s 947us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 99/150
162/162 [==============================] - 0s 949us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 100/150
162/162 [==============================] - 0s 926us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 101/150
162/162 [==============================] - 0s 962us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 102/150
162/162 [==============================] - 0s 935us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 103/150
162/162 [==============================] - 0s 954us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 104/150
162/162 [==============================] - 0s 941us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 105/150
162/162 [==============================] - 0s 957us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 106/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 107/150
162/162 [==============================] - 0s 931us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 108/150
162/162 [==============================] - 0s 974us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 109/150
162/162 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 110/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 111/150
162/162 [==============================] - 0s 937us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 112/150
162/162 [==============================] - 0s 926us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 113/150
162/162 [==============================] - 0s 938us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 114/150
162/162 [==============================] - 0s 933us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 115/150
162/162 [==============================] - 0s 936us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 116/150
162/162 [==============================] - 0s 932us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150
162/162 [==============================] - 0s 949us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 118/150
162/162 [==============================] - 0s 951us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 119/150
162/162 [==============================] - 0s 935us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 120/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 121/150
162/162 [==============================] - 0s 966us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 122/150
162/162 [==============================] - 0s 954us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 123/150
162/162 [==============================] - 0s 961us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 124/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 125/150
162/162 [==============================] - 0s 943us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 126/150
162/162 [==============================] - 0s 977us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 127/150
162/162 [==============================] - 0s 954us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 128/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 129/150
162/162 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 130/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 131/150
162/162 [==============================] - 0s 975us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 132/150
162/162 [==============================] - 0s 944us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 133/150
162/162 [==============================] - 0s 938us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 134/150
162/162 [==============================] - 0s 936us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 135/150
162/162 [==============================] - 0s 952us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 136/150
162/162 [==============================] - 0s 927us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 137/150
162/162 [==============================] - 0s 937us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 138/150
162/162 [==============================] - 0s 934us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 139/150
162/162 [==============================] - 0s 948us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 140/150
162/162 [==============================] - 0s 938us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 141/150
 82/162 [==============>...............] - ETA: 0s - loss: 0.0034 - mae: 0.0503 - mse: 0.0034
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 142/150
162/162 [==============================] - 0s 950us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 143/150
162/162 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 144/150
162/162 [==============================] - 0s 1ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 145/150
162/162 [==============================] - 0s 942us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 146/150
162/162 [==============================] - 0s 944us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 147/150
162/162 [==============================] - 0s 931us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 148/150
162/162 [==============================] - 0s 927us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 149/150
162/162 [==============================] - 0s 936us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 150/150
162/162 [==============================] - 0s 936us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap_adam_0.09466036914606556LR_[20]HN_32BS_5P_val_mseM_150epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/150
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0019s). Check your callbacks.
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230419_192213-zzb1sz9l\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
122/122 [==============================] - 0s 1ms/step - loss: 5.0191e-04 - mae: 0.0147 - mse: 4.0847e-04 - val_loss: 5.1862e-04 - val_mae: 0.0152 - val_mse: 4.2514e-04 - lr: 1.0000e-05
Epoch 118/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0190e-04 - mae: 0.0147 - mse: 4.0843e-04 - val_loss: 5.1880e-04 - val_mae: 0.0152 - val_mse: 4.2532e-04 - lr: 1.0000e-05
Epoch 119/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0180e-04 - mae: 0.0147 - mse: 4.0829e-04 - val_loss: 5.1849e-04 - val_mae: 0.0152 - val_mse: 4.2499e-04 - lr: 1.0000e-05
Epoch 120/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0179e-04 - mae: 0.0147 - mse: 4.0826e-04 - val_loss: 5.1796e-04 - val_mae: 0.0152 - val_mse: 4.2444e-04 - lr: 1.0000e-05
Epoch 121/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0181e-04 - mae: 0.0147 - mse: 4.0831e-04 - val_loss: 5.1813e-04 - val_mae: 0.0152 - val_mse: 4.2465e-04 - lr: 1.0000e-05
Epoch 122/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0183e-04 - mae: 0.0147 - mse: 4.0836e-04 - val_loss: 5.1816e-04 - val_mae: 0.0152 - val_mse: 4.2471e-04 - lr: 1.0000e-05
Epoch 123/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0174e-04 - mae: 0.0147 - mse: 4.0826e-04 - val_loss: 5.1820e-04 - val_mae: 0.0152 - val_mse: 4.2479e-04 - lr: 1.0000e-05
Epoch 124/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0176e-04 - mae: 0.0147 - mse: 4.0836e-04 - val_loss: 5.1819e-04 - val_mae: 0.0152 - val_mse: 4.2477e-04 - lr: 1.0000e-05
Epoch 125/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0179e-04 - mae: 0.0147 - mse: 4.0836e-04 - val_loss: 5.1808e-04 - val_mae: 0.0152 - val_mse: 4.2464e-04 - lr: 1.0000e-05
Epoch 126/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0171e-04 - mae: 0.0147 - mse: 4.0824e-04 - val_loss: 5.1833e-04 - val_mae: 0.0152 - val_mse: 4.2483e-04 - lr: 1.0000e-05
Epoch 127/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0174e-04 - mae: 0.0147 - mse: 4.0827e-04 - val_loss: 5.1861e-04 - val_mae: 0.0152 - val_mse: 4.2517e-04 - lr: 1.0000e-05
Epoch 128/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0176e-04 - mae: 0.0147 - mse: 4.0832e-04 - val_loss: 5.1866e-04 - val_mae: 0.0152 - val_mse: 4.2522e-04 - lr: 1.0000e-05
Epoch 129/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0172e-04 - mae: 0.0147 - mse: 4.0830e-04 - val_loss: 5.1821e-04 - val_mae: 0.0152 - val_mse: 4.2479e-04 - lr: 1.0000e-05
Epoch 130/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0173e-04 - mae: 0.0147 - mse: 4.0832e-04 - val_loss: 5.1837e-04 - val_mae: 0.0152 - val_mse: 4.2494e-04 - lr: 1.0000e-05
Epoch 131/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0178e-04 - mae: 0.0147 - mse: 4.0837e-04 - val_loss: 5.1809e-04 - val_mae: 0.0152 - val_mse: 4.2468e-04 - lr: 1.0000e-05
Epoch 132/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0165e-04 - mae: 0.0147 - mse: 4.0819e-04 - val_loss: 5.1837e-04 - val_mae: 0.0152 - val_mse: 4.2494e-04 - lr: 1.0000e-05
Epoch 133/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0168e-04 - mae: 0.0147 - mse: 4.0824e-04 - val_loss: 5.1804e-04 - val_mae: 0.0152 - val_mse: 4.2458e-04 - lr: 1.0000e-05
Epoch 134/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0164e-04 - mae: 0.0147 - mse: 4.0820e-04 - val_loss: 5.1800e-04 - val_mae: 0.0152 - val_mse: 4.2457e-04 - lr: 1.0000e-05
Epoch 135/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0167e-04 - mae: 0.0147 - mse: 4.0822e-04 - val_loss: 5.1803e-04 - val_mae: 0.0152 - val_mse: 4.2459e-04 - lr: 1.0000e-05
Epoch 136/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0161e-04 - mae: 0.0147 - mse: 4.0815e-04 - val_loss: 5.1848e-04 - val_mae: 0.0152 - val_mse: 4.2506e-04 - lr: 1.0000e-05
Epoch 137/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0165e-04 - mae: 0.0147 - mse: 4.0825e-04 - val_loss: 5.1807e-04 - val_mae: 0.0152 - val_mse: 4.2465e-04 - lr: 1.0000e-05
Epoch 138/150
122/122 [==============================] - 0s 2ms/step - loss: 5.0164e-04 - mae: 0.0147 - mse: 4.0826e-04 - val_loss: 5.1887e-04 - val_mae: 0.0152 - val_mse: 4.2550e-04 - lr: 1.0000e-05
Epoch 139/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0165e-04 - mae: 0.0147 - mse: 4.0826e-04 - val_loss: 5.1799e-04 - val_mae: 0.0152 - val_mse: 4.2456e-04 - lr: 1.0000e-05
Epoch 140/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0160e-04 - mae: 0.0147 - mse: 4.0816e-04 - val_loss: 5.1818e-04 - val_mae: 0.0152 - val_mse: 4.2478e-04 - lr: 1.0000e-05
Epoch 141/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0158e-04 - mae: 0.0147 - mse: 4.0810e-04 - val_loss: 5.1821e-04 - val_mae: 0.0152 - val_mse: 4.2477e-04 - lr: 1.0000e-05
Epoch 142/150
122/122 [==============================] - 0s 2ms/step - loss: 5.0161e-04 - mae: 0.0147 - mse: 4.0816e-04 - val_loss: 5.1873e-04 - val_mae: 0.0152 - val_mse: 4.2528e-04 - lr: 1.0000e-05
Epoch 143/150
122/122 [==============================] - 0s 2ms/step - loss: 5.0160e-04 - mae: 0.0147 - mse: 4.0813e-04 - val_loss: 5.1848e-04 - val_mae: 0.0152 - val_mse: 4.2503e-04 - lr: 1.0000e-05
Epoch 144/150
122/122 [==============================] - 0s 2ms/step - loss: 5.0157e-04 - mae: 0.0147 - mse: 4.0816e-04 - val_loss: 5.1833e-04 - val_mae: 0.0152 - val_mse: 4.2493e-04 - lr: 1.0000e-05
Epoch 145/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0154e-04 - mae: 0.0147 - mse: 4.0814e-04 - val_loss: 5.1855e-04 - val_mae: 0.0152 - val_mse: 4.2513e-04 - lr: 1.0000e-05
Epoch 146/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0157e-04 - mae: 0.0147 - mse: 4.0807e-04 - val_loss: 5.1869e-04 - val_mae: 0.0152 - val_mse: 4.2522e-04 - lr: 1.0000e-05
Epoch 147/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0158e-04 - mae: 0.0147 - mse: 4.0810e-04 - val_loss: 5.1847e-04 - val_mae: 0.0152 - val_mse: 4.2502e-04 - lr: 1.0000e-05
Epoch 148/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0152e-04 - mae: 0.0147 - mse: 4.0804e-04 - val_loss: 5.1794e-04 - val_mae: 0.0152 - val_mse: 4.2446e-04 - lr: 1.0000e-05
Epoch 149/150
122/122 [==============================] - 0s 2ms/step - loss: 5.0153e-04 - mae: 0.0147 - mse: 4.0810e-04 - val_loss: 5.1776e-04 - val_mae: 0.0152 - val_mse: 4.2433e-04 - lr: 1.0000e-05
Epoch 150/150
122/122 [==============================] - 0s 1ms/step - loss: 5.0150e-04 - mae: 0.0147 - mse: 4.0810e-04 - val_loss: 5.1794e-04 - val_mae: 0.0152 - val_mse: 4.2449e-04 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap_adam_0.09466036914606556LR_[20]HN_32BS_5P_val_mseM_150epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/150
122/122 [==============================] - 0s 2ms/step - loss: 0.0169 - mae: 0.0387 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0381 - val_mse: 0.0019 - lr: 0.0947
Epoch 2/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0020 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0023 - val_mae: 0.0393 - val_mse: 0.0021 - lr: 0.0947
Epoch 3/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0021 - mae: 0.0381 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0947
Epoch 4/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0020 - mae: 0.0381 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0388 - val_mse: 0.0020 - lr: 0.0947
Epoch 5/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0021 - mae: 0.0384 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0947
Epoch 6/150
 80/122 [==================>...........] - ETA: 0s - loss: 0.0022 - mae: 0.0382 - mse: 0.0020
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.047330185770988464.
122/122 [==============================] - 0s 1ms/step - loss: 0.0022 - mae: 0.0385 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0947
Epoch 7/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0020 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0386 - val_mse: 0.0020 - lr: 0.0473
Epoch 8/150
122/122 [==============================] - 0s 996us/step - loss: 0.0020 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0473
Epoch 9/150
122/122 [==============================] - 0s 999us/step - loss: 0.0020 - mae: 0.0382 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0386 - val_mse: 0.0020 - lr: 0.0473
Epoch 10/150
122/122 [==============================] - 0s 999us/step - loss: 0.0020 - mae: 0.0381 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0473
Epoch 11/150
 81/122 [==================>...........] - ETA: 0s - loss: 0.0020 - mae: 0.0383 - mse: 0.0020
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.023665092885494232.
122/122 [==============================] - 0s 985us/step - loss: 0.0020 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0473
Epoch 12/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0237
Epoch 13/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0237
Epoch 14/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0237
Epoch 15/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0237
Epoch 16/150
 82/122 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0382 - mse: 0.0019
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.011832546442747116.
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0237
Epoch 17/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0118
Epoch 18/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0118
Epoch 19/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0118
Epoch 20/150
122/122 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0388 - val_mse: 0.0020 - lr: 0.0118
Epoch 21/150
122/122 [==============================] - ETA: 0s - loss: 0.0019 - mae: 0.0380 - mse: 0.0019
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.005916273221373558.
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0118
Epoch 22/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0059
Epoch 23/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0059
Epoch 24/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0059
Epoch 25/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0059
Epoch 26/150
116/122 [===========================>..] - ETA: 0s - loss: 0.0019 - mae: 0.0379 - mse: 0.0019
Epoch 26: ReduceLROnPlateau reducing learning rate to 0.002958136610686779.
122/122 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0059
Epoch 27/150
122/122 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0030
Epoch 28/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0030
Epoch 29/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0030
Epoch 30/150
122/122 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0030
Epoch 31/150
105/122 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0377 - mse: 0.0019
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0014790683053433895.
122/122 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0030
Epoch 32/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0015
Epoch 33/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0015
Epoch 34/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0015
Epoch 35/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0015
Epoch 36/150
101/122 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0381 - mse: 0.0019
Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0007395341526716948.
122/122 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0015
Epoch 37/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 7.3953e-04
Epoch 38/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 7.3953e-04
Epoch 39/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 7.3953e-04
Epoch 40/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 7.3953e-04
Epoch 41/150
 80/122 [==================>...........] - ETA: 0s - loss: 0.0019 - mae: 0.0379 - mse: 0.0019
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0003697670763358474.
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 7.3953e-04
Epoch 42/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 3.6977e-04
Epoch 43/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 3.6977e-04
Epoch 44/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 3.6977e-04
Epoch 45/150
122/122 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 3.6977e-04
Epoch 46/150
 57/122 [=============>................] - ETA: 0s - loss: 0.0020 - mae: 0.0384 - mse: 0.0020
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001848835381679237.
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 3.6977e-04
Epoch 47/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 1.8488e-04
Epoch 48/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 1.8488e-04
Epoch 49/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 1.8488e-04
Epoch 50/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 1.8488e-04
Epoch 51/150
 66/122 [===============>..............] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
Epoch 51: ReduceLROnPlateau reducing learning rate to 9.244176908396184e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 1.8488e-04
Epoch 52/150
122/122 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 9.2442e-05
Epoch 53/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 9.2442e-05
Epoch 54/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 9.2442e-05
Epoch 55/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 9.2442e-05
Epoch 56/150
120/122 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0379 - mse: 0.0019
Epoch 56: ReduceLROnPlateau reducing learning rate to 4.622088454198092e-05.
122/122 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 9.2442e-05
Epoch 57/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 4.6221e-05
Epoch 58/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 4.6221e-05
Epoch 59/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 4.6221e-05
Epoch 60/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 4.6221e-05
Epoch 61/150
 70/122 [================>.............] - ETA: 0s - loss: 0.0020 - mae: 0.0385 - mse: 0.0020
Epoch 61: ReduceLROnPlateau reducing learning rate to 2.311044227099046e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 4.6221e-05
Epoch 62/150
122/122 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 2.3110e-05
Epoch 63/150
  1/122 [..............................] - ETA: 0s - loss: 0.0018 - mae: 0.0371 - mse: 0.0018
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0015s). Check your callbacks.
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0019s). Check your callbacks.
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0031s). Check your callbacks.
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 959us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 1.0000e-05