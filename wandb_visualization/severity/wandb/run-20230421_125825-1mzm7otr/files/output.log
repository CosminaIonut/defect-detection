Epoch 1/200
56/81 [===================>..........] - ETA: 0s - loss: 0.4553 - mae: 0.1771 - mse: 0.0470
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0019s). Check your callbacks.
81/81 [==============================] - 2s 17ms/step - loss: 0.3897 - mae: 0.1430 - mse: 0.0342 - val_loss: 0.2028 - val_mae: 0.0617 - val_mse: 0.0048 - lr: 0.0658
Epoch 2/200
59/81 [====================>.........] - ETA: 0s - loss: 0.1424 - mae: 0.0570 - mse: 0.0040
81/81 [==============================] - 1s 17ms/step - loss: 0.1263 - mae: 0.0559 - mse: 0.0039 - val_loss: 0.0716 - val_mae: 0.0534 - val_mse: 0.0035 - lr: 0.0658
Epoch 3/200
53/81 [==================>...........] - ETA: 0s - loss: 0.0528 - mae: 0.0515 - mse: 0.0033
81/81 [==============================] - 1s 17ms/step - loss: 0.0457 - mae: 0.0513 - mse: 0.0033 - val_loss: 0.0272 - val_mae: 0.0505 - val_mse: 0.0031 - lr: 0.0658
Epoch 4/200
67/81 [=======================>......] - ETA: 0s - loss: 0.0195 - mae: 0.0491 - mse: 0.0030
81/81 [==============================] - 1s 15ms/step - loss: 0.0184 - mae: 0.0489 - mse: 0.0030 - val_loss: 0.0120 - val_mae: 0.0485 - val_mse: 0.0029 - lr: 0.0658
Epoch 5/200
51/81 [=================>............] - ETA: 0s - loss: 0.0098 - mae: 0.0473 - mse: 0.0028
81/81 [==============================] - 1s 17ms/step - loss: 0.0088 - mae: 0.0472 - mse: 0.0028 - val_loss: 0.0066 - val_mae: 0.0470 - val_mse: 0.0027 - lr: 0.0658
Epoch 6/200
69/81 [========================>.....] - ETA: 0s - loss: 0.0055 - mae: 0.0458 - mse: 0.0026
81/81 [==============================] - 1s 16ms/step - loss: 0.0054 - mae: 0.0458 - mse: 0.0026 - val_loss: 0.0045 - val_mae: 0.0458 - val_mse: 0.0026 - lr: 0.0658
Epoch 7/200
81/81 [==============================] - ETA: 0s - loss: 0.0040 - mae: 0.0447 - mse: 0.0025
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 17ms/step - loss: 0.0040 - mae: 0.0447 - mse: 0.0025 - val_loss: 0.0036 - val_mae: 0.0447 - val_mse: 0.0025 - lr: 0.0658
Epoch 8/200
81/81 [==============================] - 1s 16ms/step - loss: 0.0034 - mae: 0.0437 - mse: 0.0024 - val_loss: 0.0032 - val_mae: 0.0438 - val_mse: 0.0024 - lr: 0.0658
Epoch 9/200
70/81 [========================>.....] - ETA: 0s - loss: 0.0030 - mae: 0.0428 - mse: 0.0023
81/81 [==============================] - 1s 15ms/step - loss: 0.0030 - mae: 0.0429 - mse: 0.0023 - val_loss: 0.0029 - val_mae: 0.0430 - val_mse: 0.0023 - lr: 0.0658
Epoch 10/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0028 - mae: 0.0420 - mse: 0.0022
81/81 [==============================] - 1s 15ms/step - loss: 0.0028 - mae: 0.0422 - mse: 0.0022 - val_loss: 0.0028 - val_mae: 0.0425 - val_mse: 0.0023 - lr: 0.0658
Epoch 11/200
68/81 [========================>.....] - ETA: 0s - loss: 0.0026 - mae: 0.0417 - mse: 0.0022
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 2s 20ms/step - loss: 0.0026 - mae: 0.0417 - mse: 0.0022 - val_loss: 0.0026 - val_mae: 0.0419 - val_mse: 0.0022 - lr: 0.0658
Epoch 12/200
80/81 [============================>.] - ETA: 0s - loss: 0.0025 - mae: 0.0413 - mse: 0.0022
81/81 [==============================] - 1s 15ms/step - loss: 0.0025 - mae: 0.0412 - mse: 0.0022 - val_loss: 0.0025 - val_mae: 0.0415 - val_mse: 0.0022 - lr: 0.0658
Epoch 13/200
81/81 [==============================] - 2s 23ms/step - loss: 0.0024 - mae: 0.0408 - mse: 0.0021 - val_loss: 0.0024 - val_mae: 0.0411 - val_mse: 0.0022 - lr: 0.0658
Epoch 14/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0023 - mae: 0.0405 - mse: 0.0021
81/81 [==============================] - 1s 15ms/step - loss: 0.0023 - mae: 0.0404 - mse: 0.0021 - val_loss: 0.0023 - val_mae: 0.0408 - val_mse: 0.0021 - lr: 0.0658
Epoch 15/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0023 - mae: 0.0402 - mse: 0.0021
81/81 [==============================] - 1s 18ms/step - loss: 0.0023 - mae: 0.0402 - mse: 0.0021 - val_loss: 0.0023 - val_mae: 0.0405 - val_mse: 0.0021 - lr: 0.0658
Epoch 16/200
53/81 [==================>...........] - ETA: 0s - loss: 0.0022 - mae: 0.0399 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 14ms/step - loss: 0.0022 - mae: 0.0399 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0658
Epoch 17/200
80/81 [============================>.] - ETA: 0s - loss: 0.0022 - mae: 0.0396 - mse: 0.0020
81/81 [==============================] - 1s 16ms/step - loss: 0.0022 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0399 - val_mse: 0.0021 - lr: 0.0658
Epoch 18/200
81/81 [==============================] - 1s 18ms/step - loss: 0.0021 - mae: 0.0394 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 0.0658
Epoch 19/200
75/81 [==========================>...] - ETA: 0s - loss: 0.0021 - mae: 0.0391 - mse: 0.0020
81/81 [==============================] - 1s 16ms/step - loss: 0.0021 - mae: 0.0391 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0395 - val_mse: 0.0020 - lr: 0.0658
Epoch 20/200
73/81 [==========================>...] - ETA: 0s - loss: 0.0021 - mae: 0.0389 - mse: 0.0020
81/81 [==============================] - 1s 14ms/step - loss: 0.0021 - mae: 0.0389 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0393 - val_mse: 0.0020 - lr: 0.0658
Epoch 21/200
72/81 [=========================>....] - ETA: 0s - loss: 0.0020 - mae: 0.0388 - mse: 0.0020
81/81 [==============================] - 1s 15ms/step - loss: 0.0020 - mae: 0.0388 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0391 - val_mse: 0.0020 - lr: 0.0658
Epoch 22/200
57/81 [====================>.........] - ETA: 0s - loss: 0.0020 - mae: 0.0385 - mse: 0.0020
81/81 [==============================] - 1s 16ms/step - loss: 0.0020 - mae: 0.0386 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0390 - val_mse: 0.0020 - lr: 0.0658
Epoch 23/200
54/81 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0381 - mse: 0.0019
81/81 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0385 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0389 - val_mse: 0.0020 - lr: 0.0658
Epoch 24/200
74/81 [==========================>...] - ETA: 0s - loss: 0.0020 - mae: 0.0383 - mse: 0.0020
81/81 [==============================] - 1s 16ms/step - loss: 0.0020 - mae: 0.0383 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0387 - val_mse: 0.0020 - lr: 0.0658
Epoch 25/200
67/81 [=======================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0383 - mse: 0.0020
81/81 [==============================] - 2s 21ms/step - loss: 0.0020 - mae: 0.0382 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0386 - val_mse: 0.0020 - lr: 0.0658
Epoch 26/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0380 - mse: 0.0019
81/81 [==============================] - 2s 19ms/step - loss: 0.0020 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0658
Epoch 27/200
60/81 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0381 - mse: 0.0019
81/81 [==============================] - 2s 19ms/step - loss: 0.0020 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0658
Epoch 28/200
79/81 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0378 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 16ms/step - loss: 0.0020 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0658
Epoch 29/200
81/81 [==============================] - 1s 16ms/step - loss: 0.0020 - mae: 0.0378 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0382 - val_mse: 0.0020 - lr: 0.0658
Epoch 30/200
70/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0377 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0377 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0381 - val_mse: 0.0020 - lr: 0.0658
Epoch 31/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0377 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0658
Epoch 32/200
78/81 [===========================>..] - ETA: 0s - loss: 0.0019 - mae: 0.0376 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0658
Epoch 33/200
62/81 [=====================>........] - ETA: 0s - loss: 0.0019 - mae: 0.0375 - mse: 0.0019
81/81 [==============================] - 1s 17ms/step - loss: 0.0019 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0658
Epoch 34/200
63/81 [======================>.......] - ETA: 0s - loss: 0.0019 - mae: 0.0375 - mse: 0.0019
Epoch 34: ReduceLROnPlateau reducing learning rate to 0.03288450092077255.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0658
Epoch 35/200
81/81 [==============================] - 2s 19ms/step - loss: 0.0019 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0329
Epoch 36/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0329
Epoch 37/200
76/81 [===========================>..] - ETA: 0s - loss: 0.0019 - mae: 0.0375 - mse: 0.0019
81/81 [==============================] - 1s 17ms/step - loss: 0.0019 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0378 - val_mse: 0.0020 - lr: 0.0329
Epoch 38/200
70/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0375 - mse: 0.0019
81/81 [==============================] - 1s 18ms/step - loss: 0.0019 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0378 - val_mse: 0.0020 - lr: 0.0329
Epoch 39/200
75/81 [==========================>...] - ETA: 0s - loss: 0.0019 - mae: 0.0374 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 19ms/step - loss: 0.0019 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0378 - val_mse: 0.0020 - lr: 0.0329
Epoch 40/200
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0378 - val_mse: 0.0020 - lr: 0.0329
Epoch 41/200
63/81 [======================>.......] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0378 - val_mse: 0.0020 - lr: 0.0329
Epoch 42/200
81/81 [==============================] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 2s 20ms/step - loss: 0.0019 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0378 - val_mse: 0.0020 - lr: 0.0329
Epoch 43/200
53/81 [==================>...........] - ETA: 0s - loss: 0.0019 - mae: 0.0375 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0329
Epoch 44/200
58/81 [====================>.........] - ETA: 0s - loss: 0.0019 - mae: 0.0374 - mse: 0.0019
Epoch 44: ReduceLROnPlateau reducing learning rate to 0.016442250460386276.
81/81 [==============================] - 1s 17ms/step - loss: 0.0019 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0329
Epoch 45/200
58/81 [====================>.........] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 46/200
74/81 [==========================>...] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 18ms/step - loss: 0.0019 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 47/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 2s 19ms/step - loss: 0.0019 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 48/200
78/81 [===========================>..] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 18ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 49/200
74/81 [==========================>...] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
70/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 50/200
57/81 [====================>.........] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 51/200
57/81 [====================>.........] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
78/81 [===========================>..] - ETA: 0s - loss: 0.0019 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 52/200
78/81 [===========================>..] - ETA: 0s - loss: 0.0019 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 53/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
80/81 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 54/200
80/81 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 54: ReduceLROnPlateau reducing learning rate to 0.008221125230193138.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 2s 20ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0164
Epoch 55/200
75/81 [==========================>...] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0082
Epoch 56/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 12ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0082
Epoch 57/200
65/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0375 - mse: 0.0019
81/81 [==============================] - 2s 19ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0082
Epoch 58/200
65/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 2s 21ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 0.0082
Epoch 59/200
68/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 2s 22ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0082
Epoch 60/200
72/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 2s 22ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0082
Epoch 61/200
72/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0082
Epoch 62/200
44/81 [===============>..............] - ETA: 0s - loss: 0.0019 - mae: 0.0370 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0082
Epoch 63/200
65/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0368 - mse: 0.0019
81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0082
Epoch 64/200
81/81 [==============================] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
Epoch 64: ReduceLROnPlateau reducing learning rate to 0.004110562615096569.
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0082
Epoch 65/200
63/81 [======================>.......] - ETA: 0s - loss: 0.0019 - mae: 0.0370 - mse: 0.0019
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0041
Epoch 66/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0041
Epoch 67/200
68/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 2s 19ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0041
Epoch 68/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 17ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0041
Epoch 69/200
77/81 [===========================>..] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0041
Epoch 70/200
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0041
Epoch 71/200
59/81 [====================>.........] - ETA: 0s - loss: 0.0019 - mae: 0.0370 - mse: 0.0019
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0041
Epoch 72/200
42/81 [==============>...............] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0041
Epoch 73/200
72/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0041
Epoch 74/200
59/81 [====================>.........] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0020552813075482845.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0041
Epoch 75/200
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0021
Epoch 76/200
63/81 [======================>.......] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0021
Epoch 77/200
52/81 [==================>...........] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 1s 18ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0021
Epoch 78/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0021
Epoch 79/200
64/81 [======================>.......] - ETA: 0s - loss: 0.0019 - mae: 0.0370 - mse: 0.0019
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0021
Epoch 80/200
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0021
Epoch 81/200
67/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0021
Epoch 82/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 1s 17ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0021
Epoch 83/200
61/81 [=====================>........] - ETA: 0s - loss: 0.0019 - mae: 0.0374 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0021
Epoch 84/200
57/81 [====================>.........] - ETA: 0s - loss: 0.0019 - mae: 0.0367 - mse: 0.0019
Epoch 84: ReduceLROnPlateau reducing learning rate to 0.0010276406537741423.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0021
Epoch 85/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0370 - mse: 0.0019
81/81 [==============================] - 2s 20ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0010
Epoch 86/200
77/81 [===========================>..] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 2s 20ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0010
Epoch 87/200
81/81 [==============================] - 1s 17ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0010
Epoch 88/200
69/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 2s 23ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0010
Epoch 89/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0010
Epoch 90/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0010
Epoch 91/200
73/81 [==========================>...] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0010
Epoch 92/200
81/81 [==============================] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0010
Epoch 93/200
55/81 [===================>..........] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0010
Epoch 94/200
67/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
Epoch 94: ReduceLROnPlateau reducing learning rate to 0.0005138203268870711.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 17ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 0.0010
Epoch 95/200
65/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 18ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 5.1382e-04
Epoch 96/200
73/81 [==========================>...] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 5.1382e-04
Epoch 97/200
69/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 5.1382e-04
Epoch 98/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 5.1382e-04
Epoch 99/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 5.1382e-04
Epoch 100/200
64/81 [======================>.......] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 5.1382e-04
Epoch 101/200
68/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0369 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 5.1382e-04
Epoch 102/200
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 5.1382e-04
Epoch 103/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 5.1382e-04
Epoch 104/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
Epoch 104: ReduceLROnPlateau reducing learning rate to 0.00025691016344353557.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 5.1382e-04
Epoch 105/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 2.5691e-04
Epoch 106/200
64/81 [======================>.......] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 2s 21ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 2.5691e-04
Epoch 107/200
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 2.5691e-04
Epoch 108/200
64/81 [======================>.......] - ETA: 0s - loss: 0.0019 - mae: 0.0370 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 2.5691e-04
Epoch 109/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 2.5691e-04
Epoch 110/200
65/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 17ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 2.5691e-04
Epoch 111/200
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 2.5691e-04
Epoch 112/200
75/81 [==========================>...] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 2.5691e-04
Epoch 113/200
68/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 2.5691e-04
Epoch 114/200
60/81 [=====================>........] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
Epoch 114: ReduceLROnPlateau reducing learning rate to 0.00012845508172176778.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 2.5691e-04
Epoch 115/200
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.2846e-04
Epoch 116/200
68/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 2s 20ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.2846e-04
Epoch 117/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.2846e-04
Epoch 118/200
62/81 [=====================>........] - ETA: 0s - loss: 0.0019 - mae: 0.0371 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.2846e-04
Epoch 119/200
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.2846e-04
Epoch 120/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0019 - mae: 0.0369 - mse: 0.0019
81/81 [==============================] - 1s 17ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.2846e-04
Epoch 121/200
73/81 [==========================>...] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 14ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.2846e-04
Epoch 122/200
66/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.2846e-04
Epoch 123/200
52/81 [==================>...........] - ETA: 0s - loss: 0.0019 - mae: 0.0370 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.2846e-04
Epoch 124/200
68/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0370 - mse: 0.0019
Epoch 124: ReduceLROnPlateau reducing learning rate to 6.422754086088389e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_125825-1mzm7otr\files\model-best)... Done. 0.0s
81/81 [==============================] - 1s 15ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.2846e-04
Epoch 125/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 6.4228e-05
Epoch 126/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 6.4228e-05
Epoch 127/200
69/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 6.4228e-05
Epoch 128/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 6.4228e-05
Epoch 129/200
73/81 [==========================>...] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 6.4228e-05
81/81 [==============================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 6.4228e-05
Epoch 131/200========================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 6.4228e-05
Epoch 131/200========================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 6.4228e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 3.2114e-055
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 3.2114e-05
Epoch 138/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 3.2114e-05
Epoch 139/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 3.2114e-05
Epoch 140/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 3.2114e-055
Epoch 141/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 3.2114e-05
Epoch 142/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 3.2114e-05
Epoch 143/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 3.2114e-05
Epoch 144/200
58/81 [====================>.........] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
Epoch 144: ReduceLROnPlateau reducing learning rate to 1.6056885215220973e-05.
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 3.2114e-05
Epoch 145/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.6057e-05
Epoch 146/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.6057e-05
Epoch 147/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.6057e-05
Epoch 148/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.6057e-05
Epoch 149/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.6057e-05
Epoch 150/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.6057e-05
Epoch 151/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.6057e-05
Epoch 152/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.6057e-05
Epoch 153/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.6057e-05
Epoch 154/200
67/81 [=======================>......] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
Epoch 154: ReduceLROnPlateau reducing learning rate to 1e-05.
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.6057e-05
Epoch 155/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 156/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 157/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 158/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 159/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 160/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 161/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 162/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 163/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 164/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 165/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 166/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 167/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 168/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 169/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 170/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 171/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 172/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 173/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 174/200
68/81 [========================>.....] - ETA: 0s - loss: 0.0019 - mae: 0.0372 - mse: 0.0019
Epoch 13/200=========================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 175/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 176/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 177/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 178/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 179/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 180/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 181/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 182/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 183/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 184/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 185/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 186/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 187/200
81/81 [==============================] - 0s 5ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 188/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 189/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 190/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 191/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 192/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 193/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 194/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 195/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 196/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 197/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 198/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 199/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 200/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.06576900040682378LR_[28]HN_48BS_10P_val_mseM_200epochs/model_1.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
108/108 [==============================] - 1s 4ms/step - loss: 0.3241 - mae: 0.1107 - mse: 0.0186 - val_loss: 0.1460 - val_mae: 0.0924 - val_mse: 0.0104 - lr: 0.0658
Epoch 2/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0834 - mae: 0.0923 - mse: 0.0104 - val_loss: 0.0436 - val_mae: 0.0924 - val_mse: 0.0104 - lr: 0.0658
Epoch 3/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0289 - mae: 0.0921 - mse: 0.0103 - val_loss: 0.0195 - val_mae: 0.0919 - val_mse: 0.0103 - lr: 0.0658
Epoch 4/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0918 - mse: 0.0102 - val_loss: 0.0135 - val_mae: 0.0916 - val_mse: 0.0102 - lr: 0.0658
Epoch 5/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0915 - mse: 0.0102 - val_loss: 0.0119 - val_mae: 0.0915 - val_mse: 0.0102 - lr: 0.0658
Epoch 6/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0116 - mae: 0.0914 - mse: 0.0102 - val_loss: 0.0113 - val_mae: 0.0913 - val_mse: 0.0101 - lr: 0.0658
Epoch 7/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0913 - mse: 0.0101 - val_loss: 0.0110 - val_mae: 0.0913 - val_mse: 0.0101 - lr: 0.0658
Epoch 8/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0912 - mse: 0.0101 - val_loss: 0.0108 - val_mae: 0.0912 - val_mse: 0.0101 - lr: 0.0658
Epoch 9/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0107 - mae: 0.0911 - mse: 0.0101 - val_loss: 0.0107 - val_mae: 0.0911 - val_mse: 0.0101 - lr: 0.0658
Epoch 10/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0106 - mae: 0.0910 - mse: 0.0101 - val_loss: 0.0106 - val_mae: 0.0910 - val_mse: 0.0101 - lr: 0.0658
Epoch 11/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0105 - mae: 0.0910 - mse: 0.0101 - val_loss: 0.0105 - val_mae: 0.0910 - val_mse: 0.0101 - lr: 0.0658
Epoch 12/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0104 - val_mae: 0.0909 - val_mse: 0.0101 - lr: 0.0658
Epoch 13/200=========================] - 1s 16ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
108/108 [==============================] - 0s 3ms/step - loss: 0.0104 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0104 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0658
Epoch 14/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0104 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0103 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0658
Epoch 15/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0103 - val_mae: 0.0909 - val_mse: 0.0101 - lr: 0.0658
Epoch 16/200
 83/108 [======================>.......] - ETA: 0s - loss: 0.0103 - mae: 0.0910 - mse: 0.0101
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.03288450092077255.
108/108 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0658
Epoch 17/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0329
Epoch 18/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0329
Epoch 19/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0329
Epoch 20/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0329
Epoch 21/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0329
Epoch 22/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0329
Epoch 23/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0329
Epoch 24/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0329
Epoch 25/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0102 - mae: 0.0907 - mse: 0.0101 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0329
Epoch 26/200
 96/108 [=========================>....] - ETA: 0s - loss: 0.0102 - mae: 0.0908 - mse: 0.0101
Epoch 26: ReduceLROnPlateau reducing learning rate to 0.016442250460386276.
108/108 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0907 - mse: 0.0101 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0329
Epoch 27/200
 87/108 [=======================>......] - ETA: 0s - loss: 0.0102 - mae: 0.0910 - mse: 0.0101
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 28/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0164
Epoch 29/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0164
Epoch 30/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0164
Epoch 31/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0164
Epoch 32/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0164
Epoch 33/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0164
Epoch 34/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0164
Epoch 35/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0164
Epoch 36/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0164
Epoch 37/200
 96/108 [=========================>....] - ETA: 0s - loss: 0.0101 - mae: 0.0908 - mse: 0.0101
Epoch 37: ReduceLROnPlateau reducing learning rate to 0.008221125230193138.
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0164
Epoch 38/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0082
Epoch 39/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0082
Epoch 40/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0082
Epoch 41/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0082
Epoch 42/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0082
Epoch 43/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0082
Epoch 44/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0082
Epoch 45/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0082
Epoch 46/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0082
Epoch 47/200
 99/108 [==========================>...] - ETA: 0s - loss: 0.0101 - mae: 0.0906 - mse: 0.0100
Epoch 47: ReduceLROnPlateau reducing learning rate to 0.004110562615096569.
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0082
Epoch 48/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0041
Epoch 49/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0041
Epoch 50/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0041
Epoch 51/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0041
Epoch 52/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0041
Epoch 53/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0041
Epoch 54/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0041
Epoch 55/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0041
Epoch 56/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0041
Epoch 57/200
 82/108 [=====================>........] - ETA: 0s - loss: 0.0100 - mae: 0.0904 - mse: 0.0100
Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0020552813075482845.
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0041
Epoch 58/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0021
Epoch 59/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0021
Epoch 60/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0021
Epoch 61/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0021
Epoch 62/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0021
Epoch 63/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0021
Epoch 64/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0021
Epoch 65/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0021
Epoch 66/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0021
Epoch 67/200
 82/108 [=====================>........] - ETA: 0s - loss: 0.0101 - mae: 0.0906 - mse: 0.0100
Epoch 67: ReduceLROnPlateau reducing learning rate to 0.0010276406537741423.
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0021
Epoch 68/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0010
Epoch 69/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0010
Epoch 70/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0010
Epoch 71/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0010
Epoch 72/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0010
Epoch 73/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0010
Epoch 74/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0010
Epoch 75/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0010
Epoch 76/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0010
Epoch 77/200
102/108 [===========================>..] - ETA: 0s - loss: 0.0101 - mae: 0.0907 - mse: 0.0101
Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0005138203268870711.
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0010
Epoch 78/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 79/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 80/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 81/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 82/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 83/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 84/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 85/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 86/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 87/200
107/108 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0906 - mse: 0.0100
Epoch 87: ReduceLROnPlateau reducing learning rate to 0.00025691016344353557.
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 5.1382e-04
Epoch 88/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 2.5691e-04
Epoch 89/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 2.5691e-04
Epoch 90/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 2.5691e-04
Epoch 91/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 2.5691e-04
Epoch 92/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 2.5691e-04
Epoch 93/200
108/108 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 2.5691e-04
Epoch 94/200
108/108 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 2.5691e-04
Epoch 95/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 2.5691e-04
Epoch 96/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 2.5691e-04
Epoch 97/200
 93/108 [========================>.....] - ETA: 0s - loss: 0.0101 - mae: 0.0906 - mse: 0.0100
Epoch 97: ReduceLROnPlateau reducing learning rate to 0.00012845508172176778.
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 2.5691e-04
Epoch 98/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.2846e-04
Epoch 99/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.2846e-04
Epoch 100/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.2846e-04
Epoch 101/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.2846e-04
Epoch 102/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.2846e-04
Epoch 103/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.2846e-04
Epoch 104/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.2846e-04
Epoch 105/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.2846e-04
Epoch 106/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.2846e-04
Epoch 107/200
 95/108 [=========================>....] - ETA: 0s - loss: 0.0101 - mae: 0.0906 - mse: 0.0100
Epoch 107: ReduceLROnPlateau reducing learning rate to 6.422754086088389e-05.
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.2846e-04
Epoch 108/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 6.4228e-05
Epoch 109/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 6.4228e-05
Epoch 110/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 6.4228e-05
Epoch 111/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 6.4228e-05
Epoch 112/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 6.4228e-05
Epoch 113/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 6.4228e-05
Epoch 114/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 6.4228e-05
Epoch 115/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 6.4228e-05
Epoch 116/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 6.4228e-05
Epoch 117/200
 97/108 [=========================>....] - ETA: 0s - loss: 0.0101 - mae: 0.0907 - mse: 0.0100
Epoch 117: ReduceLROnPlateau reducing learning rate to 3.2113770430441946e-05.
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 6.4228e-05
Epoch 118/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.2114e-05
Epoch 119/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.2114e-05
Epoch 120/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.2114e-05
Epoch 121/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.2114e-05
Epoch 122/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.2114e-05
Epoch 123/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.2114e-05
Epoch 124/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.2114e-05
Epoch 125/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.2114e-05
Epoch 126/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.2114e-05
Epoch 127/200
 95/108 [=========================>....] - ETA: 0s - loss: 0.0101 - mae: 0.0908 - mse: 0.0101
Epoch 127: ReduceLROnPlateau reducing learning rate to 1.6056885215220973e-05.
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.2114e-05
Epoch 128/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.6057e-05
Epoch 129/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.6057e-05
Epoch 130/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.6057e-05
Epoch 131/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.6057e-05
Epoch 132/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.6057e-05
Epoch 133/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.6057e-05
Epoch 134/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.6057e-05
Epoch 135/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.6057e-05
Epoch 136/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.6057e-05
Epoch 137/200
108/108 [==============================] - ETA: 0s - loss: 0.0101 - mae: 0.0907 - mse: 0.0100
Epoch 137: ReduceLROnPlateau reducing learning rate to 1e-05.
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.6057e-05
Epoch 138/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 139/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 140/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 141/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 142/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 143/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 144/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 145/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 146/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 147/200
108/108 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 148/200
108/108 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 149/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 150/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 151/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 152/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 153/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 154/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 155/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 156/200
108/108 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 157/200
108/108 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 158/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 159/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 160/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 161/200
108/108 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 162/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 163/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 164/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 165/200
108/108 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 166/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 167/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 168/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 169/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 170/200
108/108 [==============================] - 1s 5ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 171/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 172/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 173/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 174/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 175/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 176/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 177/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 178/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 179/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 180/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 181/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 182/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 183/200
108/108 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 184/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 185/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 186/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 187/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 188/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 189/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 190/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 191/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 192/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 193/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 194/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 195/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 196/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 197/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 198/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 199/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 200/200
108/108 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.06576900040682378LR_[28]HN_48BS_10P_val_mseM_200epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
81/81 [==============================] - 1s 3ms/step - loss: 0.4035 - mae: 0.1660 - mse: 0.0338 - val_loss: 0.2286 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0658
Epoch 2/200
81/81 [==============================] - 0s 3ms/step - loss: 0.1499 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0939 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0658
Epoch 3/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0674 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0487 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0658
Epoch 4/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0396 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0335 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0658
Epoch 5/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0302 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0282 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0658
Epoch 6/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0269 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0263 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0658
Epoch 7/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0257 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0256 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0658
Epoch 8/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0252 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0252 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0658
Epoch 9/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0249 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0251 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0658
Epoch 10/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0248 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0250 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0658
Epoch 11/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0247 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0658
Epoch 12/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0658
Epoch 13/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0658
Epoch 14/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0658
Epoch 15/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0658
Epoch 16/200
62/81 [=====================>........] - ETA: 0s - loss: 0.0244 - mae: 0.1521 - mse: 0.0243
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.03288450092077255.
81/81 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0658
Epoch 17/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0329
Epoch 18/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0329
Epoch 19/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0329
Epoch 20/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0329
Epoch 21/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0329
Epoch 22/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0329
Epoch 23/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0329
Epoch 24/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0329
Epoch 25/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0329
Epoch 26/200
68/81 [========================>.....] - ETA: 0s - loss: 0.0244 - mae: 0.1525 - mse: 0.0244
Epoch 26: ReduceLROnPlateau reducing learning rate to 0.016442250460386276.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0329
Epoch 27/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0164
Epoch 28/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0164
Epoch 29/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0164
Epoch 30/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0164
Epoch 31/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0164
Epoch 32/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0164
Epoch 33/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0164
Epoch 34/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0164
Epoch 35/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0164
Epoch 36/200
60/81 [=====================>........] - ETA: 0s - loss: 0.0245 - mae: 0.1526 - mse: 0.0244
Epoch 36: ReduceLROnPlateau reducing learning rate to 0.008221125230193138.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0164
Epoch 37/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0082
Epoch 38/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0082
Epoch 39/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0082
Epoch 40/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0082
Epoch 41/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0082
Epoch 42/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0082
Epoch 43/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0082
Epoch 44/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0082
Epoch 45/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0082
Epoch 46/200
71/81 [=========================>....] - ETA: 0s - loss: 0.0244 - mae: 0.1524 - mse: 0.0244
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.004110562615096569.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0082
Epoch 47/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0041
Epoch 48/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0041
Epoch 49/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0041
Epoch 50/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0041
Epoch 51/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0041
Epoch 52/200
81/81 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0041
Epoch 53/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0041
Epoch 54/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0041
Epoch 55/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0041
Epoch 56/200
76/81 [===========================>..] - ETA: 0s - loss: 0.0244 - mae: 0.1526 - mse: 0.0244
Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0020552813075482845.
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0041
Epoch 57/200
81/81 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0021
Epoch 58/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0021
Epoch 59/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0021
Epoch 60/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0021
Epoch 61/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0021
Epoch 62/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0021
Epoch 63/200
81/81 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0021
Epoch 64/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0021
Epoch 65/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0021
Epoch 66/200
62/81 [=====================>........] - ETA: 0s - loss: 0.0244 - mae: 0.1523 - mse: 0.0243
Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0010276406537741423.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0021
Epoch 67/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0010
Epoch 68/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0010
Epoch 69/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0010
Epoch 70/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0010
Epoch 71/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0010
Epoch 72/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0010
Epoch 73/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0010
Epoch 74/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0010
Epoch 75/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0010
Epoch 76/200
58/81 [====================>.........] - ETA: 0s - loss: 0.0245 - mae: 0.1527 - mse: 0.0245
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0005138203268870711.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0010
Epoch 77/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.1382e-04
Epoch 78/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.1382e-04
Epoch 79/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.1382e-04
Epoch 80/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.1382e-04
Epoch 81/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.1382e-04
Epoch 82/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.1382e-04
Epoch 83/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.1382e-04
Epoch 84/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.1382e-04
Epoch 85/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.1382e-04
Epoch 86/200
68/81 [========================>.....] - ETA: 0s - loss: 0.0243 - mae: 0.1522 - mse: 0.0243
Epoch 86: ReduceLROnPlateau reducing learning rate to 0.00025691016344353557.
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.1382e-04
Epoch 87/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.5691e-04
Epoch 88/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.5691e-04
Epoch 89/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.5691e-04
Epoch 90/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.5691e-04
Epoch 91/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.5691e-04
Epoch 92/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.5691e-04
Epoch 93/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.5691e-04
Epoch 94/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.5691e-04
Epoch 95/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.5691e-04
Epoch 96/200
57/81 [====================>.........] - ETA: 0s - loss: 0.0245 - mae: 0.1526 - mse: 0.0245
Epoch 96: ReduceLROnPlateau reducing learning rate to 0.00012845508172176778.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.5691e-04
Epoch 97/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.2846e-04
Epoch 98/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.2846e-04
Epoch 99/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.2846e-04
Epoch 100/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.2846e-04
Epoch 101/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.2846e-04
Epoch 102/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.2846e-04
Epoch 103/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.2846e-04
Epoch 104/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.2846e-04
Epoch 105/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.2846e-04
Epoch 106/200
81/81 [==============================] - ETA: 0s - loss: 0.0244 - mae: 0.1525 - mse: 0.0244
Epoch 106: ReduceLROnPlateau reducing learning rate to 6.422754086088389e-05.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.2846e-04
Epoch 107/200
81/81 [==============================] - 0s 6ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.4228e-05
Epoch 108/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.4228e-05
Epoch 109/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.4228e-05
Epoch 110/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.4228e-05
Epoch 111/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.4228e-05
Epoch 112/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.4228e-05
Epoch 113/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.4228e-05
Epoch 114/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.4228e-05
Epoch 115/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.4228e-05
Epoch 116/200
72/81 [=========================>....] - ETA: 0s - loss: 0.0245 - mae: 0.1528 - mse: 0.0245
Epoch 116: ReduceLROnPlateau reducing learning rate to 3.2113770430441946e-05.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.4228e-05
Epoch 117/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.2114e-05
Epoch 118/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.2114e-05
Epoch 119/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.2114e-05
Epoch 120/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.2114e-05
Epoch 121/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.2114e-05
Epoch 122/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.2114e-05
Epoch 123/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.2114e-05
Epoch 124/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.2114e-05
Epoch 125/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.2114e-05
Epoch 126/200
67/81 [=======================>......] - ETA: 0s - loss: 0.0245 - mae: 0.1527 - mse: 0.0244
Epoch 126: ReduceLROnPlateau reducing learning rate to 1.6056885215220973e-05.
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.2114e-05
Epoch 127/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.6057e-05
Epoch 128/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.6057e-05
Epoch 129/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.6057e-05
Epoch 130/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.6057e-05
Epoch 131/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.6057e-05
Epoch 132/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.6057e-05
Epoch 133/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.6057e-05
Epoch 134/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.6057e-05
Epoch 135/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.6057e-05
Epoch 136/200
74/81 [==========================>...] - ETA: 0s - loss: 0.0244 - mae: 0.1524 - mse: 0.0244
Epoch 136: ReduceLROnPlateau reducing learning rate to 1e-05.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.6057e-05
Epoch 137/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 138/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 139/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 140/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 141/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 142/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 143/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 144/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 145/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 146/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 147/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 148/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 149/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 150/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 151/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 152/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 153/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 154/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 155/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 156/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 157/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 158/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 159/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 160/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 161/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 162/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 163/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 164/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 165/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 166/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 167/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 168/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 169/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 170/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 171/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 172/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 173/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 174/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 175/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 176/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 177/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 178/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 179/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 180/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 181/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 182/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 183/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 184/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 185/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 186/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 187/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 188/200
74/81 [==========================>...] - ETA: 0s - loss: 0.0244 - mae: 0.1525 - mse: 0.0244
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 189/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 190/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 191/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 192/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 193/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 194/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 195/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 196/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 197/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 198/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 199/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 200/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.06576900040682378LR_[28]HN_48BS_10P_val_mseM_200epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
81/81 [==============================] - 1s 4ms/step - loss: 0.4166 - mae: 0.2075 - mse: 0.0502 - val_loss: 0.2443 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0658
Epoch 2/200
81/81 [==============================] - 0s 3ms/step - loss: 0.1660 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.1104 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 3/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0841 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0656 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 4/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0566 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0506 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 5/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0473 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0454 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 6/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0441 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0436 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 7/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0430 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0430 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 8/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 9/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 10/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 11/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 12/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 13/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 14/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 15/200
72/81 [=========================>....] - ETA: 0s - loss: 0.0421 - mae: 0.2024 - mse: 0.0421
Epoch 15: ReduceLROnPlateau reducing learning rate to 0.03288450092077255.
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0658
Epoch 16/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0329
Epoch 17/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0329
Epoch 18/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0329
Epoch 19/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0329
Epoch 20/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0329
Epoch 21/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0329
Epoch 22/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0329
Epoch 23/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0329
Epoch 24/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0329
Epoch 25/200
74/81 [==========================>...] - ETA: 0s - loss: 0.0421 - mae: 0.2023 - mse: 0.0421
Epoch 25: ReduceLROnPlateau reducing learning rate to 0.016442250460386276.
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0329
Epoch 26/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0164
Epoch 27/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0164
Epoch 28/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0164
Epoch 29/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0164
Epoch 30/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0164
Epoch 31/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0164
Epoch 32/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0164
Epoch 33/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0164
Epoch 34/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0164
Epoch 35/200
54/81 [===================>..........] - ETA: 0s - loss: 0.0422 - mae: 0.2027 - mse: 0.0422
Epoch 35: ReduceLROnPlateau reducing learning rate to 0.008221125230193138.
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0164
Epoch 36/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0082
Epoch 37/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0082
Epoch 38/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0082
Epoch 39/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0082
Epoch 40/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0082
Epoch 41/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0082
Epoch 42/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0082
Epoch 43/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0082
Epoch 44/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0082
Epoch 45/200
72/81 [=========================>....] - ETA: 0s - loss: 0.0423 - mae: 0.2028 - mse: 0.0423
Epoch 45: ReduceLROnPlateau reducing learning rate to 0.004110562615096569.
81/81 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0082
Epoch 46/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0041
Epoch 47/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0041
Epoch 48/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0041
Epoch 49/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0041
Epoch 50/200
81/81 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0041
Epoch 51/200
81/81 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0041
Epoch 52/200
81/81 [==============================] - 0s 4ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0041
Epoch 53/200
81/81 [==============================] - 0s 5ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0041
Epoch 54/200
81/81 [==============================] - ETA: 0s - loss: 0.0422 - mae: 0.2025 - mse: 0.0422
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0017s). Check your callbacks.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0018s). Check your callbacks.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0032s). Check your callbacks.
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05