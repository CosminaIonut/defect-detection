Epoch 1/150
42/54 [======================>.......] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
54/54 [==============================] - 3s 38ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 2/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 3/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 4/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 5/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 6/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 7/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 8/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 9/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 10/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 11/150
49/54 [==========================>...] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.046577345579862595.
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0932
Epoch 12/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0466
Epoch 13/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0466
Epoch 14/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0466
Epoch 15/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0466
Epoch 16/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0466
Epoch 17/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0466
Epoch 18/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0466
Epoch 19/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0466
Epoch 20/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0466
Epoch 21/150
51/54 [===========================>..] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.8578
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.023288672789931297.
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0466
Epoch 22/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0233
Epoch 23/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0233
Epoch 24/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0233
Epoch 25/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0233
Epoch 26/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0233
Epoch 27/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0233
Epoch 28/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0233
Epoch 29/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0233
Epoch 30/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0233
Epoch 31/150
45/54 [========================>.....] - ETA: 0s - loss: 0.8565 - mae: 0.9244 - mse: 0.8565
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.011644336394965649.
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0233
Epoch 32/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 33/150
54/54 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 34/150
54/54 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 35/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 36/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 37/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 38/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 39/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 40/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 41/150
48/54 [=========================>....] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.8581
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005822168197482824.
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 42/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 43/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 44/150
54/54 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 45/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 46/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 47/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 48/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 49/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 50/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 51/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002911084098741412.
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 52/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 53/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 54/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 55/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 56/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 57/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 58/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 59/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 60/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 61/150
42/54 [======================>.......] - ETA: 0s - loss: 0.8584 - mae: 0.9255 - mse: 0.8584
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.001455542049370706.
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 62/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 63/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 64/150
45/54 [========================>.....] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 68/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 69/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 70/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 71/150
32/54 [================>.............] - ETA: 0s - loss: 0.8572 - mae: 0.9248 - mse: 0.8572
54/54 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2777e-04
Epoch 76/150
54/54 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2777e-04
Epoch 77/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2777e-04
Epoch 78/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2777e-04
Epoch 79/150
38/54 [====================>.........] - ETA: 0s - loss: 0.8564 - mae: 0.9244 - mse: 0.8564
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6389e-04
Epoch 84/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6389e-04
Epoch 85/150
54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6389e-04
Epoch 86/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6389e-04
Epoch 87/150
48/54 [=========================>....] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6389e-04
Epoch 92/150
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8194e-04
Epoch 93/150
46/54 [========================>.....] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6389e-04
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0971e-05
Epoch 109/150
 1/54 [..............................] - ETA: 0s - loss: 0.8468 - mae: 0.9191 - mse: 0.8468
36/54 [===================>..........] - ETA: 0s - loss: 0.8568 - mae: 0.9246 - mse: 0.85688579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0971e-05
24/54 [============>.................] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5486e-05
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2743e-05
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1371e-05
50/54 [==========================>...] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1371e-05
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0033s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0033s). Check your callbacks.
54/54 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
72/72 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0932e-05
46/72 [==================>...........] - ETA: 0s - loss: 0.6420 - mae: 0.7991 - mse: 0.64206431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0466e-05
72/72 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0233e-05
72/72 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0233e-05
72/72 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0233e-05
72/72 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0116e-05
72/72 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0058e-05
72/72 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
72/72 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
72/72 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 72/150=========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 78/150=========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 82/150=========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 89/150=========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 94/150=========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 100/150========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 105/150========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 111/150========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 118/150========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 123/150========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 128/150========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 133/150========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 140/150========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
Epoch 145/150========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 14/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 14/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 22/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 31/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 39/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 45/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 53/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 61/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 69/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 77/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 85/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 91/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 99/150ained_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 105/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 112/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 120/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 128/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 135/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 141/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 148/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 148/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 148/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0033s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0033s). Check your callbacks.
Epoch 148/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 14/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 14/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 22/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 31/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 39/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 45/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 53/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 61/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 68/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 75/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 82/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 90/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 97/1500ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 104/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 111/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 118/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 124/150ined_models/models_segments_overlap-cnn_adam_0.09315468795791076LR_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 131: ReduceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 140/150duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 148/150duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 148/150duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 148/150duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0039s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0039s). Check your callbacks.
Epoch 148/150duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 14/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 22/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 31/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 39/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 47/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 55/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 55/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 63/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 80/150duceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 88/150duceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 96/150duceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 104/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 111: ReduceLROnPlateau reducing learning rate to 4.5485689042834565e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 119/150duceLROnPlateau reducing learning rate to 4.5485689042834565e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 127/150duceLROnPlateau reducing learning rate to 4.5485689042834565e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 131: ReduceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 139/150duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 147/150duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 147/150duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 147/150duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 147/150duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0028s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0028s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 14/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 14/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 22/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 31/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 39/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 47/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 55/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 63/1500duceLROnPlateau reducing learning rate to 1.1371422260708641e-05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 79/150duceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 87/150duceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 94/150duceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 101/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 109/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 117/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 123/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 131/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 139/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 147/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 147/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 147/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0040s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0040s). Check your callbacks.
Epoch 147/150uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 14/1500uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 22/1500uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 22/1500uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 31/1500uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 39/1500uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 47/1500uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 55/1500uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 63/1500uceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 77/150duceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 83/150duceLROnPlateau reducing learning rate to 0.000727771024685353.05.R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00018194275617133826..R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
Epoch 99/150duceLROnPlateau reducing learning rate to 0.00018194275617133826..R_[39]CHN_100CNNI_72BS_10P_val_mseM_150epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0015e-05
54/54 [==============================] - 0s 5ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 114/150========================] - 0s 5ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 130/150duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 138/150duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 142/150duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 150/150duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 150/150duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 150/150duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/15050duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 14/1500duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 22/1500duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 31/1500duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 38/1500duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 45/1500duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 52/1500duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 61/1500duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 68/1500duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 75/1500duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 83/1500duceLROnPlateau reducing learning rate to 2.2742844521417283e-05.752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 97/150duceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 105/150uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 113/150uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 121/150uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 128/150uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 135/150uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 142/150uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 150/150uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 150/150uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 150/150uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/15050uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 14/1500uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 22/1500uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 31/1500uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 39/1500uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 45/1500uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 52/1500uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 61/1500uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 69/1500uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 75/1500uceLROnPlateau reducing learning rate to 0.00018194275617133826..752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 88/150duceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 94/150duceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 102/150uceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 111/150uceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 118/150uceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 124/150uceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 132/150uceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 141/150uceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 145/150uceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 145/150uceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05
Epoch 145/150uceLROnPlateau reducing learning rate to 0.0003638855123426765...752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 9.0971e-05