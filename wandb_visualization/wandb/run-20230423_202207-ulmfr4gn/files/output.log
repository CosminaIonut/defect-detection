Epoch 1/200
202/243 [=======================>......] - ETA: 0s - loss: 0.8576 - mae: 0.9251 - mse: 0.8576
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.

241/243 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
243/243 [==============================] - 4s 12ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 2/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 3/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 4/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 5/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 6/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 7/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 8/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 9/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 10/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 11/200
237/243 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.046819742769002914.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0936
Epoch 12/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0468
Epoch 13/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0468
Epoch 14/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0468
Epoch 15/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0468
Epoch 16/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0468
Epoch 17/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0468
Epoch 18/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0468
Epoch 19/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0468
Epoch 20/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0468
Epoch 21/200
237/243 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.8578
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.023409871384501457.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0468
Epoch 22/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0234
Epoch 23/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0234
Epoch 24/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0234
Epoch 25/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0234
Epoch 26/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0234
Epoch 27/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0234
Epoch 28/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0234
Epoch 29/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0234
Epoch 30/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0234
Epoch 31/200
230/243 [===========================>..] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.011704935692250729.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0234
Epoch 32/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0117
Epoch 33/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0117
Epoch 34/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0117
Epoch 35/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0117
Epoch 36/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0117
Epoch 37/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0117
Epoch 38/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0117
Epoch 39/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0117
Epoch 40/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0117
Epoch 41/200
236/243 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005852467846125364.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0117
Epoch 42/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0059
Epoch 43/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0059
Epoch 44/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0059
Epoch 45/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0059
Epoch 46/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0059
Epoch 47/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0059
Epoch 48/200
 68/243 [=======>......................] - ETA: 0s - loss: 0.8590 - mae: 0.9258 - mse: 0.85908579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0059
Epoch 49/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0059
Epoch 50/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0059
Epoch 51/200
234/243 [===========================>..] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002926233923062682.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0059
Epoch 52/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 53/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 54/200
 84/243 [=========>....................] - ETA: 0s - loss: 0.8563 - mae: 0.9244 - mse: 0.8563
243/243 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
243/243 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
237/243 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
 78/243 [========>.....................] - ETA: 0s - loss: 0.8565 - mae: 0.9244 - mse: 0.85658579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.3156e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.3156e-04
125/243 [==============>...............] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.85838579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.3156e-04
147/243 [=================>............] - ETA: 0s - loss: 0.8586 - mae: 0.9256 - mse: 0.85868579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.3156e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6578e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6578e-04
231/243 [===========================>..] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6578e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6578e-04
239/243 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6578e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8289e-04
176/243 [====================>.........] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8289e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8289e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8289e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8289e-04
239/243 [============================>.] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.85838579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8289e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.1445e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.1445e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.1445e-05
233/243 [===========================>..] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.1445e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5722e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5722e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5722e-05
 34/243 [===>..........................] - ETA: 0s - loss: 0.8604 - mae: 0.9266 - mse: 0.86048579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5722e-05
 85/243 [=========>....................] - ETA: 0s - loss: 0.8557 - mae: 0.9240 - mse: 0.85578579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5722e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2861e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2861e-05
234/243 [===========================>..] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2861e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2861e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2861e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1431e-05
238/243 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1431e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1431e-05
 40/243 [===>..........................] - ETA: 0s - loss: 0.8528 - mae: 0.9225 - mse: 0.85288579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1431e-05
243/243 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 46/243 [====>.........................] - ETA: 0s - loss: 0.8624 - mae: 0.9276 - mse: 0.86248579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 35/243 [===>..........................] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
140/243 [================>.............] - ETA: 0s - loss: 0.8604 - mae: 0.9266 - mse: 0.86048579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
229/243 [===========================>..] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
205/243 [========================>.....] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 58/243 [======>.......................] - ETA: 0s - loss: 0.8572 - mae: 0.9248 - mse: 0.85728579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
296/323 [==========================>...] - ETA: 0s - loss: 0.6428 - mae: 0.7996 - mse: 0.6428  79 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
296/323 [==========================>...] - ETA: 0s - loss: 0.6428 - mae: 0.7996 - mse: 0.6428  79 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0936e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0936e-05
274/323 [========================>.....] - ETA: 0s - loss: 0.6427 - mae: 0.7996 - mse: 0.64276431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0936e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0468e-05
279/323 [========================>.....] - ETA: 0s - loss: 0.6427 - mae: 0.7996 - mse: 0.64276431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0468e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0468e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0468e-05
 53/323 [===>..........................] - ETA: 0s - loss: 0.6408 - mae: 0.7984 - mse: 0.64086431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0468e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0234e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0234e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0234e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0234e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0117e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0117e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0117e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0059e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0059e-05
217/323 [===================>..........] - ETA: 0s - loss: 0.6441 - mae: 0.8005 - mse: 0.64416431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0059e-05
145/323 [============>.................] - ETA: 0s - loss: 0.6421 - mae: 0.7992 - mse: 0.64216431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0059e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
221/323 [===================>..........] - ETA: 0s - loss: 0.6425 - mae: 0.7994 - mse: 0.64256431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
 58/323 [====>.........................] - ETA: 0s - loss: 0.6438 - mae: 0.8002 - mse: 0.64386431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
 70/323 [=====>........................] - ETA: 0s - loss: 0.6410 - mae: 0.7985 - mse: 0.64106431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
147/323 [============>.................] - ETA: 0s - loss: 0.6434 - mae: 0.8001 - mse: 0.64346431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0015e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 81/200===========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 83/200===========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 86/200===========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 89/200===========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00018288962019141763.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 94/200duceLROnPlateau reducing learning rate to 0.00018288962019141763.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 97/200duceLROnPlateau reducing learning rate to 0.00018288962019141763.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 100/200uceLROnPlateau reducing learning rate to 0.00018288962019141763.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 101: ReduceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 104/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 107/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 110/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 112/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 115/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 118/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 121/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 122/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 125/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 128/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 131/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 133/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 136/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 139/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 141/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 142/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 145/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 147/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 149/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 151/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 153/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 155/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 157/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 159/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 161/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 163/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 166/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 168/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 170/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 173/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 175/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 178/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 180/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 183/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 186/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 189/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 192/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 195/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 198/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 200/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 1/20000duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 1/20000duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0032s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0032s). Check your callbacks.
Epoch 1/20000duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/20000duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 9/20000duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 9/20000duceLROnPlateau reducing learning rate to 9.144481009570882e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 14/200duceLROnPlateau reducing learning rate to 0.046819742769002914.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 17/200duceLROnPlateau reducing learning rate to 0.046819742769002914.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 20/200duceLROnPlateau reducing learning rate to 0.046819742769002914.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 22/200duceLROnPlateau reducing learning rate to 0.046819742769002914.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 25/200duceLROnPlateau reducing learning rate to 0.046819742769002914.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 28/200duceLROnPlateau reducing learning rate to 0.046819742769002914.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 31/200duceLROnPlateau reducing learning rate to 0.046819742769002914.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 33/200duceLROnPlateau reducing learning rate to 0.046819742769002914.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 36/200duceLROnPlateau reducing learning rate to 0.046819742769002914.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 39/200duceLROnPlateau reducing learning rate to 0.046819742769002914.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 44/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 47/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 50/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 52/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 55/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 58/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 58/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 63/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 63/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 66/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 69/200duceLROnPlateau reducing learning rate to 0.005852467846125364.5.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 74/200duceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 77/200duceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 80/200duceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 82/200duceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 91/200duceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 92/200duceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 95/200duceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 98/200duceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 100/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 102/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 105/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 111/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 112/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 115/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 118/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 123/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 126/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 126/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 129/200uceLROnPlateau reducing learning rate to 0.0007315584807656705..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 131: ReduceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 134/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 137/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 140/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 142/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 145/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 148/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 151/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 153/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 155/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 158/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 161/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 164/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 167/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 170/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 173/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 175/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 178/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 180/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 182/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 185/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 188/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 190/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 196/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 1/20000duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 1/20000duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0032s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0032s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 1/20000duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 6/20000duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 9/20000duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 9/20000duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 14/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 14/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 17/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 20/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 22/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 25/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 28/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 31/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 32/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 35/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 38/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 41/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 43/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 46/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 49/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002926233923062682.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 54/200duceLROnPlateau reducing learning rate to 0.002926233923062682.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 56/200duceLROnPlateau reducing learning rate to 0.002926233923062682.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 59/200duceLROnPlateau reducing learning rate to 0.002926233923062682.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 61/200duceLROnPlateau reducing learning rate to 0.002926233923062682.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 63/200duceLROnPlateau reducing learning rate to 0.002926233923062682.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 66/200duceLROnPlateau reducing learning rate to 0.002926233923062682.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 69/200duceLROnPlateau reducing learning rate to 0.002926233923062682.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 74/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 77/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 80/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 82/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 92/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 95/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 98/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 100/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 102/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 105/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 110/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 112/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 115/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 118/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 121/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 123/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 126/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 129/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 131/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 132/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 135/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 138/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 141/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 143/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 146/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 149/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 152/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 155/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 158/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 160/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 162/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 165/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 167/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 170/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 173/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 175/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 178/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 181/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 183/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 186/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 188/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 191/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 194/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 197/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 199/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 1/20000uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 1/20000uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 1/20000uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/20000uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 6/20000uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 9/20000uceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 14/200duceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 17/200duceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 20/200duceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 22/200duceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 25/200duceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 28/200duceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 33/200duceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 33/200duceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 36/200duceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 39/200duceLROnPlateau reducing learning rate to 0.046819742769002914..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 44/200duceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 47/200duceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 52/200duceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 55/200duceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 55/200duceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 58/200duceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 63/200duceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 66/200duceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 66/200duceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 69/200duceLROnPlateau reducing learning rate to 0.005852467846125364..5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 74/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 77/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 80/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 82/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 91/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 93/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 96/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 104/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 107/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 110/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 112/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 115/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 118/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 121/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 123/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 126/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 129/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 131: ReduceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 134/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 137/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 140/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 142/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 145/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 148/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 151/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 154/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 157/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 160/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 163/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 166/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 169/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 172/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 175/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 178/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 181/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 184/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 187/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 190/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 190/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 196/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0028s vs `on_train_batch_end` time: 0.0036s). Check your callbacks.
Epoch 1/20000duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 1/20000duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_202207-ulmfr4gn\files\model-best)... Done. 0.0s
Epoch 6/20000duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 9/20000duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 14/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 17/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 20/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 22/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 25/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 28/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 31/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 33/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 36/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 39/200duceLROnPlateau reducing learning rate to 0.046819742769002914.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005852467846125364.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 44/200duceLROnPlateau reducing learning rate to 0.005852467846125364.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 47/200duceLROnPlateau reducing learning rate to 0.005852467846125364.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 50/200duceLROnPlateau reducing learning rate to 0.005852467846125364.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 52/200duceLROnPlateau reducing learning rate to 0.005852467846125364.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 55/200duceLROnPlateau reducing learning rate to 0.005852467846125364.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 58/200duceLROnPlateau reducing learning rate to 0.005852467846125364.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 61/200duceLROnPlateau reducing learning rate to 0.005852467846125364.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 63/200duceLROnPlateau reducing learning rate to 0.005852467846125364.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 66/200duceLROnPlateau reducing learning rate to 0.005852467846125364.05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 74/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 77/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 77/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 82/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 91/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 93/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 96/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 101: ReduceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 104/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 107/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 110/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 112/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 115/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 118/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 118/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 123/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 126/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 126/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 129/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 131: ReduceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 134/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 137/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 140/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 142/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 145/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 148/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 151/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 154/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 154/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 160/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 160/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 163/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 166/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 169/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 172/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 174/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 177/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 180/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 183/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 185/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 188/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 190/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 195/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
Epoch 198/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.3156e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0030s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0030s). Check your callbacks.
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/200rained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 9/200rained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 9/200rained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 14/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 17/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 17/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 20/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 22/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 25/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 28/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 31/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 33/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 36/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 39/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 44/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 46/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 49/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002926233923062682.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 54/200duceLROnPlateau reducing learning rate to 0.002926233923062682.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 57/200duceLROnPlateau reducing learning rate to 0.002926233923062682.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 59/200duceLROnPlateau reducing learning rate to 0.002926233923062682.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.001463116961531341.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 64/200duceLROnPlateau reducing learning rate to 0.001463116961531341.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 67/200duceLROnPlateau reducing learning rate to 0.001463116961531341.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 70/200duceLROnPlateau reducing learning rate to 0.001463116961531341.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 74/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 77/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 80/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 82/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 92/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 95/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 98/200duceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 100/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 102/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 105/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 110/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 112/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 115/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 118/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 121/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 123/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 126/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 129/200uceLROnPlateau reducing learning rate to 0.0007315584807656705.2628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 131: ReduceLROnPlateau reducing learning rate to 1.1430601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 134/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 137/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 139/200duceLROnPlateau reducing learning rate to 1.1430601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 141: ReduceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 144/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 147/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 150/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 152/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 155/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 157/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 160/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 163/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 166/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 168/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 171/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 173/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 176/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 178/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 180/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 183/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 186/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 188/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 191/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 194/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 196/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 199/200duceLROnPlateau reducing learning rate to 1e-05.601261963602e-05.28LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_6.h5al_mse: 0.6442 - lr: 7.3156e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/200rained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 6/200rained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 9/200rained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 14/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 17/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 17/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 22/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 25/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 25/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 28/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 31/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 33/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 36/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 39/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 44/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 47/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 52/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 52/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 55/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 58/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 61/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 62/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 65/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 68/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 71/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 73/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 76/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 79/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 81/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 83/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 86/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 89/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00018288962019141763.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 94/200duceLROnPlateau reducing learning rate to 0.00018288962019141763.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 97/200duceLROnPlateau reducing learning rate to 0.00018288962019141763.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 100/200uceLROnPlateau reducing learning rate to 0.00018288962019141763.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 104/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 104/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 107/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 110/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 112/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 115/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 118/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 121/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 123/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 126/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 129/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 131/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 132/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 135/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 138/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 140/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 145/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 148/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 151/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 154/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 157/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 160/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 160/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 163/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 166/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 168/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 171/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 177/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 183/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 189/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 198/200duceLROnPlateau reducing learning rate to 9.144481009570882e-05.628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_7.h5al_mse: 0.6442 - lr: 7.3156e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200rained_models/models_segments_overlap-cnn_rmsprop_0.09363948311192628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 14/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 20/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 25/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 33/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 39/200duceLROnPlateau reducing learning rate to 0.046819742769002914.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 44/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 47/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 55/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
  1/243 [..............................] - ETA: 0s - loss: 0.0064 - mae: 0.0695 - mse: 0.0064
Epoch 61/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0029
Epoch 59/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0029
Epoch 60/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0029
Epoch 61/200duceLROnPlateau reducing learning rate to 0.005852467846125364.92628LR_[516]CHN_20CNNI_16BS_10P_val_lossM_200epochs/model_8.h5al_mse: 0.6442 - lr: 7.3156e-04
Epoch 64/200====================>......] - ETA: 0s - loss: 0.0076 - mae: 0.0754 - mse: 0.0076
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.001463116961531341.
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0029
Epoch 62/200
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0015
Epoch 63/200
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0015
Epoch 64/200====================>......] - ETA: 0s - loss: 0.0076 - mae: 0.0754 - mse: 0.0076
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0015
Epoch 65/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0015
Epoch 66/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0015
Epoch 67/200
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0015
Epoch 68/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0015
Epoch 69/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0015
Epoch 70/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0015
Epoch 71/200
212/243 [=========================>....] - ETA: 0s - loss: 0.0075 - mae: 0.0750 - mse: 0.0075
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007315584807656705.
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0015
Epoch 72/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 7.3156e-04
Epoch 73/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 7.3156e-04
Epoch 74/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 7.3156e-04
Epoch 75/200
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 7.3156e-04
Epoch 76/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 7.3156e-04
Epoch 77/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 7.3156e-04
Epoch 78/200
104/243 [===========>..................] - ETA: 0s - loss: 0.0074 - mae: 0.0739 - mse: 0.0074
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 7.3156e-04
Epoch 80/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 7.3156e-04
Epoch 81/200
188/243 [======================>.......] - ETA: 0s - loss: 0.0076 - mae: 0.0754 - mse: 0.0076
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.00036577924038283527.
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 7.3156e-04
Epoch 82/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 3.6578e-04
Epoch 83/200
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 3.6578e-04
Epoch 84/200
 81/243 [=========>....................] - ETA: 0s - loss: 0.0075 - mae: 0.0753 - mse: 0.0075
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 3.6578e-04
Epoch 86/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 3.6578e-04
Epoch 87/200
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 3.6578e-04
Epoch 88/200
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 3.6578e-04
Epoch 89/200
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 3.6578e-04
Epoch 90/200
  1/243 [..............................] - ETA: 0s - loss: 0.0085 - mae: 0.0819 - mse: 0.0085
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 3.6578e-04
Epoch 92/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.8289e-04
Epoch 93/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.8289e-04
Epoch 94/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.8289e-04
Epoch 95/200
232/243 [===========================>..] - ETA: 0s - loss: 0.0075 - mae: 0.0750 - mse: 0.0075
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.8289e-04
Epoch 99/200
187/243 [======================>.......] - ETA: 0s - loss: 0.0075 - mae: 0.0748 - mse: 0.0075
146/243 [=================>............] - ETA: 0s - loss: 0.0075 - mae: 0.0747 - mse: 0.00750076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.8289e-04
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 9.1445e-05
191/243 [======================>.......] - ETA: 0s - loss: 0.0076 - mae: 0.0750 - mse: 0.00760076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 9.1445e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.5722e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 2.2861e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 2.2861e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 2.2861e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1431e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
 38/243 [===>..........................] - ETA: 0s - loss: 0.0075 - mae: 0.0747 - mse: 0.00750076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.0000e-05