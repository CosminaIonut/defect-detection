wandb: WARNING The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.
Epoch 1/20
103/122 [========================>.....] - ETA: 0s - loss: 0.0460 - mae: 0.0544 - mse: 0.0079
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_114921-pfr2vxeu\files\model-best)... Done. 0.0s
122/122 [==============================] - 2s 14ms/step - loss: 0.0394 - mae: 0.0521 - mse: 0.0071 - val_loss: 0.0025 - val_mae: 0.0404 - val_mse: 0.0021 - lr: 0.0368
Epoch 2/20
117/122 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0387 - mse: 0.0022
122/122 [==============================] - 1s 11ms/step - loss: 0.0026 - mae: 0.0386 - mse: 0.0022 - val_loss: 0.0024 - val_mae: 0.0365 - val_mse: 0.0020 - lr: 0.0368
Epoch 3/20
122/122 [==============================] - ETA: 0s - loss: 0.0023 - mae: 0.0374 - mse: 0.0020
122/122 [==============================] - 1s 11ms/step - loss: 0.0023 - mae: 0.0374 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0367 - val_mse: 0.0019 - lr: 0.0368
Epoch 4/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0024 - mae: 0.0376 - mse: 0.0022 - val_loss: 0.0032 - val_mae: 0.0497 - val_mse: 0.0031 - lr: 0.0368
Epoch 5/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0363 - mse: 0.0019 - val_loss: 0.0022 - val_mae: 0.0398 - val_mse: 0.0020 - lr: 0.0368
Epoch 6/20
107/122 [=========================>....] - ETA: 0s - loss: 0.0045 - mae: 0.0430 - mse: 0.0033
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.018414722755551338.
122/122 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0423 - mse: 0.0032 - val_loss: 0.0025 - val_mae: 0.0431 - val_mse: 0.0023 - lr: 0.0368
Epoch 7/20
120/122 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0355 - mse: 0.0017
122/122 [==============================] - 1s 10ms/step - loss: 0.0019 - mae: 0.0355 - mse: 0.0017 - val_loss: 0.0020 - val_mae: 0.0350 - val_mse: 0.0018 - lr: 0.0184
Epoch 8/20
 72/122 [================>.............] - ETA: 0s - loss: 0.0019 - mae: 0.0355 - mse: 0.0018
122/122 [==============================] - 1s 9ms/step - loss: 0.0019 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0353 - val_mse: 0.0018 - lr: 0.0184
Epoch 9/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0352 - mse: 0.0017 - val_loss: 0.0021 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 0.0184
Epoch 10/20
119/122 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0352 - mse: 0.0017
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.009207361377775669.
122/122 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0351 - mse: 0.0017 - val_loss: 0.0020 - val_mae: 0.0344 - val_mse: 0.0018 - lr: 0.0184
Epoch 11/20
121/122 [============================>.] - ETA: 0s - loss: 0.0018 - mae: 0.0350 - mse: 0.0017
122/122 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0350 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0358 - val_mse: 0.0017 - lr: 0.0092
Epoch 12/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0349 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0365 - val_mse: 0.0018 - lr: 0.0092
Epoch 13/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0348 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0370 - val_mse: 0.0018 - lr: 0.0092
Epoch 14/20
103/122 [========================>.....] - ETA: 0s - loss: 0.0018 - mae: 0.0348 - mse: 0.0017
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0046036806888878345.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_114921-pfr2vxeu\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 9ms/step - loss: 0.0018 - mae: 0.0348 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0356 - val_mse: 0.0017 - lr: 0.0092
Epoch 15/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0347 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0351 - val_mse: 0.0017 - lr: 0.0046
Epoch 16/20
117/122 [===========================>..] - ETA: 0s - loss: 0.0018 - mae: 0.0347 - mse: 0.0017
122/122 [==============================] - 2s 14ms/step - loss: 0.0018 - mae: 0.0347 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0355 - val_mse: 0.0017 - lr: 0.0046
Epoch 17/20
119/122 [============================>.] - ETA: 0s - loss: 0.0018 - mae: 0.0347 - mse: 0.0017
Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0023018403444439173.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_114921-pfr2vxeu\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0347 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0353 - val_mse: 0.0017 - lr: 0.0046
Epoch 18/20
122/122 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0347 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0361 - val_mse: 0.0017 - lr: 0.0023
Epoch 19/20
 98/122 [=======================>......] - ETA: 0s - loss: 0.0017 - mae: 0.0344 - mse: 0.0016
122/122 [==============================] - 1s 12ms/step - loss: 0.0018 - mae: 0.0347 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0355 - val_mse: 0.0017 - lr: 0.0023
Epoch 20/20
107/122 [=========================>....] - ETA: 0s - loss: 0.0018 - mae: 0.0348 - mse: 0.0017
Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0011509201722219586.
122/122 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0347 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0352 - val_mse: 0.0017 - lr: 0.0023
>Saved ../trained_models/severity/models__adam_0.03682944402851565LR_[27]HN_32BS_3P_val_lossM_20epochs/model_1.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
162/162 [==============================] - 1s 4ms/step - loss: 0.0377 - mae: 0.0978 - mse: 0.0133 - val_loss: 0.0121 - val_mae: 0.0970 - val_mse: 0.0118 - lr: 0.0368
Epoch 2/20
162/162 [==============================] - 1s 3ms/step - loss: 0.0106 - mae: 0.0911 - mse: 0.0102 - val_loss: 0.0103 - val_mae: 0.0902 - val_mse: 0.0099 - lr: 0.0368
Epoch 3/20
162/162 [==============================] - 1s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0099 - val_loss: 0.0106 - val_mae: 0.0902 - val_mse: 0.0102 - lr: 0.0368
Epoch 4/20
162/162 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0104 - val_mae: 0.0902 - val_mse: 0.0101 - lr: 0.0368
Epoch 5/20
162/162 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0368
Epoch 6/20
162/162 [==============================] - 0s 3ms/step - loss: 0.0099 - mae: 0.0904 - mse: 0.0097 - val_loss: 0.0100 - val_mae: 0.0902 - val_mse: 0.0098 - lr: 0.0368
Epoch 7/20
162/162 [==============================] - 0s 3ms/step - loss: 0.0099 - mae: 0.0904 - mse: 0.0097 - val_loss: 0.0099 - val_mae: 0.0906 - val_mse: 0.0098 - lr: 0.0368
Epoch 8/20
106/162 [==================>...........] - ETA: 0s - loss: 0.0099 - mae: 0.0907 - mse: 0.0098
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.018414722755551338.
162/162 [==============================] - 0s 2ms/step - loss: 0.0098 - mae: 0.0904 - mse: 0.0097 - val_loss: 0.0099 - val_mae: 0.0906 - val_mse: 0.0098 - lr: 0.0368
Epoch 9/20
162/162 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0184
Epoch 10/20
162/162 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0098 - val_mae: 0.0906 - val_mse: 0.0098 - lr: 0.0184
Epoch 11/20
162/162 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0184
Epoch 12/20
150/162 [==========================>...] - ETA: 0s - loss: 0.0097 - mae: 0.0905 - mse: 0.0096
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.009207361377775669.
162/162 [==============================] - 0s 2ms/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0184
Epoch 13/20
162/162 [==============================] - 0s 2ms/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0092
Epoch 14/20
162/162 [==============================] - 1s 3ms/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0097 - lr: 0.0092
Epoch 15/20
146/162 [==========================>...] - ETA: 0s - loss: 0.0096 - mae: 0.0902 - mse: 0.0095
Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0046036806888878345.
162/162 [==============================] - 0s 2ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0098 - val_mae: 0.0904 - val_mse: 0.0097 - lr: 0.0092
Epoch 16/20
162/162 [==============================] - 0s 2ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0046
Epoch 17/20
162/162 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0046
Epoch 18/20
149/162 [==========================>...] - ETA: 0s - loss: 0.0096 - mae: 0.0903 - mse: 0.0096
Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0023018403444439173.
162/162 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0903 - val_mse: 0.0096 - lr: 0.0046
Epoch 19/20
162/162 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0023
Epoch 20/20
162/162 [==============================] - 1s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0023
>Saved ../trained_models/severity/models__adam_0.03682944402851565LR_[27]HN_32BS_3P_val_lossM_20epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
Epoch 1/20
122/122 [==============================] - 1s 4ms/step - loss: 0.0593 - mae: 0.1555 - mse: 0.0270 - val_loss: 0.0250 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0368
Epoch 2/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0249 - mae: 0.1525 - mse: 0.0247 - val_loss: 0.0250 - val_mae: 0.1532 - val_mse: 0.0249 - lr: 0.0368
Epoch 3/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0368
Epoch 4/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0368
Epoch 5/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0368
Epoch 6/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0368
Epoch 7/20
113/122 [==========================>...] - ETA: 0s - loss: 0.0244 - mae: 0.1526 - mse: 0.0243
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.018414722755551338.
122/122 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0252 - val_mae: 0.1532 - val_mse: 0.0250 - lr: 0.0368
Epoch 8/20
114/122 [===========================>..] - ETA: 0s - loss: 0.0242 - mae: 0.1525 - mse: 0.02410242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0184
Epoch 9/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0184
Epoch 10/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0184
Epoch 11/20
102/122 [========================>.....] - ETA: 0s - loss: 0.0242 - mae: 0.1524 - mse: 0.0241
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.009207361377775669.
122/122 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0184
Epoch 12/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0092
Epoch 13/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0046
Epoch 14/20
104/122 [========================>.....] - ETA: 0s - loss: 0.0242 - mae: 0.1524 - mse: 0.0241
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0046036806888878345.
122/122 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0092
Epoch 15/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0046
Epoch 16/20
122/122 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0046
Epoch 17/20
100/122 [=======================>......] - ETA: 0s - loss: 0.0242 - mae: 0.1526 - mse: 0.0241
Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0023018403444439173.
122/122 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0046
Epoch 18/20
122/122 [==============================] - 0s 2ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0023
Epoch 19/20
 93/122 [=====================>........] - ETA: 0s - loss: 0.0243 - mae: 0.1529 - mse: 0.0242
122/122 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0023
Epoch 20/20
104/122 [========================>.....] - ETA: 0s - loss: 0.0242 - mae: 0.1526 - mse: 0.0241
Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0011509201722219586.
122/122 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0023
>Saved ../trained_models/severity/models__adam_0.03682944402851565LR_[27]HN_32BS_3P_val_lossM_20epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
122/122 [==============================] - 1s 3ms/step - loss: 0.0755 - mae: 0.2038 - mse: 0.0443 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0368
Epoch 2/20
 25/122 [=====>........................] - ETA: 0s - loss: 0.0426 - mae: 0.2024 - mse: 0.0424
122/122 [==============================] - 0s 2ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0368
Epoch 6/20
122/122 [==============================] - 0s 2ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0368
Epoch 7/20
121/122 [============================>.] - ETA: 0s - loss: 0.0421 - mae: 0.2025 - mse: 0.0420
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.018414722755551338.
122/122 [==============================] - 0s 3ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0368
Epoch 8/20
118/122 [============================>.] - ETA: 0s - loss: 0.0420 - mae: 0.2026 - mse: 0.0419
122/122 [==============================] - 0s 3ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0184
Epoch 13/20
 96/122 [======================>.......] - ETA: 0s - loss: 0.0420 - mae: 0.2027 - mse: 0.0419
Epoch 13: ReduceLROnPlateau reducing learning rate to 0.009207361377775669.
122/122 [==============================] - 0s 3ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0184
Epoch 14/20
 28/122 [=====>........................] - ETA: 0s - loss: 0.0421 - mae: 0.2029 - mse: 0.0420
122/122 [==============================] - 0s 3ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0046
Epoch 19/20
 77/122 [=================>............] - ETA: 0s - loss: 0.0419 - mae: 0.2025 - mse: 0.0419
122/122 [==============================] - 0s 3ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0046
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
122/122 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0368
122/122 [==============================] - 0s 3ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0184
122/122 [==============================] - 0s 2ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0046
122/122 [==============================] - 0s 2ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0046
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0036s). Check your callbacks.
122/122 [==============================] - 0s 4ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0368
122/122 [==============================] - 0s 3ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0092
122/122 [==============================] - 0s 3ms/step - loss: 0.0925 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0929 - lr: 0.0023
122/122 [==============================] - 0s 3ms/step - loss: 0.0925 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0929 - lr: 0.0023
122/122 [==============================] - 0s 3ms/step - loss: 0.0925 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0929 - lr: 0.0023
122/122 [==============================] - 1s 4ms/step - loss: 0.1562 - mae: 0.3525 - mse: 0.1257 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0368
122/122 [==============================] - 0s 3ms/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0184
122/122 [==============================] - 0s 4ms/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0046
122/122 [==============================] - 0s 3ms/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0012
122/122 [==============================] - 0s 3ms/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0012
121/122 [============================>.] - ETA: 0s - loss: 0.1632 - mae: 0.4025 - mse: 0.16321254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0012
113/122 [==========================>...] - ETA: 0s - loss: 0.1632 - mae: 0.4026 - mse: 0.16321632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0184
122/122 [==============================] - 0s 3ms/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0046
122/122 [==============================] - 0s 3ms/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0012
122/122 [==============================] - 0s 3ms/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0012
121/122 [============================>.] - ETA: 0s - loss: 0.2060 - mae: 0.4526 - mse: 0.20601632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0012
113/122 [==========================>...] - ETA: 0s - loss: 0.2059 - mae: 0.4525 - mse: 0.20592059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0368
 31/122 [======>.......................] - ETA: 0s - loss: 0.2071 - mae: 0.4538 - mse: 0.20712059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0092
122/122 [==============================] - 0s 3ms/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0023
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0023
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0023