Epoch 1/200
68/70 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
70/70 [==============================] - 2s 23ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0999
Epoch 2/200
41/70 [================>.............] - ETA: 0s - loss: 0.8573 - mae: 0.9248 - mse: 0.8573
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0499577596783638.
70/70 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0999
Epoch 3/200
63/70 [==========================>...] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0249788798391819.
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0500
Epoch 4/200
66/70 [===========================>..] - ETA: 0s - loss: 0.8584 - mae: 0.9254 - mse: 0.8584
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.01248943991959095.
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0250
Epoch 5/200
66/70 [===========================>..] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.006244719959795475.
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0125
Epoch 6/200
63/70 [==========================>...] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0031223599798977375.
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0062
Epoch 7/200
62/70 [=========================>....] - ETA: 0s - loss: 0.8569 - mae: 0.9246 - mse: 0.8569
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0015611799899488688.
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0031
Epoch 8/200
68/70 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.8580
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0007805899949744344.
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0016
Epoch 9/200
55/70 [======================>.......] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003902949974872172.
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.8059e-04
Epoch 10/200
62/70 [=========================>....] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001951474987436086.
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.9029e-04
Epoch 11/200
54/70 [======================>.......] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
Epoch 11: ReduceLROnPlateau reducing learning rate to 9.75737493718043e-05.
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.9515e-04
Epoch 12/200
62/70 [=========================>....] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.878687468590215e-05.
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.7574e-05
Epoch 13/200
56/70 [=======================>......] - ETA: 0s - loss: 0.8589 - mae: 0.9257 - mse: 0.8589
Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4393437342951074e-05.
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.8787e-05
Epoch 14/200
62/70 [=========================>....] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 14: ReduceLROnPlateau reducing learning rate to 1.2196718671475537e-05.
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.4393e-05
Epoch 15/200
68/70 [============================>.] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-05.
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.2197e-05
Epoch 16/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 17/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 18/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 20/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 22/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 24/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 28/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 30/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 32/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 33/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 35/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 37/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 38/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 39/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 41/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 42/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 43/200
70/70 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 44/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 45/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 48/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 49/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 51/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 52/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 53/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 54/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 55/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 56/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 57/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 58/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 59/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 60/200
62/70 [=========================>....] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 64/200
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 65/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 66/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 67/200
17/70 [======>.......................] - ETA: 0s - loss: 0.8601 - mae: 0.9264 - mse: 0.8601
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 71/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 72/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 73/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 74/200
 1/70 [..............................] - ETA: 0s - loss: 0.8519 - mae: 0.9219 - mse: 0.8519
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 78/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 79/200
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 80/200
48/70 [===================>..........] - ETA: 0s - loss: 0.8569 - mae: 0.9247 - mse: 0.8569
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 85/200
60/70 [========================>.....] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 92/200
63/70 [==========================>...] - ETA: 0s - loss: 0.8584 - mae: 0.9254 - mse: 0.8584
65/70 [==========================>...] - ETA: 0s - loss: 0.8576 - mae: 0.9251 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
49/70 [====================>.........] - ETA: 0s - loss: 0.8592 - mae: 0.9259 - mse: 0.85928579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
54/70 [======================>.......] - ETA: 0s - loss: 0.8595 - mae: 0.9261 - mse: 0.85958579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
70/70 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0021s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0021s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
93/93 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0125e-05
93/93 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 9.7574e-05
93/93 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
87/93 [===========================>..] - ETA: 0s - loss: 0.6435 - mae: 0.8001 - mse: 0.64356431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
73/93 [======================>.......] - ETA: 0s - loss: 0.6432 - mae: 0.7999 - mse: 0.64326431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
16/93 [====>.........................] - ETA: 0s - loss: 0.6426 - mae: 0.7994 - mse: 0.64266431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
79/93 [========================>.....] - ETA: 0s - loss: 0.6437 - mae: 0.8002 - mse: 0.64376431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
93/93 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 107/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 111/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 120/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 126/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 130/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 141/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 145/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 155/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 160/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 165/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 169/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 174/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 179/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 183/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 188/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 193/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 200/200========================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 16/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 23/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 30/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 156/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 163/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 177/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 184/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 184/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 197/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
70/70 [==============================] - ETA: 0s - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
70/70 [==============================] - ETA: 0s - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0039s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0039s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 16/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 23/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 30/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 156/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 156/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 163/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 181/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 188/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 194/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0046s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0046s). Check your callbacks.
Epoch 200/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 2/20000uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 11/200uceLROnPlateau reducing learning rate to 0.0015611799899488688..2 - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 41/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 48/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 55/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 62/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 76/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 83/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 111/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 132/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 139/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 153/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 160/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 160/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 173/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 180/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 194/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 194/200uceLROnPlateau reducing learning rate to 1.2196718671475537e-05. - mse: 0.3328 431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
70/70 [==============================] - 0s 5ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
70/70 [==============================] - 0s 5ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0034s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0034s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_183247-wkqlatw2\files\model-best)... Done. 0.0s
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.006244719959795475. 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 16/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 23/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 30/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 134/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 141/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 153/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 160/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 166/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 173/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 180/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 191/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 197/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05..4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
50/70 [====================>.........] - ETA: 0s - loss: 0.0779 - mae: 0.2757 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
50/70 [====================>.........] - ETA: 0s - loss: 0.0779 - mae: 0.2757 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0033s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0033s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 16/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 23/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 30/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 156/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 156/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 162/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 168/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 174/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 180/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 186/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 193/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200duceLROnPlateau reducing learning rate to 0.006244719959795475.5.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 16/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 23/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 30/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 134/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 141/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 153/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 160/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 166/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 177/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 184/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 190/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 196/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.7 - mse: 0.0779 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
70/70 [==============================] - ETA: 0s - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
70/70 [==============================] - ETA: 0s - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0036s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0036s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200duceLROnPlateau reducing learning rate to 0.006244719959795475.752 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 16/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 23/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 30/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 156/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 163/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 163/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 177/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 183/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 188/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 194/200uceLROnPlateau reducing learning rate to 4.878687468590215e-05.2 - mse: 0.0076 277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.0999155177021081LR_[38]CHN_20CNNI_56BS_1P_val_lossM_200epochs/model_9.h5 - val_mse: 0.2272 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.0999155177021081LR_[38]CHN_20CNNI_56BS_1P_val_lossM_200epochs/model_9.h5 - val_mse: 0.2272 - lr: 1.0000e-05