Epoch 1/200
 86/122 [====================>.........] - ETA: 0s - loss: 0.0478 - mae: 0.0496 - mse: 0.0066
122/122 [==============================] - 3s 17ms/step - loss: 0.0347 - mae: 0.0465 - mse: 0.0053 - val_loss: 0.0029 - val_mae: 0.0458 - val_mse: 0.0026 - lr: 0.0970
Epoch 2/200
116/122 [===========================>..] - ETA: 0s - loss: 0.0032 - mae: 0.0392 - mse: 0.0024
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_122123-cy4o6agr\files\model-best)... Done. 0.0s
122/122 [==============================] - 2s 13ms/step - loss: 0.0031 - mae: 0.0391 - mse: 0.0024 - val_loss: 0.0022 - val_mae: 0.0365 - val_mse: 0.0019 - lr: 0.0970
Epoch 3/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0378 - mse: 0.0021 - val_loss: 0.0023 - val_mae: 0.0352 - val_mse: 0.0021 - lr: 0.0970
Epoch 4/200
122/122 [==============================] - 2s 13ms/step - loss: 0.0021 - mae: 0.0364 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0353 - val_mse: 0.0018 - lr: 0.0970
Epoch 5/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0358 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0345 - val_mse: 0.0019 - lr: 0.0970
Epoch 6/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0352 - mse: 0.0017 - val_loss: 0.0020 - val_mae: 0.0342 - val_mse: 0.0019 - lr: 0.0970
Epoch 7/200
118/122 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0350 - mse: 0.0017
122/122 [==============================] - 2s 15ms/step - loss: 0.0019 - mae: 0.0351 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0345 - val_mse: 0.0018 - lr: 0.0970
Epoch 8/200
 99/122 [=======================>......] - ETA: 0s - loss: 0.0018 - mae: 0.0349 - mse: 0.0017
122/122 [==============================] - 1s 12ms/step - loss: 0.0018 - mae: 0.0348 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0350 - val_mse: 0.0017 - lr: 0.0970
Epoch 9/200
122/122 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0347 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0352 - val_mse: 0.0017 - lr: 0.0970
Epoch 10/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0345 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0343 - val_mse: 0.0017 - lr: 0.0970
Epoch 11/200
102/122 [========================>.....] - ETA: 0s - loss: 0.0017 - mae: 0.0345 - mse: 0.0017
122/122 [==============================] - 2s 15ms/step - loss: 0.0017 - mae: 0.0344 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0353 - val_mse: 0.0017 - lr: 0.0970
Epoch 12/200
108/122 [=========================>....] - ETA: 0s - loss: 0.0017 - mae: 0.0342 - mse: 0.0016
122/122 [==============================] - 2s 14ms/step - loss: 0.0017 - mae: 0.0343 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0344 - val_mse: 0.0017 - lr: 0.0970
Epoch 13/200
121/122 [============================>.] - ETA: 0s - loss: 0.0018 - mae: 0.0343 - mse: 0.0017
122/122 [==============================] - 2s 15ms/step - loss: 0.0018 - mae: 0.0343 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0349 - val_mse: 0.0017 - lr: 0.0970
Epoch 14/200
122/122 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0342 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0339 - val_mse: 0.0017 - lr: 0.0970
Epoch 15/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0340 - mse: 0.0016 - val_loss: 0.0019 - val_mae: 0.0376 - val_mse: 0.0019 - lr: 0.0970
Epoch 16/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0340 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0353 - val_mse: 0.0017 - lr: 0.0970
Epoch 17/200
108/122 [=========================>....] - ETA: 0s - loss: 0.0017 - mae: 0.0339 - mse: 0.0016
Epoch 17: ReduceLROnPlateau reducing learning rate to 0.04851161316037178.
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0339 - mse: 0.0016 - val_loss: 0.0019 - val_mae: 0.0375 - val_mse: 0.0018 - lr: 0.0970
Epoch 18/200
 94/122 [======================>.......] - ETA: 0s - loss: 0.0017 - mae: 0.0337 - mse: 0.0016
122/122 [==============================] - 2s 14ms/step - loss: 0.0017 - mae: 0.0338 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0349 - val_mse: 0.0017 - lr: 0.0485
Epoch 19/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0338 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0339 - val_mse: 0.0017 - lr: 0.0485
Epoch 20/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0338 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0343 - val_mse: 0.0017 - lr: 0.0485
Epoch 21/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0337 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0339 - val_mse: 0.0017 - lr: 0.0485
Epoch 22/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0337 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0335 - val_mse: 0.0017 - lr: 0.0485
Epoch 23/200
101/122 [=======================>......] - ETA: 0s - loss: 0.0017 - mae: 0.0337 - mse: 0.0016
122/122 [==============================] - 2s 14ms/step - loss: 0.0017 - mae: 0.0336 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0340 - val_mse: 0.0017 - lr: 0.0485
Epoch 24/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0336 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0335 - val_mse: 0.0017 - lr: 0.0485
Epoch 25/200
 88/122 [====================>.........] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.0016
122/122 [==============================] - 1s 12ms/step - loss: 0.0016 - mae: 0.0335 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0346 - val_mse: 0.0017 - lr: 0.0485
Epoch 26/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0336 - mse: 0.0016 - val_loss: 0.0019 - val_mae: 0.0333 - val_mse: 0.0018 - lr: 0.0485
Epoch 27/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0335 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0339 - val_mse: 0.0016 - lr: 0.0485
Epoch 28/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0335 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0339 - val_mse: 0.0017 - lr: 0.0485
Epoch 29/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0339 - val_mse: 0.0017 - lr: 0.0485
Epoch 30/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0335 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0017 - lr: 0.0485
Epoch 31/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0333 - val_mse: 0.0017 - lr: 0.0485
Epoch 32/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0335 - val_mse: 0.0017 - lr: 0.0485
Epoch 33/200
101/122 [=======================>......] - ETA: 0s - loss: 0.0017 - mae: 0.0335 - mse: 0.0016
Epoch 33: ReduceLROnPlateau reducing learning rate to 0.02425580658018589.
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0019 - val_mae: 0.0331 - val_mse: 0.0018 - lr: 0.0485
Epoch 34/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0350 - val_mse: 0.0017 - lr: 0.0243
Epoch 35/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0332 - val_mse: 0.0017 - lr: 0.0243
Epoch 36/200
121/122 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0333 - mse: 0.0016
122/122 [==============================] - 2s 15ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0339 - val_mse: 0.0016 - lr: 0.0243
Epoch 37/200
110/122 [==========================>...] - ETA: 0s - loss: 0.0016 - mae: 0.0334 - mse: 0.0016
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_122123-cy4o6agr\files\model-best)... Done. 0.0s
122/122 [==============================] - 2s 13ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0344 - val_mse: 0.0016 - lr: 0.0243
Epoch 38/200
122/122 [==============================] - 2s 15ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0341 - val_mse: 0.0016 - lr: 0.0243
Epoch 39/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0362 - val_mse: 0.0017 - lr: 0.0243
Epoch 40/200
122/122 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0344 - val_mse: 0.0016 - lr: 0.0243
Epoch 41/200
122/122 [==============================] - 0s 4ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 0.0243
Epoch 42/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0333 - val_mse: 0.0016 - lr: 0.0243
Epoch 43/200
115/122 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0333 - mse: 0.0016
Epoch 43: ReduceLROnPlateau reducing learning rate to 0.012127903290092945.
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 0.0243
Epoch 44/200
122/122 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0334 - val_mse: 0.0017 - lr: 0.0121
Epoch 45/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.0121
Epoch 46/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.0121
Epoch 47/200
116/122 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.0016
122/122 [==============================] - 2s 13ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.0121
Epoch 48/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0333 - val_mse: 0.0017 - lr: 0.0121
Epoch 49/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 0.0121
Epoch 50/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0334 - val_mse: 0.0016 - lr: 0.0121
Epoch 51/200
109/122 [=========================>....] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.0016
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_122123-cy4o6agr\files\model-best)... Done. 0.0s
122/122 [==============================] - 2s 16ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 0.0121
Epoch 52/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 0.0121
Epoch 53/200
119/122 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.0016
Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0060639516450464725.
122/122 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.00160016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 0.00611
Epoch 55/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 0.0061
Epoch 56/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 0.0061
Epoch 57/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 0.0061
Epoch 58/200
122/122 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.00160016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 0.00611
122/122 [==============================] - 2s 14ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.0061
Epoch 59/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0333 - val_mse: 0.0017 - lr: 0.0061
Epoch 60/200
 94/122 [======================>.......] - ETA: 0s - loss: 0.0016 - mae: 0.0335 - mse: 0.0016
122/122 [==============================] - 2s 14ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.0061
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_122123-cy4o6agr\files\model-best)... Done. 0.0s
122/122 [==============================] - 2s 14ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.0061
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 0.00611
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 0.00611
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 0.00301
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 0.00301
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_122123-cy4o6agr\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_122123-cy4o6agr\files\model-best)... Done. 0.0s
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 0.00301
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 0.00301
105/122 [========================>.....] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.00160016 - val_loss: 0.0017 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 0.00301
105/122 [========================>.....] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.00160016 - val_loss: 0.0017 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 0.00301
122/122 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 0.00151
122/122 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 0.00151
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_122123-cy4o6agr\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_122123-cy4o6agr\files\model-best)... Done. 0.0s
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 0.00151
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 0.00151
122/122 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0016 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 7.5799e-04
122/122 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0016 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 7.5799e-04
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 3.7900e-04
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 3.7900e-04
107/122 [=========================>....] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.00150015 - val_loss: 0.0016 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 3.7900e-04
107/122 [=========================>....] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.00150015 - val_loss: 0.0016 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 3.7900e-04
122/122 [==============================] - 0s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.8950e-044
Epoch 106/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 1.8950e-04
Epoch 107/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 1.8950e-04
Epoch 108/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 1.8950e-04
Epoch 109/200
122/122 [==============================] - 0s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.8950e-044
Epoch 110/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 1.8950e-04
Epoch 111/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 1.8950e-04
Epoch 112/200
106/122 [=========================>....] - ETA: 0s - loss: 0.0016 - mae: 0.0329 - mse: 0.0015
106/122 [=========================>....] - ETA: 0s - loss: 0.0016 - mae: 0.0329 - mse: 0.0015
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 9.4749e-054
Epoch 119/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 9.4749e-05
Epoch 120/200
114/122 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.0015
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 9.4749e-054
122/122 [==============================] - 0s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 4.7375e-054
Epoch 125/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 4.7375e-05
Epoch 126/200
122/122 [==============================] - 0s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 4.7375e-05
Epoch 127/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 4.7375e-05
Epoch 128/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 4.7375e-05
Epoch 129/200
 96/122 [======================>.......] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.0016
122/122 [==============================] - 0s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 4.7375e-054
Epoch 132/200
117/122 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.0016
122/122 [==============================] - 0s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 4.7375e-054
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 2.3687e-054
Epoch 138/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 2.3687e-05
Epoch 139/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 2.3687e-05
Epoch 140/200
122/122 [==============================] - 0s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 2.3687e-05
Epoch 141/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 2.3687e-05
Epoch 142/200
122/122 [==============================] - 0s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 2.3687e-05
Epoch 143/200
106/122 [=========================>....] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.0016
Epoch 143: ReduceLROnPlateau reducing learning rate to 1.1843655556731392e-05.
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 2.3687e-054
Epoch 144/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.1844e-05
Epoch 145/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.1844e-05
Epoch 146/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.1844e-05
Epoch 147/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.1844e-05
Epoch 148/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.1844e-05
Epoch 149/200
105/122 [========================>.....] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.0015
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.1844e-05
Epoch 153/200
106/122 [=========================>....] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.0016
Epoch 153: ReduceLROnPlateau reducing learning rate to 1e-05.
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.1844e-05
Epoch 154/200
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
Epoch 155/200
  1/122 [..............................] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0018
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
Epoch 160/200
 22/122 [====>.........................] - ETA: 0s - loss: 0.0015 - mae: 0.0330 - mse: 0.0015
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
122/122 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
 47/122 [==========>...................] - ETA: 0s - loss: 0.0016 - mae: 0.0335 - mse: 0.00160015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-05
162/162 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
162/162 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 14/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 18/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 22/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 26/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 30/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 33/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 38/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 42/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 46/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 51/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 54/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 59/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 62/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 66/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 70/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 72/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 77/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 81/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 86/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 90/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 94/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 98/200===========================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 101: ReduceLROnPlateau reducing learning rate to 0.00037899697781540453..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 106/200duceLROnPlateau reducing learning rate to 0.00037899697781540453..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 110/200duceLROnPlateau reducing learning rate to 0.00037899697781540453..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 111: ReduceLROnPlateau reducing learning rate to 0.00018949848890770227..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 116/200duceLROnPlateau reducing learning rate to 0.00018949848890770227..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 120/200duceLROnPlateau reducing learning rate to 0.00018949848890770227..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 125/200duceLROnPlateau reducing learning rate to 0.00018949848890770227..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 129/200duceLROnPlateau reducing learning rate to 0.00018949848890770227..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 132/200duceLROnPlateau reducing learning rate to 0.00018949848890770227..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 137/200duceLROnPlateau reducing learning rate to 0.00018949848890770227..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 141/200duceLROnPlateau reducing learning rate to 0.00018949848890770227..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 145/200duceLROnPlateau reducing learning rate to 0.00018949848890770227..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 149/200duceLROnPlateau reducing learning rate to 0.00018949848890770227..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 156/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 159/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 162/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 167/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 171/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 179/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 187/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.
Epoch 3/20000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 8/20000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 13/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 18/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 24/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 30/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 35/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 39/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 44/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 46/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 51/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0030319758225232363.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 0.0030319758225232363.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 87/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 120/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 126/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 136/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 140/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 146/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 149: ReduceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 154/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 159/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 164/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 169/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 174/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 179/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 184/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 3/20000duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 8/20000duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 13/2000duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 17/2000duceLROnPlateau reducing learning rate to 1e-05.59879112616181.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 27/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 49/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 54/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 62/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 68/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 77/200duceLROnPlateau reducing learning rate to 0.04851161316037178.1.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 87/200duceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 98/200duceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 132/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 156/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 166/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 171/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 176/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 181/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 186/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 190/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.79939556308091.5..0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
121/122 [============================>.] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 4/200==========================>.] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 8/200==========================>.] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 12/200=========================>.] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 18/200=========================>.] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 22/200=========================>.] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 27/200=========================>.] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.012127903290092945.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 0.012127903290092945.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 42/200duceLROnPlateau reducing learning rate to 0.012127903290092945.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 48/200duceLROnPlateau reducing learning rate to 0.012127903290092945.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 53/200duceLROnPlateau reducing learning rate to 0.012127903290092945.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.012127903290092945.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 0.012127903290092945.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 0.012127903290092945.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 71/200duceLROnPlateau reducing learning rate to 0.012127903290092945.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 77/200duceLROnPlateau reducing learning rate to 0.012127903290092945.2525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 80: ReduceLROnPlateau reducing learning rate to 0.0007579939556308091.525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.0007579939556308091.525 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 90: ReduceLROnPlateau reducing learning rate to 0.00037899697781540453.25 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 0.00037899697781540453.25 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.00037899697781540453.25 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 0.00037899697781540453.25 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 110: ReduceLROnPlateau reducing learning rate to 9.474924445385113e-05.25 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 116/200duceLROnPlateau reducing learning rate to 9.474924445385113e-05.25 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 120: ReduceLROnPlateau reducing learning rate to 4.737462222692557e-05.25 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 126/200duceLROnPlateau reducing learning rate to 4.737462222692557e-05.25 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 131/200duceLROnPlateau reducing learning rate to 4.737462222692557e-05.25 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 136/200duceLROnPlateau reducing learning rate to 4.737462222692557e-05.25 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 140: ReduceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 146/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 150: ReduceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 156/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 166/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 169/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 174/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 179/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 3/20000duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 8/20000duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 14/2000duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 21/2000duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 26/2000duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 33/2000duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 40/2000duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 46/2000duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 60/2000duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 60/2000duceLROnPlateau reducing learning rate to 1e-05.655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 67: ReduceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 117/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.0015159879112616181.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 127: ReduceLROnPlateau reducing learning rate to 2.3687311113462783e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 133/200duceLROnPlateau reducing learning rate to 2.3687311113462783e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 137: ReduceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 143/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 147/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 152/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 157/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 163/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 168/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 178/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 197/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 197/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 197/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 3/20000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 7/20000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.04851161316037178.-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 16/200duceLROnPlateau reducing learning rate to 0.04851161316037178.-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.02425580658018589.-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 27/200duceLROnPlateau reducing learning rate to 0.02425580658018589.-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 41/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 52/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 62/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 94/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 129/200uceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 0.012127903290092945.05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 148/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 155/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 163/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 163/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 171/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 179/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 187/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 3/20000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 8/20000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 15/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 21/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 28/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 34/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 39/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 45/2000duceLROnPlateau reducing learning rate to 1.1843655556731392e-05.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0030319758225232363.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 59/200duceLROnPlateau reducing learning rate to 0.0030319758225232363.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 66/200duceLROnPlateau reducing learning rate to 0.0030319758225232363.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 115/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 137/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 144/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 151/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 159/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 180/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 188/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 193/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 0.0007579939556308091.5.5 - mse: 0.06490098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0970e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.2060 - mae: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 5/200============================] - 0s 3ms/step - loss: 0.2060 - mae: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 10/200===========================] - 0s 3ms/step - loss: 0.2060 - mae: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 16/200===========================] - 0s 3ms/step - loss: 0.2060 - mae: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.02425580658018589.e: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 28/200duceLROnPlateau reducing learning rate to 0.02425580658018589.e: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 0.02425580658018589.e: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 39/200duceLROnPlateau reducing learning rate to 0.02425580658018589.e: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 0.02425580658018589.e: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.02425580658018589.e: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.02425580658018589.e: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 0.02425580658018589.e: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 136/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 141/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 153/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 159/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 165/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 171/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 178/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 193/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 0.0007579939556308091. 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0970e-05