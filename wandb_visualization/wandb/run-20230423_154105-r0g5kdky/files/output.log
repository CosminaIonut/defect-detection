Epoch 1/150
143/162 [=========================>....] - ETA: 0s - loss: 0.8573 - mae: 0.9249 - mse: 0.8573
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
162/162 [==============================] - 2s 13ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0923
Epoch 2/150
156/162 [===========================>..] - ETA: 0s - loss: 0.8586 - mae: 0.9256 - mse: 0.8586
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.04616504907608032.
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0923
Epoch 3/150
160/162 [============================>.] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.02308252453804016.
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 4/150
157/162 [============================>.] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.01154126226902008.
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 5/150
162/162 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00577063113451004.
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0115
Epoch 6/150
162/162 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00288531556725502.
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 7/150
150/162 [==========================>...] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.8574
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00144265778362751.
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 8/150
159/162 [============================>.] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.000721328891813755.
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 9/150
154/162 [===========================>..] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003606644459068775.
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2133e-04
Epoch 10/150
151/162 [==========================>...] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00018033222295343876.
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6066e-04
Epoch 11/150
161/162 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 11: ReduceLROnPlateau reducing learning rate to 9.016611147671938e-05.
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8033e-04
Epoch 12/150
150/162 [==========================>...] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.8574
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.508305573835969e-05.
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0166e-05
Epoch 13/150
160/162 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 13: ReduceLROnPlateau reducing learning rate to 2.2541527869179845e-05.
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5083e-05
Epoch 14/150
159/162 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.8580
Epoch 14: ReduceLROnPlateau reducing learning rate to 1.1270763934589922e-05.
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2542e-05
Epoch 15/150
147/162 [==========================>...] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.8580
Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-05.
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1271e-05
Epoch 16/150
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 17/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 18/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 20/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/150
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 22/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/150
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 24/150
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 28/150
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 30/150
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31/150
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 32/150
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 33/150
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 35/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/150
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 37/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 38/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 39/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 41/150
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 42/150
103/162 [==================>...........] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.8581
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 44/150
162/162 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/150
 15/162 [=>............................] - ETA: 0s - loss: 0.8586 - mae: 0.9256 - mse: 0.8586
100/162 [=================>............] - ETA: 0s - loss: 0.8571 - mae: 0.9248 - mse: 0.85718579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
149/162 [==========================>...] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.85778579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 89/162 [===============>..............] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.85828579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
153/162 [===========================>..] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.85758579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 17/162 [==>...........................] - ETA: 0s - loss: 0.8562 - mae: 0.9242 - mse: 0.85628579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 30/162 [====>.........................] - ETA: 0s - loss: 0.8570 - mae: 0.9247 - mse: 0.85708579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
147/162 [==========================>...] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 66/162 [===========>..................] - ETA: 0s - loss: 0.8570 - mae: 0.9247 - mse: 0.85708579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
151/162 [==========================>...] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
  1/162 [..............................] - ETA: 0s - loss: 0.8128 - mae: 0.9010 - mse: 0.81288579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
118/162 [====================>.........] - ETA: 0s - loss: 0.8588 - mae: 0.9257 - mse: 0.85888579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
200/216 [==========================>...] - ETA: 0s - loss: 0.6430 - mae: 0.7998 - mse: 0.64306431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0115e-05
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00144265778362751.
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029
Epoch 8/150
  1/216 [..............................] - ETA: 1s - loss: 0.6509 - mae: 0.8051 - mse: 0.6509
204/216 [===========================>..] - ETA: 0s - loss: 0.6431 - mae: 0.7998 - mse: 0.64316431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 9/150
216/216 [==============================] - ETA: 0s - loss: 0.6431 - mae: 0.7998 - mse: 0.6431
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003606644459068775.
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.2133e-04
Epoch 10/150
216/216 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.8033e-04
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00018033222295343876.
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.6066e-04
Epoch 11/150
200/216 [==========================>...] - ETA: 0s - loss: 0.6430 - mae: 0.7998 - mse: 0.6430
Epoch 11: ReduceLROnPlateau reducing learning rate to 9.016611147671938e-05.
216/216 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.8033e-04
Epoch 12/150
209/216 [============================>.] - ETA: 0s - loss: 0.6430 - mae: 0.7998 - mse: 0.6430
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.508305573835969e-05.
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 9.0166e-05
Epoch 13/150
216/216 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 13: ReduceLROnPlateau reducing learning rate to 2.2541527869179845e-05.
216/216 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 4.5083e-05
Epoch 14/150
203/216 [===========================>..] - ETA: 0s - loss: 0.6424 - mae: 0.7994 - mse: 0.6424
Epoch 14: ReduceLROnPlateau reducing learning rate to 1.1270763934589922e-05.
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.2542e-05
Epoch 15/150
198/216 [==========================>...] - ETA: 0s - loss: 0.6432 - mae: 0.7999 - mse: 0.6432
Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-05.
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.1271e-05
Epoch 16/150
216/216 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 17/150
216/216 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 18/150
216/216 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 19/150
216/216 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/150
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 21/150
191/216 [=========================>....] - ETA: 0s - loss: 0.6431 - mae: 0.7998 - mse: 0.6431
216/216 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 25/150
205/216 [===========================>..] - ETA: 0s - loss: 0.6432 - mae: 0.7999 - mse: 0.6432
216/216 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
216/216 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
216/216 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
216/216 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
216/216 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
 26/216 [==>...........................] - ETA: 0s - loss: 0.6446 - mae: 0.8007 - mse: 0.64466431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 63/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 66/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 69/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 71/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 74/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 77/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 83/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 88/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 90/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 93/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 96/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 101/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 104/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 106/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 109/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 112/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 115/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 117/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 119/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 122/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 127/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 130/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 133/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 136/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 139/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 141/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 143/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/150==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
 86/162 [==============>...............] - ETA: 0s - loss: 0.4592 - mae: 0.6762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
 86/162 [==============>...............] - ETA: 0s - loss: 0.4592 - mae: 0.6762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
 86/162 [==============>...............] - ETA: 0s - loss: 0.4592 - mae: 0.6762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00577063113451004.0.6762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 9/150duceLROnPlateau reducing learning rate to 0.00577063113451004.0.6762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 16/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 16/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 23/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 26/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 30/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 37/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 41/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 47/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 54/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 59/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 64/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 70/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 76/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 93/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 99/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 104/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 115/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 120/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 129/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 134/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 139/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 145/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 145/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.
Epoch 150/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00577063113451004.05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 16/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 23/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 23/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 30/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 37/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 37/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 55/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 60/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 65/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 70/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 81/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 88/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 95/150duceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 102/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 109/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 130/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 137/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 144/150uceLROnPlateau reducing learning rate to 4.508305573835969e-05.762 - mse: 0.4592  31 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.09233009784689784LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.09233009784689784LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11: ReduceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 28/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 35/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 42/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 49/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 56/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 63/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 70/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 84/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 91/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 98/150duceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 105/150uceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 112/150uceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 119/150uceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 126/150uceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 140/150uceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
 37/162 [=====>........................] - ETA: 0s - loss: 0.2296 - mae: 0.4772 - mse: 0.2296
Epoch 147/150uceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 148/150
  1/162 [..............................] - ETA: 2s - loss: 0.2385 - mae: 0.4869 - mse: 0.2385
Epoch 147/150uceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 147/150uceLROnPlateau reducing learning rate to 9.016611147671938e-05.LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003606644459068775..LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
151/162 [==========================>...] - ETA: 0s - loss: 0.1426 - mae: 0.3751 - mse: 0.1426
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00288531556725502.
162/162 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 0.0058
Epoch 7/150
120/162 [=====================>........] - ETA: 0s - loss: 0.1426 - mae: 0.3751 - mse: 0.1426
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00144265778362751.
162/162 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 0.0029
Epoch 8/150
119/162 [=====================>........] - ETA: 0s - loss: 0.1423 - mae: 0.3747 - mse: 0.1423
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.000721328891813755.
162/162 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 0.0014
Epoch 9/150
151/162 [==========================>...] - ETA: 0s - loss: 0.1426 - mae: 0.3751 - mse: 0.1426
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003606644459068775..LR_[25]CHN_32CNNI_24BS_1P_val_mseM_150epochs/model_4.h5005 - val_mse: 0.6442 - lr: 1.0000e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.2133e-04
Epoch 10/150
118/162 [====================>.........] - ETA: 0s - loss: 0.1429 - mae: 0.3755 - mse: 0.1429
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00018033222295343876.
162/162 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 3.6066e-04
Epoch 11/150
113/162 [===================>..........] - ETA: 0s - loss: 0.1426 - mae: 0.3751 - mse: 0.1426
Epoch 11: ReduceLROnPlateau reducing learning rate to 9.016611147671938e-05.
162/162 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.8033e-04
Epoch 12/150
112/162 [===================>..........] - ETA: 0s - loss: 0.1430 - mae: 0.3756 - mse: 0.1430
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.508305573835969e-05.
162/162 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 9.0166e-05
Epoch 13/150
160/162 [============================>.] - ETA: 0s - loss: 0.1427 - mae: 0.3752 - mse: 0.1427
Epoch 13: ReduceLROnPlateau reducing learning rate to 2.2541527869179845e-05.
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 4.5083e-05
Epoch 14/150
136/162 [========================>.....] - ETA: 0s - loss: 0.1425 - mae: 0.3750 - mse: 0.1425
Epoch 14: ReduceLROnPlateau reducing learning rate to 1.1270763934589922e-05.
162/162 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 2.2542e-05
Epoch 15/150
149/162 [==========================>...] - ETA: 0s - loss: 0.1427 - mae: 0.3752 - mse: 0.1427
Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-05.
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.1271e-05
Epoch 16/150
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 17/150
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 18/150
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 19/150
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 20/150
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 21/150
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 22/150
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 23/150
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 24/150
145/162 [=========================>....] - ETA: 0s - loss: 0.1428 - mae: 0.3753 - mse: 0.1428
162/162 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 39/150===========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 39/150===========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 56/150===========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 64/150===========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 72/150===========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 79/150===========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 87/150===========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 96/150===========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 96/150===========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 109/150==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 117/150==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 125/150==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 133/150==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 141/150==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 149/150==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 149/150==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_154105-r0g5kdky\files\model-best)... Done. 0.0s
162/162 [==============================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 34/150===========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 41/150===========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 50/150===========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 58/150===========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 58/150===========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 72/150===========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 80/150===========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 88/150===========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 96/150===========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 105/150==========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 113/150==========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 121/150==========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 129/150==========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 137/150==========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 144/150==========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 1/15050==========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
Epoch 1/15050==========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_154105-r0g5kdky\files\model-best)... Done. 0.0s
Epoch 5/15050==========================] - 0s 2ms/step - loss: 0.0776 - mae: 0.2752 - mse: 0.0776 - val_loss: 0.0773 - val_mae: 0.2746 - val_mse: 0.0773 - lr: 1.1271e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 30/150===========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 30/150===========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 44/150===========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 52/150===========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 60/150===========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 68/150===========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 76/150===========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 84/150===========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 92/150===========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 100/150==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 109/150==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 116/150==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 125/150==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 133/150==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 141/150==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 149/150==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 149/150==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
Epoch 149/150==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 5/15050==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.1271e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 30/150===========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 38/150===========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 47/150===========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 55/150===========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 62/150===========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 70/150===========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 78/150===========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 86/150===========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 94/150===========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 102/150==========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 110/150==========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 118/150==========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 126/150==========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 134/150==========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 134/150==========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 148/150==========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 148/150==========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05
Epoch 148/150==========================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.1271e-05