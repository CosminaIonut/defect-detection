Epoch 1/200
239/243 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
243/243 [==============================] - 2s 8ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 2/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 3/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 4/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 5/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 6/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 7/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 8/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 9/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 10/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 11/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 12/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 13/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 14/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 15/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 16/200
232/243 [===========================>..] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00570972403511405.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0114
Epoch 17/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 18/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 19/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 20/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 21/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 22/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 23/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 24/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 25/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 26/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 27/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 28/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 29/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 30/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 31/200
228/243 [===========================>..] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.002854862017557025.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0057
Epoch 32/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 33/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 34/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 35/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 36/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 37/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 38/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 39/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 40/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 41/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 42/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 43/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 44/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 45/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 46/200
241/243 [============================>.] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0014274310087785125.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 47/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 48/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 49/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 50/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 51/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 52/200
116/243 [=============>................] - ETA: 0s - loss: 0.8592 - mae: 0.9259 - mse: 0.8592
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 54/200
161/243 [==================>...........] - ETA: 0s - loss: 0.8569 - mae: 0.9246 - mse: 0.8569
 56/243 [=====>........................] - ETA: 0s - loss: 0.8567 - mae: 0.9245 - mse: 0.85678579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
226/243 [==========================>...] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.85778579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
  1/243 [..............................] - ETA: 0s - loss: 0.8546 - mae: 0.9236 - mse: 0.85468579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.1372e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.1372e-04
228/243 [===========================>..] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.1372e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.1372e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.1372e-04
204/243 [========================>.....] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.85818579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.1372e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5686e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5686e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5686e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5686e-04
 24/243 [=>............................] - ETA: 0s - loss: 0.8567 - mae: 0.9245 - mse: 0.85678579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5686e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5686e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5686e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7843e-04
 44/243 [====>.........................] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7843e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7843e-04
243/243 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7843e-04
 61/243 [======>.......................] - ETA: 0s - loss: 0.8611 - mae: 0.9269 - mse: 0.86118579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7843e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7843e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7843e-04
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7843e-04
206/243 [========================>.....] - ETA: 0s - loss: 0.8573 - mae: 0.9249 - mse: 0.85738579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7843e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.9214e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.9214e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.9214e-05
237/243 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.9214e-05
239/243 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.9214e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.4607e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.4607e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.4607e-05
186/243 [=====================>........] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.85748579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.4607e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.4607e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.4607e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2304e-05
227/243 [===========================>..] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2304e-05
243/243 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2304e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2304e-05
228/243 [===========================>..] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2304e-05
  1/243 [..............................] - ETA: 0s - loss: 0.8442 - mae: 0.9179 - mse: 0.84428579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2304e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2304e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2304e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1152e-05
 22/243 [=>............................] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1152e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1152e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1152e-05
100/243 [===========>..................] - ETA: 0s - loss: 0.8557 - mae: 0.9240 - mse: 0.85578579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1152e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1152e-05
235/243 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1152e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 99/243 [===========>..................] - ETA: 0s - loss: 0.8561 - mae: 0.9242 - mse: 0.85618579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
241/243 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
231/243 [===========================>..] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 44/243 [====>.........................] - ETA: 0s - loss: 0.8577 - mae: 0.9250 - mse: 0.85778579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 192/200==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 194/200==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 196/200==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 198/200==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 200/200==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 200/200==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
Epoch 1/20000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 1/20000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_210205-39xl6mt9\files\model-best)... Done. 0.0s
Epoch 3/20000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 6/20000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 6/20000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 10/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 10/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 13/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 13/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 17/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 17/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 32/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 37/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 37/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 43/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 43/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 49/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 49/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 52/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 52/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 55/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 57/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 57/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 60/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 60/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 62/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 62/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 66/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 68/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 68/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 68/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 72/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 74/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 74/2000==========================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 87/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 87/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 94/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.0003568577521946281. 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 106: ReduceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 106: ReduceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 110/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 110/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 113/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 115/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 115/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 118/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 118/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 121/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 122/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 124/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 126/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 126/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 129/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 131/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 131/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 134/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 134/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 136: ReduceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 136: ReduceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 140/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 142/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 142/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 145/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 145/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 145/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 149/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 151/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 152/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 152/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 156/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 162/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 162/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 165/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 166: ReduceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 166: ReduceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 178/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 178/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 181/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 184/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 187/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 197/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 197/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/20000duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 6/20000duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 9/20000duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 11/2000duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 13/2000duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 15/2000duceLROnPlateau reducing learning rate to 1e-05.609512164257e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00570972403511405.-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 39/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 41/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 43/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 52/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 54/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 60/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 66/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 68/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 76/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 77/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 94/200duceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 120/200uceLROnPlateau reducing learning rate to 0.00017842887609731406...9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 124/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 126/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 129/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 131/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 133/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 135/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 135/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 137/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 140/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 142/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 145/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 147/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 149/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 151/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 152/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 155/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 157/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 159/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 163/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 165/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 166: ReduceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 169/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 169/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 177/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 182/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 184/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 186/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 190/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 194/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 196/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 1e-05.219024328515e-05..9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/200rained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 6/200rained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 9/200rained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 11/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 13/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 15/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 35/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 41/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 53/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 55/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 59/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 62/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 71/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 81/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 83/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 91/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 106: ReduceLROnPlateau reducing learning rate to 8.921443804865703e-05.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 109/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 109/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 113/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 116/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 118/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 120/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 124/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 126/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 128/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 130/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 132/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 134/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 136/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 137/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 140/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 143/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 145/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 147/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 149/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 151/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 152/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 155/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 157/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 159/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 163/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 165/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 167/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 169/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 171/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 178/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 182/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 184/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 186/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 190/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 194/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 196/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 1/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 1/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
Epoch 1/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 6/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 9/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 12/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 12/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 17/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 20/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 22/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 24/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 42/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 52/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 54/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 60/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 66/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 68/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 77/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 106: ReduceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 109/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 111/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 113/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 115/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 117/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 120/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 124/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 124/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 127/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 130/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 132/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 134/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 136/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 137/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 140/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 142/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 144/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 147/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 150/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 154/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 156/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 159/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 164/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 166/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 167/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 177/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 177/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 182/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 184/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 186/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 190/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 190/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_3.h5- val_mse: 0.8568 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/200rained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 9/200rained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 12/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 12/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 15/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 39/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 41/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 43/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 45/200duceLROnPlateau reducing learning rate to 0.00570972403511405.222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 49/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 54/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 60/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0007137155043892562.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 66/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 66/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 71/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 81/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 81/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 98/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 117/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 119/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 125/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 127/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 127/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 132/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 134/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 136/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 137/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 139/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 141/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 145/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 154/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 156/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 160/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 162/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 164/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 164/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 167/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 174/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 176/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 178/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 185/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 187/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 197/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_5.h5- val_mse: 0.8568 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/200rained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 6/200rained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 8/200rained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 10/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 12/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 14/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 17/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 20/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 22/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 24/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 30/200ained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.002854862017557025.22LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 0.002854862017557025.22LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/200duceLROnPlateau reducing learning rate to 0.002854862017557025.22LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 0.002854862017557025.22LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/200duceLROnPlateau reducing learning rate to 0.002854862017557025.22LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 43/200duceLROnPlateau reducing learning rate to 0.002854862017557025.22LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 45/200duceLROnPlateau reducing learning rate to 0.002854862017557025.22LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 53/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 55/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 59/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 62/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 71/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.0014274310087785125.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 81/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 81/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.2LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 94/200duceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 120/200uceLROnPlateau reducing learning rate to 0.00017842887609731406.LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 124/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 126/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 128/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 130/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 132/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 134/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 136/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 137/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 140/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 140/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 143/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 143/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 147/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 149/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 151/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 152/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 155/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 160/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 162/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 164/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 166/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 167/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 172/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 174/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 176/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 176/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 179/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 182/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 184/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 186/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 190/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 194/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 196/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 1/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 1/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 1/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 6/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 9/20000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 11/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 13/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 15/2000duceLROnPlateau reducing learning rate to 4.4607219024328515e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 35/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 39/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 42/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 52/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 54/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 59/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 71/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.0007137155043892562.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0003568577521946281.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 81/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0003568577521946281.5.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00017842887609731406..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 94/200duceLROnPlateau reducing learning rate to 0.00017842887609731406..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 0.00017842887609731406..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 98/200duceLROnPlateau reducing learning rate to 0.00017842887609731406..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 0.00017842887609731406..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.00017842887609731406..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.00017842887609731406..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 106: ReduceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 109/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 111/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 113/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 115/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 117/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 119/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 121/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 122/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 125/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 127/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 129/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 131/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 131/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 135/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 136: ReduceLROnPlateau reducing learning rate to 2.2303609512164257e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 139/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 141/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 144/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 146/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 148/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 150/200duceLROnPlateau reducing learning rate to 2.2303609512164257e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 154/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 156/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 160/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 163/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 165/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 166: ReduceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 169/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 171/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 177/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 179/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 181/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 186/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 190/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 190/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 1/20000duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 1/20000duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
Epoch 1/20000duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/20000duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 6/20000duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 8/20000duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 11/2000duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 14/2000duceLROnPlateau reducing learning rate to 1e-05.804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 22/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 30/200duceLROnPlateau reducing learning rate to 0.00570972403511405.-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 39/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 41/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 53/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 53/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 62/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 68/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 76/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 77/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 91/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.002854862017557025.05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 106: ReduceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 109/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 111/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 113/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 115/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 117/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 119/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 121/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 122/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 125/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 128/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 128/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 131/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 131/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 135/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 137/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 140/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 143/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 146/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 149/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 149/200duceLROnPlateau reducing learning rate to 8.921443804865703e-05..R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 157/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 160/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 160/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 163/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 166/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 168/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 171/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 177/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 179/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 181/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 186/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 197/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1151804756082129e-05.R_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_6.h5- val_mse: 0.8568 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_9.h5- val_mse: 0.8568 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_sgd_0.011419447731820222LR_[516]CHN_50CNNI_16BS_15P_val_lossM_200epochs/model_9.h5- val_mse: 0.8568 - lr: 1.0000e-05