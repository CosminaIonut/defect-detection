Epoch 1/200
115/122 [===========================>..] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0032s vs `on_train_batch_end` time: 0.0035s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0032s vs `on_train_batch_end` time: 0.0035s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
122/122 [==============================] - 2s 12ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.3886e-04
Epoch 2/200
118/122 [============================>.] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00031942763598635793.
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.3886e-04
Epoch 3/200
115/122 [===========================>..] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00015971381799317896.
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.1943e-04
Epoch 4/200
107/122 [=========================>....] - ETA: 0s - loss: 0.8582 - mae: 0.9253 - mse: 0.8582
Epoch 4: ReduceLROnPlateau reducing learning rate to 7.985690899658948e-05.
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.5971e-04
Epoch 5/200
122/122 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 5: ReduceLROnPlateau reducing learning rate to 3.992845449829474e-05.
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.9857e-05
Epoch 6/200
113/122 [==========================>...] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
Epoch 6: ReduceLROnPlateau reducing learning rate to 1.996422724914737e-05.
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.9928e-05
Epoch 7/200
 72/122 [================>.............] - ETA: 0s - loss: 0.8596 - mae: 0.9261 - mse: 0.8596
Epoch 7: ReduceLROnPlateau reducing learning rate to 1e-05.
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.9964e-05
Epoch 8/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 9/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 10/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 11/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 12/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 13/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 14/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 15/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 17/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 18/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 20/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 22/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 24/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 28/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 30/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 32/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 33/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 35/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 37/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 38/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 39/200
122/122 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 41/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 42/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 43/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 44/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 45/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 48/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 49/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 51/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 52/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 53/200
108/122 [=========================>....] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
  1/122 [..............................] - ETA: 1s - loss: 0.8337 - mae: 0.9121 - mse: 0.83378579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 55/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 56/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 57/200
122/122 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 58/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 59/200
122/122 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 60/200
122/122 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 61/200
103/122 [========================>.....] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
109/122 [=========================>....] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 63/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 64/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 65/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 66/200
112/122 [==========================>...] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 67/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 68/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 69/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 70/200
118/122 [============================>.] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 71/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 72/200
122/122 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 73/200
122/122 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 74/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 75/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 76/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 77/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 78/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 79/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 80/200
 22/122 [====>.........................] - ETA: 0s - loss: 0.8558 - mae: 0.9240 - mse: 0.85588579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 81/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 82/200
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 83/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 84/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 85/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 86/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 87/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 88/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 89/200
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 90/200
 92/122 [=====================>........] - ETA: 0s - loss: 0.8567 - mae: 0.9245 - mse: 0.8567
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 93/200
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 94/200
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 95/200
 99/122 [=======================>......] - ETA: 0s - loss: 0.8586 - mae: 0.9256 - mse: 0.8586
118/122 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 79/122 [==================>...........] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.85828579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
110/122 [==========================>...] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
  1/122 [..............................] - ETA: 1s - loss: 0.8816 - mae: 0.9377 - mse: 0.88168579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
162/162 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.9928e-05
150/162 [==========================>...] - ETA: 0s - loss: 0.6427 - mae: 0.7996 - mse: 0.64276431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.9928e-05
162/162 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
162/162 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
 91/162 [===============>..............] - ETA: 0s - loss: 0.6451 - mae: 0.8010 - mse: 0.64516431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 29/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 37/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 41/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 47/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 54/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 58/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 61/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 66/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 71/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 74/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 84/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 87/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 91/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 94/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 99/200===========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 103/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 108/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 112/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 120/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 123/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 127/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 132/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 137/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 140/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 144/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 148/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 152/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 156/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 160/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 162/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 166/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 169/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 173/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 177/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 182/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 186/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 195/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200==========================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200duceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 21/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 39/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 45/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 55/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 58/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 63/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 68/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 74/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 78/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 83/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 87/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 92/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 102/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 107/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 114/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 119/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 124/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 130/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 140/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 145/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 152/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 157/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 162/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 167/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 171/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 176/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 180/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 186/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 196/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 196/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0043s). Check your callbacks.
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.0006388552769029277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.0006388552769029277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200duceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 21/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 39/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 45/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 55/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 59/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 64/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 68/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 72/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 75/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 91/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 96/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 100/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 105/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 121/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 129/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 134/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 139/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 144/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 148/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 153/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 157/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 161/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 164/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 170/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 175/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 179/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 184/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 188/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 194/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 5: ReduceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 9/200duceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 21/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 39/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 45/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 49/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 53/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 57/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 62/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 65/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 70/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 75/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 84/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 89/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 94/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 99/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 102/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 107/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 120/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 130/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 139/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 144/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 148/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 153/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 158/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 164/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 168/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 173/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 179/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 185/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 195/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 198/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 198/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 198/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 9/200duceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 21/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 39/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 45/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 57/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 63/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 69/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 75/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 81/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 87/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 93/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 93/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 99/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 105/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 122/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 128/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 134/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 152/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 157/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 162/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 166/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 170/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 174/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 179/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 184/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 190/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 196/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_3.h5- val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.0006388552769029277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.0006388552769029277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0037s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0037s). Check your callbacks.
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.0006388552769029277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200duceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 21/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 39/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 45/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 45/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 57/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 63/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 69/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 73/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 84/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 90/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 95/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 101/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 107/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 112/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 117/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 122/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 130/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 136/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 142/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 154/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 159/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 162/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 168/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 174/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 178/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 183/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 187/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 197/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 197/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 197/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200duceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 21/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 39/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 45/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 45/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 56/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 62/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 65/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 71/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 75/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 92/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 98/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 102/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 107/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 112/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 117/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 123/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 129/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 139/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 144/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 156/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 156/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 162/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 168/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 174/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 179/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 183/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 187/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 190/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 193/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 195/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 198/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 198/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 198/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 198/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 198/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 5: ReduceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 9/200duceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 9/200duceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 14/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 17/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 23/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 26/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 29/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 34/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 38/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 40/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 43/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 46/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 49/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 54/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 57/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 60/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 64/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 67/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 70/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 72/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 75/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 77/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 83/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 89/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 92/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 94/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/200uceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 100/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 103/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 106/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 109/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 112/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 118/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 121/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 124/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 128/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 131/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 134/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 137/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 139/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 142/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 145/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 147/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 153/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 156/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 159/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 161/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 164/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 166/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 169/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 172/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 175/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 177/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 181/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 184/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 187/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 190/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 193/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 196/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200ceLROnPlateau reducing learning rate to 3.992845449829474e-05..277LR_[40]CHN_64CNNI_32BS_1P_val_lossM_200epochs/model_6.h5- val_mse: 0.6442 - lr: 1.0000e-05