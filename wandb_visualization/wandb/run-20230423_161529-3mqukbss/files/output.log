Epoch 1/200
24/35 [===================>..........] - ETA: 0s - loss: 0.8589 - mae: 0.9257 - mse: 0.8589
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
35/35 [==============================] - 3s 48ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 2/200
35/35 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 3/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 4/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 5/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 6/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 7/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 8/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 9/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 10/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 11/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 12/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 13/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 14/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 15/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 16/200
33/35 [===========================>..] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.046211082488298416.
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0924
Epoch 17/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 18/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 19/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 20/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 21/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 22/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 23/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 24/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 25/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 26/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 27/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 28/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 29/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 30/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 31/200
31/35 [=========================>....] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.8574
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.023105541244149208.
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 32/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 33/200
35/35 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 34/200
35/35 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 35/200
35/35 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 36/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 37/200
35/35 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 38/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 39/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 40/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 41/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 42/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 43/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 44/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 45/200
35/35 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 46/200
32/35 [==========================>...] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.011552770622074604.
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 47/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 48/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 49/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 50/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 51/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 52/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 53/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 54/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 55/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 56/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 57/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 58/200
31/35 [=========================>....] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.85748579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 59/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 60/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 61/200
29/35 [=======================>......] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.005776385311037302.
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 62/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 63/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 64/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 65/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 66/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 67/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 68/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 69/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 70/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 71/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 72/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 73/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 74/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 75/200
 1/35 [..............................] - ETA: 0s - loss: 0.8687 - mae: 0.9310 - mse: 0.8687
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 77/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 78/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 79/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 80/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 81/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 82/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 83/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 84/200
26/35 [=====================>........] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 87/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 88/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 89/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 90/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 91/200
29/35 [=======================>......] - ETA: 0s - loss: 0.8586 - mae: 0.9256 - mse: 0.8586
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0014440963277593255.
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 92/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 93/200
29/35 [=======================>......] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 96/200
35/35 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 97/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 98/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 99/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 100/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 101/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 102/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 103/200
18/35 [==============>...............] - ETA: 0s - loss: 0.8586 - mae: 0.9256 - mse: 0.8586
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 106/200
27/35 [======================>.......] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.8581
Epoch 106: ReduceLROnPlateau reducing learning rate to 0.0007220481638796628.
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 107/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 108/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 109/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 110/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 111/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 112/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 113/200
 1/35 [..............................] - ETA: 0s - loss: 0.8592 - mae: 0.9260 - mse: 0.8592
 1/35 [..............................] - ETA: 0s - loss: 0.8426 - mae: 0.9168 - mse: 0.84268579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 115/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 116/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 117/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 118/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 119/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 120/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 121/200
33/35 [===========================>..] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 121: ReduceLROnPlateau reducing learning rate to 0.0003610240819398314.
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2205e-04
Epoch 122/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 123/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 124/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 125/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 126/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 127/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 128/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 129/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 130/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 131/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 132/200
28/35 [=======================>......] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 134/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 135/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 136/200
28/35 [=======================>......] - ETA: 0s - loss: 0.8584 - mae: 0.9255 - mse: 0.8584
Epoch 136: ReduceLROnPlateau reducing learning rate to 0.0001805120409699157.
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6102e-04
Epoch 137/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 138/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 139/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 140/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 141/200
31/35 [=========================>....] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 143/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 144/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 145/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 146/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 147/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 148/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 149/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 150/200
28/35 [=======================>......] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.8581
 1/35 [..............................] - ETA: 0s - loss: 0.8468 - mae: 0.9192 - mse: 0.84688579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8051e-04
Epoch 152/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 153/200
35/35 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 154/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 155/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 156/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 157/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 158/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 159/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 160/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 161/200
35/35 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 162/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 163/200
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 164/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 165/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 166/200
31/35 [=========================>....] - ETA: 0s - loss: 0.8582 - mae: 0.9253 - mse: 0.8582
Epoch 166: ReduceLROnPlateau reducing learning rate to 4.512801024247892e-05.
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0256e-05
Epoch 167/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 168/200
34/35 [============================>.] - ETA: 0s - loss: 0.8584 - mae: 0.9255 - mse: 0.8584
35/35 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 171/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 172/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 173/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 174/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 175/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 176/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 177/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 178/200
30/35 [========================>.....] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.8578
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 181/200
28/35 [=======================>......] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
Epoch 181: ReduceLROnPlateau reducing learning rate to 2.256400512123946e-05.
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5128e-05
Epoch 182/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 183/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 184/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 185/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 186/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 187/200
28/35 [=======================>......] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 190/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 191/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 192/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 193/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 194/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 195/200
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2564e-05
Epoch 196/200
30/35 [========================>.....] - ETA: 0s - loss: 0.8586 - mae: 0.9256 - mse: 0.8586
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1282e-05
Epoch 199/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1282e-05
Epoch 200/200
35/35 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1282e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_1.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1282e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0034s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0034s). Check your callbacks.
35/35 [==============================] - 0s 7ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1282e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
47/47 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0924e-05
47/47 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0924e-05
47/47 [==============================] - 0s 6ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0462e-05
47/47 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0231e-05
47/47 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0231e-05
45/47 [===========================>..] - ETA: 0s - loss: 0.6433 - mae: 0.7999 - mse: 0.64336431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0231e-05
47/47 [==============================] - 0s 6ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0116e-05
47/47 [==============================] - 0s 6ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0058e-05
47/47 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0058e-05
47/47 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
47/47 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
 1/47 [..............................] - ETA: 0s - loss: 0.6510 - mae: 0.8046 - mse: 0.65106431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
47/47 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
47/47 [==============================] - 0s 6ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.2205e-04
47/47 [==============================] - 0s 6ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 7.2205e-04
47/47 [==============================] - 0s 6ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.6102e-04
47/47 [==============================] - 0s 6ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.8051e-04
47/47 [==============================] - 0s 6ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.8051e-04
47/47 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.8051e-04
47/47 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 9.0256e-05
47/47 [==============================] - 0s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 9.0256e-05
42/47 [=========================>....] - ETA: 0s - loss: 0.6429 - mae: 0.7997 - mse: 0.64296431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 9.0256e-05
42/47 [=========================>....] - ETA: 0s - loss: 0.6429 - mae: 0.7997 - mse: 0.64296431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 9.0256e-05
42/47 [=========================>....] - ETA: 0s - loss: 0.6429 - mae: 0.7997 - mse: 0.64296431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 9.0256e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0037s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0037s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 20/200ained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 30/200ained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 39/200ained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 48/200ained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 58/200ained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 67/200ained_models/models_segments_overlap-cnn_rmsprop_0.0924221676863507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.002888192655518651.3507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.002888192655518651.3507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.002888192655518651.3507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.002888192655518651.3507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.002888192655518651.3507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 0.002888192655518651.3507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.002888192655518651.3507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002888192655518651.3507LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 7/20000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.046211082488298416.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 0.046211082488298416.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 35/200duceLROnPlateau reducing learning rate to 0.046211082488298416.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 45/200duceLROnPlateau reducing learning rate to 0.046211082488298416.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 54/200duceLROnPlateau reducing learning rate to 0.046211082488298416.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 63/200duceLROnPlateau reducing learning rate to 0.046211082488298416.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 0.046211082488298416.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.046211082488298416.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.0014440963277593255..07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 0.0014440963277593255..07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 120/200uceLROnPlateau reducing learning rate to 0.0014440963277593255..07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 120/200uceLROnPlateau reducing learning rate to 0.0014440963277593255..07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 0.0014440963277593255..07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 148/200uceLROnPlateau reducing learning rate to 0.0014440963277593255..07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 157/200uceLROnPlateau reducing learning rate to 0.0014440963277593255..07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 157/200uceLROnPlateau reducing learning rate to 0.0014440963277593255..07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 176/200duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 185/200duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 185/200duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0028s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0028s). Check your callbacks.
Epoch 195/200duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 20/2000duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 30/2000duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 39/2000duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 48/2000duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 58/2000duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 67/2000duceLROnPlateau reducing learning rate to 4.512801024247892e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0036s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0036s). Check your callbacks.
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 20/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 30/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 39/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 48/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 58/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 67/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 20/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 20/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 30/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 48/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 58/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 67/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 67/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 20/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 30/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 39/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 48/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 58/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 67/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0037s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0037s). Check your callbacks.
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 20/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 30/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 39/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 48/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 58/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 67/2000duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002888192655518651.5.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 9.025602048495784e-05.07LR_[25]CHN_8CNNI_112BS_15P_val_lossM_200epochs/model_2.h5al_mse: 0.6442 - lr: 9.0256e-05