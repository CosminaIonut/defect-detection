Y [[0.096]
 [0.014]
 [0.026]
 ...
 [0.02 ]
 [0.074]
 [0.086]]
X [[0.00293123 0.00242012 0.00202833 ... 0.00123472 0.00110405 0.0010347 ]
 [0.01003065 0.00520174 0.00222913 ... 0.00047567 0.00160212 0.00299861]
 [0.0651209  0.0624661  0.06012098 ... 0.0531939  0.05098064 0.04881471]
 ...
 [0.0141577  0.00361839 0.00010273 ... 0.00797822 0.00888544 0.00702019]
 [0.01199593 0.00969102 0.00783776 ... 0.00348408 0.0024832  0.00170184]
 [0.02677437 0.00450759 0.00090166 ... 0.01828678 0.01288733 0.00463391]]
Epoch 1/100
84/86 [============================>.] - ETA: 0s - loss: 2.4059 - mae: 0.0403 - mse: 0.0049
wandb: WARNING The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.
86/86 [==============================] - 4s 33ms/step - loss: 2.3738 - mae: 0.0401 - mse: 0.0048 - val_loss: 0.0516 - val_mae: 0.0280 - val_mse: 0.0011 - lr: 0.0100
Epoch 2/100
86/86 [==============================] - ETA: 0s - loss: 0.0358 - mae: 0.0276 - mse: 0.0011
86/86 [==============================] - 2s 23ms/step - loss: 0.0358 - mae: 0.0276 - mse: 0.0011 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0011 - lr: 0.0100
Epoch 3/100
86/86 [==============================] - 0s 4ms/step - loss: 0.0350 - mae: 0.0268 - mse: 0.0010 - val_loss: 0.0356 - val_mae: 0.0256 - val_mse: 8.6633e-04 - lr: 0.0100
Epoch 4/100
86/86 [==============================] - 0s 4ms/step - loss: 0.0350 - mae: 0.0263 - mse: 9.7105e-04 - val_loss: 0.0354 - val_mae: 0.0262 - val_mse: 9.4265e-04 - lr: 0.0100
Epoch 5/100
86/86 [==============================] - 0s 4ms/step - loss: 0.0349 - mae: 0.0257 - mse: 9.2218e-04 - val_loss: 0.0357 - val_mae: 0.0326 - val_mse: 0.0016 - lr: 0.0100
Epoch 6/100
79/86 [==========================>...] - ETA: 0s - loss: 0.0350 - mae: 0.0266 - mse: 0.0010
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.
86/86 [==============================] - 0s 4ms/step - loss: 0.0350 - mae: 0.0264 - mse: 9.9104e-04 - val_loss: 0.0351 - val_mae: 0.0274 - val_mse: 0.0011 - lr: 0.0100
Epoch 7/100
83/86 [===========================>..] - ETA: 0s - loss: 0.0069 - mae: 0.0249 - mse: 8.4068e-04
wandb: Adding directory to artifact (C:\MyDocuments\Dizertatie\segments\wandb_visualization\wandb\run-20230406_165041-e6k21tty\files\model-best)... Done. 0.0s
86/86 [==============================] - 2s 18ms/step - loss: 0.0069 - mae: 0.0249 - mse: 8.4280e-04 - val_loss: 0.0085 - val_mae: 0.0258 - val_mse: 8.9208e-04 - lr: 0.0050
Epoch 8/100
86/86 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0250 - mse: 8.4996e-04 - val_loss: 0.0088 - val_mae: 0.0257 - val_mse: 8.6533e-04 - lr: 0.0050
Epoch 9/100
82/86 [===========================>..] - ETA: 0s - loss: 0.0088 - mae: 0.0250 - mse: 8.4348e-04
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.
86/86 [==============================] - 0s 4ms/step - loss: 0.0088 - mae: 0.0250 - mse: 8.4312e-04 - val_loss: 0.0090 - val_mae: 0.0268 - val_mse: 9.6810e-04 - lr: 0.0050
Epoch 10/100
86/86 [==============================] - 2s 19ms/step - loss: 0.0023 - mae: 0.0248 - mse: 8.2950e-04 - val_loss: 0.0028 - val_mae: 0.0257 - val_mse: 8.7213e-04 - lr: 0.0025
Epoch 11/100
86/86 [==============================] - 0s 4ms/step - loss: 0.0028 - mae: 0.0249 - mse: 8.3101e-04 - val_loss: 0.0029 - val_mae: 0.0261 - val_mse: 9.1898e-04 - lr: 0.0025
Epoch 12/100
72/86 [========================>.....] - ETA: 0s - loss: 0.0028 - mae: 0.0247 - mse: 8.2453e-04
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.
86/86 [==============================] - 0s 4ms/step - loss: 0.0028 - mae: 0.0249 - mse: 8.3322e-04 - val_loss: 0.0028 - val_mae: 0.0257 - val_mse: 8.6534e-04 - lr: 0.0025
Epoch 13/100
82/86 [===========================>..] - ETA: 0s - loss: 0.0012 - mae: 0.0247 - mse: 8.2155e-04
86/86 [==============================] - 2s 23ms/step - loss: 0.0012 - mae: 0.0248 - mse: 8.2238e-04 - val_loss: 0.0013 - val_mae: 0.0257 - val_mse: 8.7313e-04 - lr: 0.0012
Epoch 14/100
86/86 [==============================] - 0s 4ms/step - loss: 0.0013 - mae: 0.0247 - mse: 8.2168e-04 - val_loss: 0.0014 - val_mae: 0.0257 - val_mse: 8.6538e-04 - lr: 0.0012
Epoch 15/100
85/86 [============================>.] - ETA: 0s - loss: 0.0013 - mae: 0.0247 - mse: 8.2267e-04
Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.
86/86 [==============================] - 0s 4ms/step - loss: 0.0013 - mae: 0.0248 - mse: 8.2300e-04 - val_loss: 0.0014 - val_mae: 0.0257 - val_mse: 8.6540e-04 - lr: 0.0012
Epoch 16/100
77/86 [=========================>....] - ETA: 0s - loss: 9.1380e-04 - mae: 0.0248 - mse: 8.2530e-04
86/86 [==============================] - 2s 25ms/step - loss: 9.1297e-04 - mae: 0.0247 - mse: 8.2112e-04 - val_loss: 0.0010 - val_mae: 0.0257 - val_mse: 8.6618e-04 - lr: 6.2500e-04
Epoch 17/100
86/86 [==============================] - ETA: 0s - loss: 9.4537e-04 - mae: 0.0247 - mse: 8.2146e-04
86/86 [==============================] - 1s 17ms/step - loss: 9.4537e-04 - mae: 0.0247 - mse: 8.2146e-04 - val_loss: 9.9032e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 6.2500e-04
Epoch 18/100
62/86 [====================>.........] - ETA: 0s - loss: 9.4992e-04 - mae: 0.0249 - mse: 8.2567e-04
Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.
86/86 [==============================] - 0s 3ms/step - loss: 9.4602e-04 - mae: 0.0248 - mse: 8.2178e-04 - val_loss: 9.9251e-04 - val_mae: 0.0257 - val_mse: 8.6767e-04 - lr: 6.2500e-04
Epoch 19/100
73/86 [========================>.....] - ETA: 0s - loss: 8.4034e-04 - mae: 0.0247 - mse: 8.1818e-04
86/86 [==============================] - 1s 18ms/step - loss: 8.4394e-04 - mae: 0.0247 - mse: 8.2078e-04 - val_loss: 8.9738e-04 - val_mae: 0.0257 - val_mse: 8.6523e-04 - lr: 3.1250e-04
Epoch 20/100
86/86 [==============================] - 0s 4ms/step - loss: 8.5014e-04 - mae: 0.0247 - mse: 8.1920e-04 - val_loss: 8.9796e-04 - val_mae: 0.0257 - val_mse: 8.6710e-04 - lr: 3.1250e-04
Epoch 21/100
66/86 [======================>.......] - ETA: 0s - loss: 8.5133e-04 - mae: 0.0248 - mse: 8.2021e-04
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.
86/86 [==============================] - 0s 4ms/step - loss: 8.5138e-04 - mae: 0.0247 - mse: 8.2026e-04 - val_loss: 8.9766e-04 - val_mae: 0.0257 - val_mse: 8.6639e-04 - lr: 3.1250e-04
Epoch 22/100
81/86 [===========================>..] - ETA: 0s - loss: 8.2495e-04 - mae: 0.0247 - mse: 8.1941e-04
86/86 [==============================] - 2s 22ms/step - loss: 8.2555e-04 - mae: 0.0247 - mse: 8.1981e-04 - val_loss: 8.7590e-04 - val_mae: 0.0257 - val_mse: 8.6639e-04 - lr: 1.5625e-04
Epoch 23/100
75/86 [=========================>....] - ETA: 0s - loss: 8.2739e-04 - mae: 0.0247 - mse: 8.1963e-04
86/86 [==============================] - 2s 22ms/step - loss: 8.2771e-04 - mae: 0.0247 - mse: 8.1995e-04 - val_loss: 8.7301e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.5625e-04
Epoch 24/100
85/86 [============================>.] - ETA: 0s - loss: 8.2729e-04 - mae: 0.0247 - mse: 8.1952e-04
Epoch 24: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.
86/86 [==============================] - 0s 4ms/step - loss: 8.2749e-04 - mae: 0.0247 - mse: 8.1972e-04 - val_loss: 8.7317e-04 - val_mae: 0.0257 - val_mse: 8.6533e-04 - lr: 1.5625e-04
Epoch 25/100
77/86 [=========================>....] - ETA: 0s - loss: 8.2829e-04 - mae: 0.0249 - mse: 8.2690e-04
86/86 [==============================] - 2s 21ms/step - loss: 8.2105e-04 - mae: 0.0247 - mse: 8.1962e-04 - val_loss: 8.6765e-04 - val_mae: 0.0257 - val_mse: 8.6531e-04 - lr: 7.8125e-05
Epoch 26/100
59/86 [===================>..........] - ETA: 0s - loss: 8.3016e-04 - mae: 0.0249 - mse: 8.2824e-04
86/86 [==============================] - 2s 19ms/step - loss: 8.2149e-04 - mae: 0.0247 - mse: 8.1957e-04 - val_loss: 8.6725e-04 - val_mae: 0.0257 - val_mse: 8.6534e-04 - lr: 7.8125e-05
Epoch 27/100
84/86 [============================>.] - ETA: 0s - loss: 8.2427e-04 - mae: 0.0248 - mse: 8.2235e-04
Epoch 27: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Dizertatie\segments\wandb_visualization\wandb\run-20230406_165041-e6k21tty\files\model-best)... Done. 0.0s
86/86 [==============================] - 2s 18ms/step - loss: 8.2155e-04 - mae: 0.0247 - mse: 8.1962e-04 - val_loss: 8.6716e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 7.8125e-05
Epoch 28/100
72/86 [========================>.....] - ETA: 0s - loss: 8.1534e-04 - mae: 0.0246 - mse: 8.1499e-04
86/86 [==============================] - 2s 19ms/step - loss: 8.1970e-04 - mae: 0.0247 - mse: 8.1934e-04 - val_loss: 8.6592e-04 - val_mae: 0.0257 - val_mse: 8.6528e-04 - lr: 3.9062e-05
Epoch 29/100
86/86 [==============================] - ETA: 0s - loss: 8.1984e-04 - mae: 0.0247 - mse: 8.1936e-04
86/86 [==============================] - 2s 24ms/step - loss: 8.1984e-04 - mae: 0.0247 - mse: 8.1936e-04 - val_loss: 8.6572e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 3.9062e-05
Epoch 30/100
72/86 [========================>.....] - ETA: 0s - loss: 8.2798e-04 - mae: 0.0249 - mse: 8.2751e-04
Epoch 30: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.
86/86 [==============================] - 0s 5ms/step - loss: 8.1988e-04 - mae: 0.0247 - mse: 8.1940e-04 - val_loss: 8.6575e-04 - val_mae: 0.0257 - val_mse: 8.6528e-04 - lr: 3.9062e-05
Epoch 31/100
79/86 [==========================>...] - ETA: 0s - loss: 8.1178e-04 - mae: 0.0246 - mse: 8.1169e-04
86/86 [==============================] - 2s 22ms/step - loss: 8.1940e-04 - mae: 0.0247 - mse: 8.1931e-04 - val_loss: 8.6544e-04 - val_mae: 0.0257 - val_mse: 8.6529e-04 - lr: 1.9531e-05
Epoch 32/100
84/86 [============================>.] - ETA: 0s - loss: 8.1805e-04 - mae: 0.0247 - mse: 8.1793e-04
86/86 [==============================] - 2s 19ms/step - loss: 8.1941e-04 - mae: 0.0247 - mse: 8.1929e-04 - val_loss: 8.6539e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.9531e-05
Epoch 33/100
72/86 [========================>.....] - ETA: 0s - loss: 8.2151e-04 - mae: 0.0247 - mse: 8.2139e-04
Epoch 33: ReduceLROnPlateau reducing learning rate to 1e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Dizertatie\segments\wandb_visualization\wandb\run-20230406_165041-e6k21tty\files\model-best)... Done. 0.0s
86/86 [==============================] - 2s 23ms/step - loss: 8.1941e-04 - mae: 0.0247 - mse: 8.1929e-04 - val_loss: 8.6536e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.9531e-05
Epoch 34/100
83/86 [===========================>..] - ETA: 0s - loss: 8.1655e-04 - mae: 0.0247 - mse: 8.1653e-04
wandb: Adding directory to artifact (C:\MyDocuments\Dizertatie\segments\wandb_visualization\wandb\run-20230406_165041-e6k21tty\files\model-best)... Done. 0.0s
67/86 [======================>.......] - ETA: 0s - loss: 8.3260e-04 - mae: 0.0250 - mse: 8.3257e-044e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 35/100
67/86 [======================>.......] - ETA: 0s - loss: 8.3260e-04 - mae: 0.0250 - mse: 8.3257e-044e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
86/86 [==============================] - 0s 4ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-055
Epoch 36/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-055
Epoch 37/100
86/86 [==============================] - 0s 3ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-055
Epoch 38/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 39/100
86/86 [==============================] - 0s 3ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-055
Epoch 40/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 41/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 42/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1922e-04 - mae: 0.0247 - mse: 8.1919e-04 - val_loss: 8.6530e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-05
Epoch 43/100
86/86 [==============================] - 0s 3ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6530e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-05
Epoch 44/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-05
Epoch 45/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-05
Epoch 46/100
86/86 [==============================] - 0s 3ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6530e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-05
Epoch 47/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6531e-04 - val_mae: 0.0257 - val_mse: 8.6528e-04 - lr: 1.0000e-05
Epoch 48/100
86/86 [==============================] - 0s 5ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-05
Epoch 49/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 50/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 51/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 52/100
86/86 [==============================] - 0s 5ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 53/100
86/86 [==============================] - 0s 3ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 54/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 55/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 56/100
86/86 [==============================] - 0s 4ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 57/100
86/86 [==============================] - 0s 5ms/step - loss: 8.1925e-04 - mae: 0.0247 - mse: 8.1922e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6524e-04 - lr: 1.0000e-05
Epoch 58/100
73/86 [========================>.....] - ETA: 0s - loss: 8.2766e-04 - mae: 0.0249 - mse: 8.2763e-04
86/86 [==============================] - 2s 19ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 59/100
79/86 [==========================>...] - ETA: 0s - loss: 8.2087e-04 - mae: 0.0248 - mse: 8.2084e-04
86/86 [==============================] - 2s 19ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Dizertatie\segments\wandb_visualization\wandb\run-20230406_165041-e6k21tty\files\model-best)... Done. 0.0s
86/86 [==============================] - 2s 19ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Dizertatie\segments\wandb_visualization\wandb\run-20230406_165041-e6k21tty\files\model-best)... Done. 0.0s
86/86 [==============================] - 2s 19ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 93/100=========================] - 2s 19ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 93/100=========================] - 2s 19ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 93/100=========================] - 2s 19ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05