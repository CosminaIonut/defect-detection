Epoch 1/150
190/243 [======================>.......] - ETA: 0s - loss: 0.8587 - mae: 0.9256 - mse: 0.8587
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 2/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 3/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 4/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 5/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 6/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 7/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 8/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 9/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 10/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 11/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 12/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 13/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 14/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 15/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 16/150
206/243 [========================>.....] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.032664306461811066.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0653
Epoch 17/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 18/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 19/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 20/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 21/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 22/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 23/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 24/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 25/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 26/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 27/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 28/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 29/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 30/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 31/150
222/243 [==========================>...] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.016332153230905533.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0327
Epoch 32/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 33/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 34/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 35/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 36/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 37/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 38/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 39/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 40/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 41/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 42/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 43/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 44/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 45/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 46/150
227/243 [===========================>..] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.008166076615452766.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0163
Epoch 47/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 48/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 49/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 50/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 51/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 52/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 53/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 54/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 55/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 56/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 57/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 58/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 59/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 60/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 61/150
192/243 [======================>.......] - ETA: 0s - loss: 0.8572 - mae: 0.9249 - mse: 0.8572
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.004083038307726383.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0082
Epoch 62/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 63/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 64/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 65/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 66/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 67/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 68/150
143/243 [================>.............] - ETA: 0s - loss: 0.8557 - mae: 0.9240 - mse: 0.8557
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 70/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 71/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 72/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 73/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 74/150
133/243 [===============>..............] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
236/243 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0020415191538631916.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0041
Epoch 77/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0020
Epoch 78/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0020
Epoch 79/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0020
Epoch 80/150
197/243 [=======================>......] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0020
Epoch 83/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0020
Epoch 84/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0020
Epoch 85/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0020
Epoch 86/150
168/243 [===================>..........] - ETA: 0s - loss: 0.8559 - mae: 0.9241 - mse: 0.8559
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0020
Epoch 90/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0020
Epoch 91/150
 68/243 [=======>......................] - ETA: 0s - loss: 0.8533 - mae: 0.9227 - mse: 0.8533
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0010
Epoch 96/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0010
Epoch 97/150
219/243 [==========================>...] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0010
Epoch 103/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0010
Epoch 104/150
 76/243 [========>.....................] - ETA: 0s - loss: 0.8563 - mae: 0.9243 - mse: 0.8563
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.1038e-04
Epoch 109/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.1038e-04
Epoch 110/150
197/243 [=======================>......] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.1038e-04
Epoch 116/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.1038e-04
Epoch 117/150
110/243 [============>.................] - ETA: 0s - loss: 0.8595 - mae: 0.9261 - mse: 0.8595
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.1038e-04
Epoch 122/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.5519e-04
Epoch 123/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.5519e-04
Epoch 124/150
 64/243 [======>.......................] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.5519e-04
Epoch 129/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.5519e-04
Epoch 130/150
225/243 [==========================>...] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.5519e-04
Epoch 136/150
209/243 [========================>.....] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 136: ReduceLROnPlateau reducing learning rate to 0.00012759494711644948.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.5519e-04
Epoch 137/150
 68/243 [=======>......................] - ETA: 0s - loss: 0.8577 - mae: 0.9250 - mse: 0.8577
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.2759e-04
Epoch 142/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.2759e-04
Epoch 143/150
213/243 [=========================>....] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.2759e-04
Epoch 149/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.2759e-04
Epoch 150/150
213/243 [=========================>....] - ETA: 0s - loss: 0.8584 - mae: 0.9254 - mse: 0.8584
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.2759e-04
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.2759e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0653e-04
Epoch 12/150
137/323 [===========>..................] - ETA: 0s - loss: 0.6456 - mae: 0.8014 - mse: 0.6456
212/323 [==================>...........] - ETA: 0s - loss: 0.6432 - mae: 0.7999 - mse: 0.64326431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0653e-04
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0327e-04
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0327e-04
314/323 [============================>.] - ETA: 0s - loss: 0.6434 - mae: 0.8000 - mse: 0.64346431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0327e-04
161/323 [=============>................] - ETA: 0s - loss: 0.6446 - mae: 0.8007 - mse: 0.64466431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0327e-04
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0163e-04
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0163e-04
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0082e-04
215/323 [==================>...........] - ETA: 0s - loss: 0.6423 - mae: 0.7993 - mse: 0.64236431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0082e-04
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0082e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0041e-04
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0041e-04
323/323 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0041e-04
323/323 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0041e-04
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0020e-04
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0020e-04
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0020e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 108/150==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 111/150==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 114/150==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 117/150==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 120/150==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 121: ReduceLROnPlateau reducing learning rate to 0.00025518989423289895..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 125/150duceLROnPlateau reducing learning rate to 0.00025518989423289895..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 128/150duceLROnPlateau reducing learning rate to 0.00025518989423289895..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 130/150duceLROnPlateau reducing learning rate to 0.00025518989423289895..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 133/150duceLROnPlateau reducing learning rate to 0.00025518989423289895..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 136/150duceLROnPlateau reducing learning rate to 0.00025518989423289895..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 139/150duceLROnPlateau reducing learning rate to 0.00025518989423289895..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 142/150duceLROnPlateau reducing learning rate to 0.00025518989423289895..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 144/150duceLROnPlateau reducing learning rate to 0.00025518989423289895..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
Epoch 148/150duceLROnPlateau reducing learning rate to 0.00025518989423289895..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0010e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0031s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0031s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/150rained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 9/150rained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 16/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 21/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 26/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 30/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 33/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 36/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 39/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 42/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 45/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 49/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 52/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 55/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 58/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 60/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 63/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 67/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 69/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 71/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 74/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 80/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 83/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 86/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 88/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 91/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 94/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 96/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 99/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 100/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 103/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 105/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 108/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 110/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 113/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 115/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 118/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 120/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 124/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 128/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 131/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 134/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 136/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 139/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 142/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 144/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 146/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 149/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_2.h5- val_mse: 0.6442 - lr: 0.0010e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 2/150rained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 6/150rained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 9/150rained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 11/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 15/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 19/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 22/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 24/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 27/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 29/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 31/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 34/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 37/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 40/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 43/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 45/150duceLROnPlateau reducing learning rate to 0.032664306461811066.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 50/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 53/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 56/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 58/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 61/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 64/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 66/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 69/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 72/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 75/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 80/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 82/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 85/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 87/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 90/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 93/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 96/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 98/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 101/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 103/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 105/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 108/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 110/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 113/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 115/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 118/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 120/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 121: ReduceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 125/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 127/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 129/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 132/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 134/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 136: ReduceLROnPlateau reducing learning rate to 0.00012759494711644948.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 139/150duceLROnPlateau reducing learning rate to 0.00012759494711644948.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 142/150duceLROnPlateau reducing learning rate to 0.00012759494711644948.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 144/150duceLROnPlateau reducing learning rate to 0.00012759494711644948.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 146/150duceLROnPlateau reducing learning rate to 0.00012759494711644948.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 149/150duceLROnPlateau reducing learning rate to 0.00012759494711644948.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_3.h5- val_mse: 0.6442 - lr: 0.0010e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 2/150rained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 8/150rained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 10/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 13/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 15/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 18/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 20/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 22/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 24/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 27/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 29/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 34/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 37/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 39/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 42/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 44/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 46/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 48/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 50/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 53/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 56/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 58/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 61/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 63/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 66/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 68/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 72/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 75/150duceLROnPlateau reducing learning rate to 0.016332153230905533.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 80/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 82/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 85/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 87/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 90/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 94/150duceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 96/150duceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 99/150duceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 101/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 104/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 106/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 109/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 111/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 115/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 118/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 121/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 125/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 128/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 131/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 133/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 136/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 139/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 142/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 145/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 148/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 150/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 150/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 150/150uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 2/15050uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 7/15050uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 10/1500uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 12/1500uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 14/1500uceLROnPlateau reducing learning rate to 0.0010207595769315958.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.032664306461811066..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 20/150duceLROnPlateau reducing learning rate to 0.032664306461811066..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 22/150duceLROnPlateau reducing learning rate to 0.032664306461811066..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 24/150duceLROnPlateau reducing learning rate to 0.032664306461811066..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 27/150duceLROnPlateau reducing learning rate to 0.032664306461811066..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 30/150duceLROnPlateau reducing learning rate to 0.032664306461811066..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 35/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 37/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 40/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 42/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 45/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 46/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 49/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 52/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 56/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 60/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 64/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 67/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 69/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 72/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 74/150duceLROnPlateau reducing learning rate to 0.016332153230905533..3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 79/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 81/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 84/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 86/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 89/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 91/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 94/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 96/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 99/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 101/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 103/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 105/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 108/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 110/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 112/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 115/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 118/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 120/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 121: ReduceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 124/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 127/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 130/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 133/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 136/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 138/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 142/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 145/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 149/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
239/243 [============================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
239/243 [============================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0032s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0032s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 2/150==========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 7/150==========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 10/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 12/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 15/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 18/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 20/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 23/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 26/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 29/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 31/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 34/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 38/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 40/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 44/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 48/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 51/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 54/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 56/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 60/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 63/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 66/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 68/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 73/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 76/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 79/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 82/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 85/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 88/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 91/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 94/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 97/150=========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 100/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 103/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 106/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 109/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 112/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 114/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 117/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 120/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 121/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 124/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 127/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 129/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 132/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 134/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 136/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 139/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 141/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 143/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 146/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 148/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 148/150========================>.] - ETA: 0s - loss: 0.0777 - mae: 0.2752 - mse: 0.0777  _16BS_15P_val_lossM_150epochs/model_4.h5- val_mse: 0.6442 - lr: 0.0010e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0038s). Check your callbacks.
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 2/150rained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 6/150rained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 9/150rained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 11/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 13/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 16/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 18/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 21/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 23/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 26/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 28/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 31/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 34/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 35/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 38/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 40/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 42/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 45/150ained_models/models_segments_overlap-cnn_adam_0.06532861278598383LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 50/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 53/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 55/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 57/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 60/150duceLROnPlateau reducing learning rate to 0.008166076615452766.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.004083038307726383.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 65/150duceLROnPlateau reducing learning rate to 0.004083038307726383.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 67/150duceLROnPlateau reducing learning rate to 0.004083038307726383.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 70/150duceLROnPlateau reducing learning rate to 0.004083038307726383.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 72/150duceLROnPlateau reducing learning rate to 0.004083038307726383.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 75/150duceLROnPlateau reducing learning rate to 0.004083038307726383.83LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 80/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 81/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 85/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 87/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 90/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 94/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 97/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 99/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 102/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 104/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 108/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 111/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 114/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 118/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.3LR_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 121: ReduceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 126/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 129/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 132/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 136/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 140/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 144/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 147/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 150/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 150/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_215653-nwrqqh64\files\model-best)... Done. 0.0s
Epoch 2/15050duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 9/15050duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 12/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 15/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 18/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 21/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 25/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 28/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 31/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 33/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 36/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 39/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 43/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 46/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 49/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 52/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 56/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 60/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 63/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 66/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 69/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 72/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 74/1500duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 80/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 84/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 88/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 91/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 94/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 97/150duceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 102/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 105/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 109/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 113/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 116/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 119/150uceLROnPlateau reducing learning rate to 0.0020415191538631916.5.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 121: ReduceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 125/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 128/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 131/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 134/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 138/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 141/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 145/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 148/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04
Epoch 148/150duceLROnPlateau reducing learning rate to 0.00025518989423289895.R_[21]CHN_100CNNI_16BS_15P_val_lossM_150epochs/model_7.h5- val_mse: 0.6442 - lr: 0.0010e-04