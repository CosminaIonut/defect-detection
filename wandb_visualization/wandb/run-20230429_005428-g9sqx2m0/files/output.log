Epoch 1/20
23/23 [==============================] - ETA: 0s - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
23/23 [==============================] - 2s 70ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0318
Epoch 2/20
12/23 [==============>...............] - ETA: 0s - loss: 0.0026 - mae: 0.0422 - mse: 0.0026 - root_mean_squared_error: 0.0507
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230429_005428-g9sqx2m0\files\model-best)... Done. 0.0s
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
23/23 [==============================] - 2s 70ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0318
Epoch 3/20
19/23 [=======================>......] - ETA: 0s - loss: 0.0025 - mae: 0.0418 - mse: 0.0025 - root_mean_squared_error: 0.0502
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
23/23 [==============================] - 1s 58ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0318
Epoch 4/20
23/23 [==============================] - 1s 65ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0318
Epoch 5/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0318
Epoch 6/20
20/23 [=========================>....] - ETA: 0s - loss: 0.0025 - mae: 0.0419 - mse: 0.0025 - root_mean_squared_error: 0.0503
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.015895070508122444.
23/23 [==============================] - 0s 9ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0318
Epoch 7/20
17/23 [=====================>........] - ETA: 0s - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0504
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230429_005428-g9sqx2m0\files\model-best)... Done. 0.0s
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
23/23 [==============================] - 1s 66ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0159
Epoch 8/20
23/23 [==============================] - 0s 10ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0159
Epoch 9/20
23/23 [==============================] - 2s 72ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0159
Epoch 10/20
23/23 [==============================] - 0s 11ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0159
Epoch 11/20
20/23 [=========================>....] - ETA: 0s - loss: 0.0026 - mae: 0.0420 - mse: 0.0026 - root_mean_squared_error: 0.0506
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.007947535254061222.
23/23 [==============================] - 0s 9ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0159
Epoch 12/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0079
Epoch 13/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0079
Epoch 14/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0079
Epoch 15/20
23/23 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0079
Epoch 16/20
21/23 [==========================>...] - ETA: 0s - loss: 0.0025 - mae: 0.0419 - mse: 0.0025 - root_mean_squared_error: 0.0503
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.003973767627030611.
23/23 [==============================] - 0s 9ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0079
Epoch 17/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0040
Epoch 18/20
23/23 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0040
Epoch 19/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0040
Epoch 20/20
23/23 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0505 - val_loss: 0.0026 - val_mae: 0.0423 - val_mse: 0.0026 - val_root_mean_squared_error: 0.0505 - lr: 0.0040
>Saved ../trained_models/CNN/models_segments_overlap-cnn_adam_0.03179014219210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_1.h5
dict_keys(['loss', 'mae', 'mse', 'root_mean_squared_error', 'val_loss', 'val_mae', 'val_mse', 'val_root_mean_squared_error', 'lr'])
Epoch 1/20
30/30 [==============================] - 1s 29ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0318
Epoch 2/20
30/30 [==============================] - 0s 7ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0318
Epoch 3/20
30/30 [==============================] - 0s 7ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0318
Epoch 4/20
30/30 [==============================] - 0s 7ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0318
Epoch 5/20
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0318
Epoch 6/20
24/30 [=======================>......] - ETA: 0s - loss: 0.0134 - mae: 0.1000 - mse: 0.0134 - root_mean_squared_error: 0.1157
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.015895070508122444.
30/30 [==============================] - 0s 7ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0318
Epoch 7/20
30/30 [==============================] - 0s 7ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0159
Epoch 8/20
30/30 [==============================] - 0s 7ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0159
Epoch 9/20
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0159
Epoch 10/20
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0159
Epoch 11/20
22/30 [=====================>........] - ETA: 0s - loss: 0.0134 - mae: 0.0997 - mse: 0.0134 - root_mean_squared_error: 0.1156
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.007947535254061222.
30/30 [==============================] - 0s 7ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0159
Epoch 12/20
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0079
Epoch 13/20
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0079
Epoch 14/20
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0079
Epoch 15/20
30/30 [==============================] - 0s 9ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0079
Epoch 16/20
29/30 [============================>.] - ETA: 0s - loss: 0.0134 - mae: 0.1003 - mse: 0.0134 - root_mean_squared_error: 0.1159
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.003973767627030611.
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0079
Epoch 17/20
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0040
Epoch 18/20
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0040
Epoch 19/20
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0040
Epoch 20/20
30/30 [==============================] - 0s 8ms/step - loss: 0.0134 - mae: 0.1002 - mse: 0.0134 - root_mean_squared_error: 0.1159 - val_loss: 0.0133 - val_mae: 0.0995 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1154 - lr: 0.0040
>Saved ../trained_models/CNN/models_segments_overlap-cnn_adam_0.03179014219210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'root_mean_squared_error', 'val_loss', 'val_mae', 'val_mse', 'val_root_mean_squared_error', 'lr'])
Epoch 1/20
23/23 [==============================] - 1s 18ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0318
Epoch 2/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0318
Epoch 3/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0318
Epoch 4/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0318
Epoch 5/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0318
Epoch 6/20
19/23 [=======================>......] - ETA: 0s - loss: 0.0526 - mae: 0.2250 - mse: 0.0526 - root_mean_squared_error: 0.2293
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.015895070508122444.
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0318
Epoch 7/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0159
Epoch 8/20
23/23 [==============================] - 0s 8ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0159
Epoch 9/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0159
Epoch 10/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0159
Epoch 11/20
18/23 [======================>.......] - ETA: 0s - loss: 0.0526 - mae: 0.2251 - mse: 0.0526 - root_mean_squared_error: 0.2293
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.007947535254061222.
23/23 [==============================] - 0s 10ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0159
Epoch 12/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0079
Epoch 13/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0079
Epoch 14/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0079
Epoch 15/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0079
Epoch 16/20
21/23 [==========================>...] - ETA: 0s - loss: 0.0524 - mae: 0.2247 - mse: 0.0524 - root_mean_squared_error: 0.2289
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.003973767627030611.
23/23 [==============================] - 0s 8ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0079
Epoch 17/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0040
Epoch 18/20
23/23 [==============================] - 0s 8ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0040
Epoch 19/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0040
Epoch 20/20
23/23 [==============================] - 0s 9ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0040
>Saved ../trained_models/CNN/models_segments_overlap-cnn_adam_0.03179014219210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'root_mean_squared_error', 'val_loss', 'val_mae', 'val_mse', 'val_root_mean_squared_error', 'lr'])
Epoch 1/20
14/23 [=================>............] - ETA: 0s - loss: 0.1067 - mae: 0.3236 - mse: 0.1067 - root_mean_squared_error: 0.3266
23/23 [==============================] - 0s 8ms/step - loss: 0.0525 - mae: 0.2248 - mse: 0.0525 - root_mean_squared_error: 0.2290 - val_loss: 0.0528 - val_mae: 0.2254 - val_mse: 0.0528 - val_root_mean_squared_error: 0.2297 - lr: 0.0040
23/23 [==============================] - 0s 9ms/step - loss: 0.1074 - mae: 0.3248 - mse: 0.1074 - root_mean_squared_error: 0.3277 - val_loss: 0.1079 - val_mae: 0.3254 - val_mse: 0.1079 - val_root_mean_squared_error: 0.3284 - lr: 0.0079
23/23 [==============================] - 0s 9ms/step - loss: 0.1074 - mae: 0.3248 - mse: 0.1074 - root_mean_squared_error: 0.3277 - val_loss: 0.1079 - val_mae: 0.3254 - val_mse: 0.1079 - val_root_mean_squared_error: 0.3284 - lr: 0.0079
23/23 [==============================] - 0s 8ms/step - loss: 0.1824 - mae: 0.4248 - mse: 0.1824 - root_mean_squared_error: 0.4271 - val_loss: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
>Saved ../trained_models/CNN/models_segments_overlap-cnn_adam_0.03179014219210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_5.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
>Saved ../trained_models/CNN/models_segments_overlap-cnn_adam_0.03179014219210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_5.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
Epoch 15/20rained_models/CNN/models_segments_overlap-cnn_adam_0.03179014219210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_5.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
Epoch 15/20rained_models/CNN/models_segments_overlap-cnn_adam_0.03179014219210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_5.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
Epoch 15/20rained_models/CNN/models_segments_overlap-cnn_adam_0.03179014219210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_5.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
Epoch 15/20rained_models/CNN/models_segments_overlap-cnn_adam_0.03179014219210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_5.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
>Saved ../trained_models/CNN/models_segments_overlap-cnn_adam_0.03179014219210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_7.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.003973767627030611.210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_7.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.003973767627030611.210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_7.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
Epoch 13/20educeLROnPlateau reducing learning rate to 0.003973767627030611.210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_7.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
Epoch 13/20educeLROnPlateau reducing learning rate to 0.003973767627030611.210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_7.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159
Epoch 13/20educeLROnPlateau reducing learning rate to 0.003973767627030611.210138LR_[20]CHN_8CNNI_176BS_10DU_5P_val_mseM_20epochs/model_7.h5: 0.1829 - val_mae: 0.4254 - val_mse: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0159