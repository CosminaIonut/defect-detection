Epoch 1/200
231/243 [===========================>..] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
243/243 [==============================] - 2s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 2/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 3/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 4/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 5/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 6/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 7/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 8/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 9/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 10/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 11/200
238/243 [============================>.] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.04494678974151611.
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0899
Epoch 12/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0449
Epoch 13/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0449
Epoch 14/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0449
Epoch 15/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0449
Epoch 16/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0449
Epoch 17/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0449
Epoch 18/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0449
Epoch 19/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0449
Epoch 20/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0449
Epoch 21/200
235/243 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.022473394870758057.
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0449
Epoch 22/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0225
Epoch 23/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0225
Epoch 24/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0225
Epoch 25/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0225
Epoch 26/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0225
Epoch 27/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0225
Epoch 28/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0225
Epoch 29/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0225
Epoch 30/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0225
Epoch 31/200
208/243 [========================>.....] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.8580
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.011236697435379028.
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0225
Epoch 32/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0112
Epoch 33/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0112
Epoch 34/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0112
Epoch 35/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0112
Epoch 36/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0112
Epoch 37/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0112
Epoch 38/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0112
Epoch 39/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0112
Epoch 40/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0112
Epoch 41/200
209/243 [========================>.....] - ETA: 0s - loss: 0.8587 - mae: 0.9256 - mse: 0.8587
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005618348717689514.
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0112
Epoch 42/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 43/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 44/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 45/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 46/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 47/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 48/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 49/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 50/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 51/200
203/243 [========================>.....] - ETA: 0s - loss: 0.8589 - mae: 0.9257 - mse: 0.8589
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002809174358844757.
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 52/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 53/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 54/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 55/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 56/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 57/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 58/200
118/243 [=============>................] - ETA: 0s - loss: 0.8598 - mae: 0.9262 - mse: 0.8598
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 60/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 61/200
243/243 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 65/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 66/200
137/243 [===============>..............] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 71/200
209/243 [========================>.....] - ETA: 0s - loss: 0.8586 - mae: 0.9256 - mse: 0.8586
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.0229e-04
Epoch 76/200
 81/243 [=========>....................] - ETA: 0s - loss: 0.8563 - mae: 0.9243 - mse: 0.8563
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.0229e-04
213/243 [=========================>....] - ETA: 0s - loss: 0.8584 - mae: 0.9255 - mse: 0.85848579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.0229e-04
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5115e-04
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5115e-04
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7557e-04
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.7787e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.7787e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.3893e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.3893e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.3893e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1947e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1947e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0973e-05
129/243 [==============>...............] - ETA: 0s - loss: 0.8584 - mae: 0.9254 - mse: 0.85848579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0973e-05
 40/243 [===>..........................] - ETA: 0s - loss: 0.8608 - mae: 0.9266 - mse: 0.86088579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0973e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0899e-05
298/323 [==========================>...] - ETA: 0s - loss: 0.6435 - mae: 0.8001 - mse: 0.64356431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0899e-05
281/323 [=========================>....] - ETA: 0s - loss: 0.6430 - mae: 0.7997 - mse: 0.64306431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0899e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0449e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0449e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0449e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 35/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 39/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 42/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 54/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 60/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 62/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 71/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 78/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 81/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 89/200duceLROnPlateau reducing learning rate to 0.011236697435379028.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 109/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 117/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 120/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 127/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 131/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 141: ReduceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 145/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 148/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 152/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 155/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 159/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 163/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 166/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 177/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 181/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 184/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 187/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 190/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 194/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 1e-05.557339742779732.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0225e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200rained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 14/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 14/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 20/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 25/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 30/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 34/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 38/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 42/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 48/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 52/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 58/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0014045871794223785.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 0.0014045871794223785.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 76/200duceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 81/200duceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 89/200duceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 94/200duceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 98/200duceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 127/200uceLROnPlateau reducing learning rate to 0.0007022935897111893.8LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 131: ReduceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 137/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 141: ReduceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 145/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 150/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 155/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 159/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 164/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 169/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 174/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 178/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 196/200duceLROnPlateau reducing learning rate to 1e-05.337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_2.h55 - val_mse: 0.6442 - lr: 0.0225e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200rained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 14/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 20/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 25/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 25/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 31/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 36/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 40/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 45/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 50/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 54/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 59/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 62/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 68/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 72/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 78/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 82/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 87/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 91/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 95/200ained_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 100/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 105/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 110/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 114/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 119/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 124/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 128/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 132/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 138/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 142/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 146/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 151/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 156/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 160/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 165/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 170/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 175/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 180/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 185/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 189/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 194/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 199/200ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 1/20000ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 1/20000ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/20000ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 14/2000ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 20/2000ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 25/2000ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 25/2000ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 31/2000ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 36/2000ined_models/models_segments_overlap-cnn_adam_0.08989357858477998LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005618348717689514.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 47/200duceLROnPlateau reducing learning rate to 0.005618348717689514.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002809174358844757.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 0.002809174358844757.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 60/200duceLROnPlateau reducing learning rate to 0.002809174358844757.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 0.002809174358844757.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 0.002809174358844757.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 0.002809174358844757.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 78/200duceLROnPlateau reducing learning rate to 0.002809174358844757.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.002809174358844757.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.002809174358844757.98LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 111/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 115/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 119/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 132/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 152/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 156/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 161/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 166/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 169/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 174/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 179/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 183/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 188/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 193/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 197/200uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 1/20000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 1/20000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.

Epoch 9/20000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 14/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 14/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 20/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 25/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 30/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 35/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 39/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 44/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 49/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 54/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 58/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 62/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 67/2000uceLROnPlateau reducing learning rate to 0.00017557339742779732.LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 77/200duceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 81/200duceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 94/200duceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 109/200uceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 0.0007022935897111893..LR_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 2.1946674678474665e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 127/200duceLROnPlateau reducing learning rate to 2.1946674678474665e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 131: ReduceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 135/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 140/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 144/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 149/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 153/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 163/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 167/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 172/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 176/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 181/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 185/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 194/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_223118-w1qz1j95\files\model-best)... Done. 0.0s
Epoch 9/20000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 14/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 20/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 25/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 31/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 31/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 36/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 55/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 60/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 78/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 108/200uceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 0.005618348717689514.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 2.1946674678474665e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 127/200duceLROnPlateau reducing learning rate to 2.1946674678474665e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 131: ReduceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 136/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 141/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 145/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 149/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 154/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 159/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 164/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 168/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 178/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 187/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 197/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/20000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 9/20000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 14/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 20/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 25/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 29/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 34/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 38/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 42/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 48/2000duceLROnPlateau reducing learning rate to 1.0973337339237332e-05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 60/200duceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 98/200duceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 0.002809174358844757.05.R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 111: ReduceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 116/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 120/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 125/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 130/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 135/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 139/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 144/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 148/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 153/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 163/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 167/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 171/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 185/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 190/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 199/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 199/200duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_223118-w1qz1j95\files\model-best)... Done. 0.0s
Epoch 3/20000duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 14/2000duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 14/2000duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 20/2000duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 25/2000duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 29/2000duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 34/2000duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 39/2000duceLROnPlateau reducing learning rate to 4.389334935694933e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 47/200duceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.005618348717689514.5..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00017557339742779732..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 0.00017557339742779732..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 101: ReduceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 107/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 111/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 116/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 120/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 125/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 130/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 134/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 138/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 142/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 147/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 152/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 157/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 166/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 171/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 176/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 180/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 185/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 189/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05
Epoch 198/200duceLROnPlateau reducing learning rate to 8.778669871389866e-05..R_[25]CHN_8CNNI_16BS_10P_val_lossM_200epochs/model_3.h55 - val_mse: 0.6442 - lr: 0.0225e-05