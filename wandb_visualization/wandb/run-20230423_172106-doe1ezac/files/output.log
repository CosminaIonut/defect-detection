
wandb: WARNING The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.
Epoch 1/200
57/61 [===========================>..] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.8581
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
61/61 [==============================] - 3s 31ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 2/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 3/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 4/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 5/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 6/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 7/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 8/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 9/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 10/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 11/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 12/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 13/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 14/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 15/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 16/200
47/61 [======================>.......] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.04624320939183235.
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0925
Epoch 17/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 18/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 19/200
61/61 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 20/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 21/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 22/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 23/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 24/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 25/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 26/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 27/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 28/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 29/200
61/61 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 30/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 31/200
59/61 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.8578
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.023121604695916176.
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0462
Epoch 32/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 33/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 34/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 35/200
61/61 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 36/200
61/61 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 37/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 38/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 39/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 40/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 41/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 42/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 43/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 44/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 45/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 46/200
39/61 [==================>...........] - ETA: 0s - loss: 0.8570 - mae: 0.9247 - mse: 0.8570
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.011560802347958088.
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0231
Epoch 47/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 48/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 49/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 50/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 51/200
61/61 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 52/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 53/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 54/200
59/61 [============================>.] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.85778579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 55/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 56/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 57/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 58/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 59/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 60/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 61/200
59/61 [============================>.] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.85778579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.005780401173979044.
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0116
Epoch 62/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 63/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 64/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 65/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 66/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 67/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 68/200
22/61 [=========>....................] - ETA: 0s - loss: 0.8571 - mae: 0.9248 - mse: 0.8571
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 72/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 73/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 74/200
61/61 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0058
Epoch 75/200
51/61 [========================>.....] - ETA: 0s - loss: 0.8568 - mae: 0.9246 - mse: 0.8568
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 80/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 81/200
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 82/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 83/200
56/61 [==========================>...] - ETA: 0s - loss: 0.8582 - mae: 0.9253 - mse: 0.8582
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 89/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 90/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0029
Epoch 91/200
 1/61 [..............................] - ETA: 0s - loss: 0.8429 - mae: 0.9169 - mse: 0.8429
61/61 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 97/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 98/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 99/200
43/61 [====================>.........] - ETA: 0s - loss: 0.8595 - mae: 0.9260 - mse: 0.8595
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 106/200
42/61 [===================>..........] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
Epoch 106: ReduceLROnPlateau reducing learning rate to 0.0007225501467473805.
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 107/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2255e-04
Epoch 108/200
24/61 [==========>...................] - ETA: 0s - loss: 0.8596 - mae: 0.9261 - mse: 0.8596
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2255e-04
Epoch 114/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2255e-04
Epoch 115/200
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2255e-04
Epoch 116/200
35/61 [================>.............] - ETA: 0s - loss: 0.8591 - mae: 0.9259 - mse: 0.8591
61/61 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.2255e-04
Epoch 122/200
61/61 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6128e-04
Epoch 123/200
50/61 [=======================>......] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6128e-04
Epoch 131/200
36/61 [================>.............] - ETA: 0s - loss: 0.8566 - mae: 0.9245 - mse: 0.8566
33/61 [===============>..............] - ETA: 0s - loss: 0.8568 - mae: 0.9246 - mse: 0.85688579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.6128e-04
19/61 [========>.....................] - ETA: 0s - loss: 0.8559 - mae: 0.9241 - mse: 0.85598579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8064e-04
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8064e-04
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0319e-05
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.0319e-05
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5159e-05
 1/61 [..............................] - ETA: 0s - loss: 0.8734 - mae: 0.9334 - mse: 0.87348579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.5159e-05
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1290e-05
Epoch 198/200
 1/61 [..............................] - ETA: 0s - loss: 0.8567 - mae: 0.9244 - mse: 0.8567
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1290e-05
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1290e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_172106-doe1ezac\files\model-best)... Done. 0.0s
61/61 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1290e-05
81/81 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0925e-05
50/81 [=================>............] - ETA: 0s - loss: 0.6439 - mae: 0.8003 - mse: 0.64396431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0925e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0462e-05
60/81 [=====================>........] - ETA: 0s - loss: 0.6415 - mae: 0.7988 - mse: 0.64156431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0462e-05
81/81 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0231e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0116e-05
81/81 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0116e-05
17/81 [=====>........................] - ETA: 0s - loss: 0.6440 - mae: 0.8004 - mse: 0.64406431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0116e-05
81/81 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0058e-05
81/81 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0058e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
81/81 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
81/81 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0029e-05
81/81 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
81/81 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 115/200========================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 0.00036127507337369025.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 129/200duceLROnPlateau reducing learning rate to 0.00036127507337369025.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 136: ReduceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 144/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 150/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 165/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 178/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 184/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 199/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 199/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 199/200duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 6/20000duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 23/2000duceLROnPlateau reducing learning rate to 0.00018063753668684512.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 40/200duceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 48/200duceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 91/200duceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 141/200uceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 150/200uceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 158/200uceLROnPlateau reducing learning rate to 0.023121604695916176.12.998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 166: ReduceLROnPlateau reducing learning rate to 4.515938417171128e-05..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 15/2000duceLROnPlateau reducing learning rate to 4.515938417171128e-05..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 15/2000duceLROnPlateau reducing learning rate to 4.515938417171128e-05..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 23/2000duceLROnPlateau reducing learning rate to 4.515938417171128e-05..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 40/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 48/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 91/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 115/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 131/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 137/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 146/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 153/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 161/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5..998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
61/61 [==============================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 176/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 183/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 191/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 197/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 197/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 197/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0031s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0031s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/20000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 15/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 23/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 31/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 39/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 46/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 53/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 61/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 68/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 75/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 82/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 87/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 93/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 102/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 110/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 119/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 127/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 136/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 144/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 152/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 161/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 169/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 178/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 186/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 194/200========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 1/20000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 1/20000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_172106-doe1ezac\files\model-best)... Done. 0.0s
Epoch 23/2000========================] - 0s 5ms/step - loss: 0.3328 - mae: 0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 40/200duceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 48/200duceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 48/200duceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 91/200duceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 141/200uceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 158/200uceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 158/200uceLROnPlateau reducing learning rate to 0.023121604695916176.0.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 166: ReduceLROnPlateau reducing learning rate to 4.515938417171128e-05.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 175/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 200/200duceLROnPlateau reducing learning rate to 4.515938417171128e-05.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 15/2000duceLROnPlateau reducing learning rate to 4.515938417171128e-05.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 23/2000duceLROnPlateau reducing learning rate to 4.515938417171128e-05.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 40/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 48/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 48/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 57/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 89/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 109/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 139/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 146/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 153/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 162/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 179/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 187/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 195/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 195/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 195/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 195/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/20000uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 15/2000uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 23/2000uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 40/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 47/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 55/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 89/200duceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 109/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 117/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 136/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 144/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 151/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 158/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 166/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 173/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 180/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 187/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 194/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/20000uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 15/2000uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 23/2000uceLROnPlateau reducing learning rate to 0.005780401173979044.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 39/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 53/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 78/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 87/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 94/200duceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 109/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 132/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 153/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 162/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 168/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 176/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 184/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 193/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 0.023121604695916176.5.5752 - mse: 0.3328 - val_loss: 0.3321 - val_mae: 0.5746 - val_mse: 0.3321 - lr: 4.5159e-05