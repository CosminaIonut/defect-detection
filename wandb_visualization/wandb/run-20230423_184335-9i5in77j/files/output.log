Epoch 1/200
84/98 [========================>.....] - ETA: 0s - loss: 0.8573 - mae: 0.9249 - mse: 0.8573
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
98/98 [==============================] - 2s 19ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0816
Epoch 2/200
85/98 [=========================>....] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.04081803187727928.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0816
Epoch 3/200
93/98 [===========================>..] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.02040901593863964.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0408
Epoch 4/200
79/98 [=======================>......] - ETA: 0s - loss: 0.8584 - mae: 0.9255 - mse: 0.8584
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.01020450796931982.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0204
Epoch 5/200
88/98 [=========================>....] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00510225398465991.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0102
Epoch 6/200
85/98 [=========================>....] - ETA: 0s - loss: 0.8582 - mae: 0.9253 - mse: 0.8582
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.002551126992329955.
98/98 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0051
Epoch 7/200
84/98 [========================>.....] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0012755634961649776.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0026
Epoch 8/200
85/98 [=========================>....] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0006377817480824888.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 9/200
84/98 [========================>.....] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003188908740412444.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.3778e-04
Epoch 10/200
98/98 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001594454370206222.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.1889e-04
Epoch 11/200
95/98 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.8580
Epoch 11: ReduceLROnPlateau reducing learning rate to 7.97227185103111e-05.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.5945e-04
Epoch 12/200
96/98 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
Epoch 12: ReduceLROnPlateau reducing learning rate to 3.986135925515555e-05.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.9723e-05
Epoch 13/200
91/98 [==========================>...] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
Epoch 13: ReduceLROnPlateau reducing learning rate to 1.9930679627577774e-05.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.9861e-05
Epoch 14/200
91/98 [==========================>...] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-05.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.9931e-05
Epoch 15/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16/200
98/98 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 17/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 18/200
98/98 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 20/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 22/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 24/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 28/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 30/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 32/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 33/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 35/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 37/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 38/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 39/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 41/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 42/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 43/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 44/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 45/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 48/200
83/98 [========================>.....] - ETA: 0s - loss: 0.8566 - mae: 0.9245 - mse: 0.85668579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 49/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 51/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 52/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 53/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 54/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 55/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 56/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 57/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 58/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 59/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 60/200
98/98 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 61/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 62/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 63/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 64/200
17/98 [====>.........................] - ETA: 0s - loss: 0.8567 - mae: 0.9246 - mse: 0.8567
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 66/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 67/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 68/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 69/200
64/98 [==================>...........] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 72/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 73/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 74/200
71/98 [====================>.........] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 78/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 79/200
71/98 [====================>.........] - ETA: 0s - loss: 0.8574 - mae: 0.9250 - mse: 0.8574
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 84/200
66/98 [===================>..........] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
93/98 [===========================>..] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.85758579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
79/98 [=======================>......] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.85778579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
90/98 [==========================>...] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 1/98 [..............................] - ETA: 0s - loss: 0.8611 - mae: 0.9270 - mse: 0.86118579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
83/98 [========================>.....] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 34/130 [======>.......................] - ETA: 0s - loss: 0.6425 - mae: 0.7994 - mse: 0.642579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
116/130 [=========================>....] - ETA: 0s - loss: 0.6431 - mae: 0.7998 - mse: 0.643179 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
  1/130 [..............................] - ETA: 0s - loss: 0.6582 - mae: 0.8095 - mse: 0.658279 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
130/130 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
130/130 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
130/130 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
130/130 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
130/130 [==============================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
130/130 [==============================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
130/130 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
 71/130 [===============>..............] - ETA: 0s - loss: 0.6427 - mae: 0.7996 - mse: 0.64276431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
130/130 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
130/130 [==============================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 66/200===========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 69/200===========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 72/200===========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 76/200===========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/200===========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 83/200===========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 87/200===========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 90/200===========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 93/200===========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/200===========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 101/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 104/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 107/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 111/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 114/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 117/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 121/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 128/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 132/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 138/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 141/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 153/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 156/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 160/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 163/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 167/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 171/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 174/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 177/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 180/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 184/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 187/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 195/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 198/200==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 1/20000==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 1/20000==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0039s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0039s). Check your callbacks.
Epoch 1/20000==========================] - 1s 5ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0006377817480824888.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 11: ReduceLROnPlateau reducing learning rate to 7.97227185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 66/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 83/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 111/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 115/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 120/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 152/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 157/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 161/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 166/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 178/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 183/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 187/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 196/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.08163606275165192LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.08163606275165192LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0029s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0029s). Check your callbacks.
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.08163606275165192LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0006377817480824888.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 11: ReduceLROnPlateau reducing learning rate to 7.97227185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 71/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 76/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 87/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 91/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 115/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 119/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 127/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 132/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 137/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 141/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 145/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 154/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 159/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 163/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 168/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 172/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 177/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 181/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 186/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 190/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 194/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0034s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0034s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00510225398465991.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 11: ReduceLROnPlateau reducing learning rate to 7.97227185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 66/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 83/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 115/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 119/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 155/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 159/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 164/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 169/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 173/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 178/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 183/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 188/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 196/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0032s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0032s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0006377817480824888.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 11: ReduceLROnPlateau reducing learning rate to 7.97227185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 62/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 66/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 89/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 94/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 98/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 115/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 120/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 129/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 151/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 155/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 160/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 165/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 169/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 174/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 179/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 190/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 194/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 14/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 19/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 25/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 31/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 31/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 37/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 43/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 49/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 55/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 61/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 61/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 67/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 73/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 85/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 91/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 91/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/200uceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 103/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 109/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 114/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 119/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 131/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 136/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 141/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 152/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 156/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 161/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 167/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 172/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 177/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 182/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 187/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 192/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 197/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 197/200ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.
Epoch 1/20000ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 1/20000ceLROnPlateau reducing learning rate to 0.04081803187727928.5.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_184335-9i5in77j\files\model-best)... Done. 0.0s
Epoch 11: ReduceLROnPlateau reducing learning rate to 7.97227185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 62/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 68/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 98/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 144/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 154/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 159/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 163/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 169/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 174/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 179/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 190/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 195/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0031s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0031s). Check your callbacks.
Epoch 200/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0006377817480824888.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 62/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 68/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 68/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 98/200duceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 115/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 120/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 126/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 131/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 140/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 145/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 155/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 161/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 166/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 171/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 176/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 181/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 196/200uceLROnPlateau reducing learning rate to 1e-05.7185103111e-05.92LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.08163606275165192LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_9.h5 - val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.08163606275165192LR_[29]CHN_100CNNI_40BS_1P_val_lossM_200epochs/model_9.h5 - val_mse: 0.6442 - lr: 1.0000e-05