Epoch 1/200
81/98 [=======================>......] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.8581
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0018s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0018s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
98/98 [==============================] - 2s 19ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 2/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 3/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 4/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 5/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 6/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 7/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 8/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 9/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 10/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 11/200
67/98 [===================>..........] - ETA: 0s - loss: 0.8572 - mae: 0.9248 - mse: 0.8572
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.023958751931786537.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0479
Epoch 12/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0240
Epoch 13/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0240
Epoch 14/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0240
Epoch 15/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0240
Epoch 16/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0240
Epoch 17/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0240
Epoch 18/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0240
Epoch 19/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0240
Epoch 20/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0240
Epoch 21/200
82/98 [========================>.....] - ETA: 0s - loss: 0.8571 - mae: 0.9248 - mse: 0.8571
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.011979375965893269.
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0240
Epoch 22/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0120
Epoch 23/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0120
Epoch 24/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0120
Epoch 25/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0120
Epoch 26/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0120
Epoch 27/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0120
Epoch 28/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0120
Epoch 29/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0120
Epoch 30/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0120
Epoch 31/200
83/98 [========================>.....] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.005989687982946634.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0120
Epoch 32/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0060
Epoch 33/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0060
Epoch 34/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0060
Epoch 35/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0060
Epoch 36/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0060
Epoch 37/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0060
Epoch 38/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0060
Epoch 39/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0060
Epoch 40/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0060
Epoch 41/200
87/98 [=========================>....] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.002994843991473317.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0060
Epoch 42/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 43/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 44/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 45/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 46/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 47/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 48/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 49/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 50/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 51/200
86/98 [=========================>....] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0014974219957366586.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 52/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 53/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 54/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 55/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 56/200
28/98 [=======>......................] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.85758579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 57/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 58/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 59/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 60/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 61/200
97/98 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0007487109978683293.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 62/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.4871e-04
Epoch 63/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.4871e-04
Epoch 64/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.4871e-04
Epoch 65/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.4871e-04
Epoch 66/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.4871e-04
Epoch 67/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.4871e-04
Epoch 68/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.4871e-04
Epoch 69/200
45/98 [============>.................] - ETA: 0s - loss: 0.8587 - mae: 0.9256 - mse: 0.8587
97/98 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.4871e-04
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.00037435549893416464.
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.4871e-04
Epoch 72/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.7436e-04
Epoch 73/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.7436e-04
Epoch 74/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.7436e-04
Epoch 75/200
93/98 [===========================>..] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.7436e-04
Epoch 79/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.7436e-04
Epoch 80/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.7436e-04
Epoch 81/200
42/98 [===========>..................] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
19/98 [====>.........................] - ETA: 0s - loss: 0.8569 - mae: 0.9247 - mse: 0.85698579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.7436e-04
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.8718e-04
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.3589e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.3589e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.3589e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.6794e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.3397e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.3397e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1699e-05
39/98 [==========>...................] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1699e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 1/98 [..............................] - ETA: 0s - loss: 0.8488 - mae: 0.9201 - mse: 0.84888579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
88/98 [=========================>....] - ETA: 0s - loss: 0.8572 - mae: 0.9248 - mse: 0.85728579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/200==========================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 11/200=========================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16/200=========================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 20/200=========================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 24/200=========================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/200=========================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 35/200=========================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/200=========================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 49/200===========================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 52/200===========================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 58/200===========================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 66/200duceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 71/200duceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 75/200duceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 79/200duceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 82/200duceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 86/200duceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 90/200duceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 95/200duceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 100/200uceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 104/200uceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 114/200uceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 119/200uceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 124/200uceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 128/200uceLROnPlateau reducing learning rate to 0.0007487109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 131: ReduceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 137/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 142/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 146/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 151/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 156/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 161/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 166/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 171/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 174/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 179/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 184/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 189/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 193/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 198/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 198/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 198/200duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0030s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0030s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 18/2000duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 18/2000duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 25/2000duceLROnPlateau reducing learning rate to 1e-05.87109978683293. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 38/200duceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 44/200duceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 51/200duceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 58/200duceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 64/200duceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 71/200duceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 77/200duceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 83/200duceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 89/200duceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 94/200duceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 100/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 105/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 110/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 116/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 121/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 125/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 130/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 135/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 140/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 146/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 151/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 156/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 162/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 167/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 172/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 176/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 182/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 187/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 193/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 198/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 198/200uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_155636-ncvcv4yy\files\model-best)... Done. 0.0s
Epoch 3/20000uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 11/2000uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 18/2000uceLROnPlateau reducing learning rate to 0.005989687982946634.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 29/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 34/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 41/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 47/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 52/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 60/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 65/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 70/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 76/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 80/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 85/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 91/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 96/200duceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 101/200uceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 107/200uceLROnPlateau reducing learning rate to 0.011979375965893269.. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 111: ReduceLROnPlateau reducing learning rate to 2.339721868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 118/200duceLROnPlateau reducing learning rate to 2.339721868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 122/200duceLROnPlateau reducing learning rate to 2.339721868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 127/200duceLROnPlateau reducing learning rate to 2.339721868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 131: ReduceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 138/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 143/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 148/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 154/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 159/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 165/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 171/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 177/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 181/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 186/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 191/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 197/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 1/20000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 1/20000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 11/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 18/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 25/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 29/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 35/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 40/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 45/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 51/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 56/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0007487109978683293..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 67/200duceLROnPlateau reducing learning rate to 0.0007487109978683293..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 78/200duceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.00018717774946708232.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 87/200duceLROnPlateau reducing learning rate to 0.00018717774946708232.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 91: ReduceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 98/200duceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 104/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 109/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 115/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 120/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 126/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 130/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 135/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 140/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 146/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 151/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 157/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 162/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 168/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 173/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 179/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 183/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 189/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 194/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 200/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 200/200uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_155636-ncvcv4yy\files\model-best)... Done. 0.0s
Epoch 11/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 18/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 18/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 25/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 29/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 35/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 40/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 45/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 51/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 56/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 61/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 67/2000uceLROnPlateau reducing learning rate to 9.358887473354116e-05..0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 77/200duceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 81/200duceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 87/200duceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 92/200duceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 97/200duceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 102/200uceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 108/200uceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 112/200uceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 118/200uceLROnPlateau reducing learning rate to 0.00037435549893416464.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 121: ReduceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 127/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 132/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 138/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 144/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 149/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 154/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 161/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 166/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 172/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 178/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 183/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 188/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 199/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 11/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 18/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 22/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 29/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 34/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 39/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 44/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 50/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 55/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 60/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 64/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 70/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 75/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 80/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 85/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 91/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 96/2000duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 101/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 106/200duceLROnPlateau reducing learning rate to 1.1698609341692645e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 111: ReduceLROnPlateau reducing learning rate to 2.339721868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 116/200duceLROnPlateau reducing learning rate to 2.339721868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 121/200duceLROnPlateau reducing learning rate to 2.339721868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 127/200duceLROnPlateau reducing learning rate to 2.339721868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 131: ReduceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 137/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 142/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 147/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 153/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 158/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 163/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 168/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 174/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 179/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 185/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 190/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 195/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 199/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 199/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 199/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 18/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 25/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 32/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 32/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 40/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 45/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 49/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 55/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 60/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 65/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 70/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 76/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 81/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 86/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 91/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 95/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 100/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 105/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 110/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 116/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 121/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 126/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 131: ReduceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 138/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 143/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 148/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 152/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 157/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 162/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 168/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 174/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 179/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 184/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 189/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 195/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 200/200duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/20000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 11/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 17/2000duceLROnPlateau reducing learning rate to 1e-05.21868338529e-05...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 28/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 34/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 37/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 42/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 48/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 52/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 59/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 64/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 70/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 75/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 80/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 84/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 89/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 95/200duceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 100/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 105/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 110/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 115/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 121/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 125/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 130/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 136/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 141/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 146/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 151/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 157/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 163/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 166/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 172/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 177/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 182/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 188/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 193/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 199/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005
Epoch 199/200uceLROnPlateau reducing learning rate to 0.011979375965893269.5...7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.003005