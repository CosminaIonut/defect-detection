Epoch 1/200
 1/29 [>.............................] - ETA: 14s - loss: 0.9484 - mae: 0.4565 - mse: 0.2099
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 2s 44ms/step - loss: 0.6921 - mae: 0.1781 - mse: 0.0531 - val_loss: 0.5171 - val_mae: 0.0498 - val_mse: 0.0030 - lr: 0.0728
Epoch 2/200
29/29 [==============================] - 1s 36ms/step - loss: 0.4246 - mae: 0.0477 - mse: 0.0028 - val_loss: 0.3364 - val_mae: 0.0483 - val_mse: 0.0029 - lr: 0.0728
Epoch 3/200
 1/29 [>.............................] - ETA: 0s - loss: 0.3363 - mae: 0.0474 - mse: 0.0028
29/29 [==============================] - 1s 36ms/step - loss: 0.2766 - mae: 0.0480 - mse: 0.0028 - val_loss: 0.2197 - val_mae: 0.0487 - val_mse: 0.0029 - lr: 0.0728
Epoch 4/200
 1/29 [>.............................] - ETA: 0s - loss: 0.2197 - mae: 0.0486 - mse: 0.0029
29/29 [==============================] - 1s 38ms/step - loss: 0.1810 - mae: 0.0482 - mse: 0.0029 - val_loss: 0.1442 - val_mae: 0.0488 - val_mse: 0.0029 - lr: 0.0728
Epoch 5/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1443 - mae: 0.0491 - mse: 0.0030
29/29 [==============================] - 1s 36ms/step - loss: 0.1192 - mae: 0.0481 - mse: 0.0029 - val_loss: 0.0954 - val_mae: 0.0486 - val_mse: 0.0029 - lr: 0.0728
Epoch 6/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0954 - mae: 0.0480 - mse: 0.0029
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 39ms/step - loss: 0.0792 - mae: 0.0480 - mse: 0.0028 - val_loss: 0.0638 - val_mae: 0.0483 - val_mse: 0.0029 - lr: 0.0728
Epoch 7/200
29/29 [==============================] - 1s 37ms/step - loss: 0.0533 - mae: 0.0475 - mse: 0.0028 - val_loss: 0.0433 - val_mae: 0.0479 - val_mse: 0.0028 - lr: 0.0728
Epoch 8/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0434 - mae: 0.0490 - mse: 0.0029
29/29 [==============================] - 1s 46ms/step - loss: 0.0364 - mae: 0.0471 - mse: 0.0027 - val_loss: 0.0299 - val_mae: 0.0474 - val_mse: 0.0028 - lr: 0.0728
Epoch 9/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0298 - mae: 0.0464 - mse: 0.0027
29/29 [==============================] - 1s 39ms/step - loss: 0.0254 - mae: 0.0465 - mse: 0.0027 - val_loss: 0.0212 - val_mae: 0.0469 - val_mse: 0.0027 - lr: 0.0728
Epoch 10/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0213 - mae: 0.0468 - mse: 0.0027
29/29 [==============================] - 1s 40ms/step - loss: 0.0183 - mae: 0.0460 - mse: 0.0026 - val_loss: 0.0155 - val_mae: 0.0464 - val_mse: 0.0026 - lr: 0.0728
Epoch 11/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0155 - mae: 0.0468 - mse: 0.0027
29/29 [==============================] - 1s 44ms/step - loss: 0.0136 - mae: 0.0455 - mse: 0.0026 - val_loss: 0.0118 - val_mae: 0.0458 - val_mse: 0.0026 - lr: 0.0728
Epoch 12/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0117 - mae: 0.0448 - mse: 0.0025
29/29 [==============================] - 1s 44ms/step - loss: 0.0104 - mae: 0.0451 - mse: 0.0025 - val_loss: 0.0093 - val_mae: 0.0454 - val_mse: 0.0025 - lr: 0.0728
Epoch 13/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0092 - mae: 0.0446 - mse: 0.0025
29/29 [==============================] - 1s 40ms/step - loss: 0.0084 - mae: 0.0447 - mse: 0.0025 - val_loss: 0.0076 - val_mae: 0.0450 - val_mse: 0.0025 - lr: 0.0728
Epoch 14/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0076 - mae: 0.0447 - mse: 0.0025
29/29 [==============================] - 1s 36ms/step - loss: 0.0070 - mae: 0.0442 - mse: 0.0024 - val_loss: 0.0065 - val_mae: 0.0445 - val_mse: 0.0025 - lr: 0.0728
Epoch 15/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0064 - mae: 0.0439 - mse: 0.0024
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 42ms/step - loss: 0.0060 - mae: 0.0438 - mse: 0.0024 - val_loss: 0.0057 - val_mae: 0.0442 - val_mse: 0.0024 - lr: 0.0728
Epoch 16/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0056 - mae: 0.0432 - mse: 0.0023
29/29 [==============================] - 1s 48ms/step - loss: 0.0053 - mae: 0.0435 - mse: 0.0024 - val_loss: 0.0051 - val_mae: 0.0439 - val_mse: 0.0024 - lr: 0.0728
Epoch 17/200
29/29 [==============================] - 1s 37ms/step - loss: 0.0049 - mae: 0.0432 - mse: 0.0023 - val_loss: 0.0047 - val_mae: 0.0435 - val_mse: 0.0024 - lr: 0.0728
Epoch 18/200
25/29 [========================>.....] - ETA: 0s - loss: 0.0045 - mae: 0.0428 - mse: 0.0023
29/29 [==============================] - 1s 38ms/step - loss: 0.0045 - mae: 0.0428 - mse: 0.0023 - val_loss: 0.0044 - val_mae: 0.0433 - val_mse: 0.0023 - lr: 0.0728
Epoch 19/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0044 - mae: 0.0436 - mse: 0.0023
29/29 [==============================] - 1s 39ms/step - loss: 0.0043 - mae: 0.0426 - mse: 0.0023 - val_loss: 0.0042 - val_mae: 0.0430 - val_mse: 0.0023 - lr: 0.0728
Epoch 20/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0040 - mae: 0.0406 - mse: 0.0021
29/29 [==============================] - 1s 36ms/step - loss: 0.0040 - mae: 0.0424 - mse: 0.0023 - val_loss: 0.0040 - val_mae: 0.0428 - val_mse: 0.0023 - lr: 0.0728
Epoch 21/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0041 - mae: 0.0439 - mse: 0.0024
29/29 [==============================] - 1s 41ms/step - loss: 0.0039 - mae: 0.0422 - mse: 0.0022 - val_loss: 0.0039 - val_mae: 0.0426 - val_mse: 0.0023 - lr: 0.0728
Epoch 22/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0039 - mae: 0.0433 - mse: 0.0023
29/29 [==============================] - 1s 39ms/step - loss: 0.0038 - mae: 0.0420 - mse: 0.0022 - val_loss: 0.0037 - val_mae: 0.0424 - val_mse: 0.0023 - lr: 0.0728
Epoch 23/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0037 - mae: 0.0419 - mse: 0.0022
29/29 [==============================] - 1s 37ms/step - loss: 0.0036 - mae: 0.0418 - mse: 0.0022 - val_loss: 0.0036 - val_mae: 0.0422 - val_mse: 0.0022 - lr: 0.0728
Epoch 24/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0035 - mae: 0.0406 - mse: 0.0021
29/29 [==============================] - 1s 51ms/step - loss: 0.0035 - mae: 0.0416 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0420 - val_mse: 0.0022 - lr: 0.0728
Epoch 25/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0035 - mae: 0.0414 - mse: 0.0022
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0415 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0419 - val_mse: 0.0022 - lr: 0.0728
Epoch 26/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0033 - mae: 0.0397 - mse: 0.0021
29/29 [==============================] - 1s 41ms/step - loss: 0.0034 - mae: 0.0414 - mse: 0.0022 - val_loss: 0.0034 - val_mae: 0.0418 - val_mse: 0.0022 - lr: 0.0728
Epoch 27/200
29/29 [==============================] - ETA: 0s - loss: 0.0033 - mae: 0.0412 - mse: 0.0021
29/29 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0412 - mse: 0.0021 - val_loss: 0.0033 - val_mae: 0.0417 - val_mse: 0.0022 - lr: 0.0728
Epoch 28/200
29/29 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0411 - mse: 0.0021 - val_loss: 0.0033 - val_mae: 0.0415 - val_mse: 0.0022 - lr: 0.0728
Epoch 29/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0032 - mae: 0.0415 - mse: 0.0021
29/29 [==============================] - 1s 39ms/step - loss: 0.0032 - mae: 0.0410 - mse: 0.0021 - val_loss: 0.0032 - val_mae: 0.0414 - val_mse: 0.0022 - lr: 0.0728
Epoch 30/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0031 - mae: 0.0410 - mse: 0.0021
29/29 [==============================] - 1s 37ms/step - loss: 0.0031 - mae: 0.0409 - mse: 0.0021 - val_loss: 0.0032 - val_mae: 0.0413 - val_mse: 0.0022 - lr: 0.0728
Epoch 31/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0032 - mae: 0.0422 - mse: 0.0022
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0407 - mse: 0.0021 - val_loss: 0.0031 - val_mae: 0.0412 - val_mse: 0.0022 - lr: 0.0728
Epoch 32/200
29/29 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0406 - mse: 0.0021 - val_loss: 0.0031 - val_mae: 0.0411 - val_mse: 0.0022 - lr: 0.0728
Epoch 33/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0029 - mae: 0.0394 - mse: 0.0020
29/29 [==============================] - 1s 44ms/step - loss: 0.0030 - mae: 0.0406 - mse: 0.0021 - val_loss: 0.0030 - val_mae: 0.0410 - val_mse: 0.0021 - lr: 0.0728
Epoch 34/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0394 - mse: 0.0020
29/29 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0405 - mse: 0.0021 - val_loss: 0.0030 - val_mae: 0.0409 - val_mse: 0.0021 - lr: 0.0728
Epoch 35/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0029 - mae: 0.0406 - mse: 0.0021
Epoch 35: ReduceLROnPlateau reducing learning rate to 0.03637633100152016.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 36ms/step - loss: 0.0029 - mae: 0.0404 - mse: 0.0021 - val_loss: 0.0030 - val_mae: 0.0408 - val_mse: 0.0021 - lr: 0.0728
Epoch 36/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0029 - mae: 0.0413 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 35ms/step - loss: 0.0029 - mae: 0.0403 - mse: 0.0021 - val_loss: 0.0029 - val_mae: 0.0408 - val_mse: 0.0021 - lr: 0.0364
Epoch 37/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0029 - mae: 0.0400 - mse: 0.0021
29/29 [==============================] - 1s 39ms/step - loss: 0.0029 - mae: 0.0402 - mse: 0.0021 - val_loss: 0.0029 - val_mae: 0.0407 - val_mse: 0.0021 - lr: 0.0364
Epoch 38/200
29/29 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0402 - mse: 0.0021 - val_loss: 0.0029 - val_mae: 0.0407 - val_mse: 0.0021 - lr: 0.0364
Epoch 39/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0401 - mse: 0.0020
29/29 [==============================] - 1s 36ms/step - loss: 0.0028 - mae: 0.0402 - mse: 0.0021 - val_loss: 0.0029 - val_mae: 0.0407 - val_mse: 0.0021 - lr: 0.0364
Epoch 40/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0028 - mae: 0.0401 - mse: 0.0021
29/29 [==============================] - 1s 40ms/step - loss: 0.0028 - mae: 0.0402 - mse: 0.0021 - val_loss: 0.0029 - val_mae: 0.0406 - val_mse: 0.0021 - lr: 0.0364
Epoch 41/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0403 - mse: 0.0021
29/29 [==============================] - 1s 47ms/step - loss: 0.0028 - mae: 0.0401 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0406 - val_mse: 0.0021 - lr: 0.0364
Epoch 42/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0401 - mse: 0.0021
29/29 [==============================] - 1s 40ms/step - loss: 0.0028 - mae: 0.0401 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0406 - val_mse: 0.0021 - lr: 0.0364
Epoch 43/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0401 - mse: 0.0020
29/29 [==============================] - 1s 41ms/step - loss: 0.0028 - mae: 0.0401 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0405 - val_mse: 0.0021 - lr: 0.0364
Epoch 44/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0393 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 40ms/step - loss: 0.0028 - mae: 0.0400 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0405 - val_mse: 0.0021 - lr: 0.0364
Epoch 45/200
29/29 [==============================] - 1s 38ms/step - loss: 0.0027 - mae: 0.0400 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0404 - val_mse: 0.0021 - lr: 0.0364
Epoch 46/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0410 - mse: 0.0021
29/29 [==============================] - 1s 40ms/step - loss: 0.0027 - mae: 0.0400 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0404 - val_mse: 0.0021 - lr: 0.0364
Epoch 47/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 40ms/step - loss: 0.0027 - mae: 0.0399 - mse: 0.0020 - val_loss: 0.0028 - val_mae: 0.0404 - val_mse: 0.0021 - lr: 0.0364
Epoch 48/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0403 - mse: 0.0021
Epoch 48: ReduceLROnPlateau reducing learning rate to 0.01818816550076008.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 35ms/step - loss: 0.0027 - mae: 0.0399 - mse: 0.0020 - val_loss: 0.0028 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0364
Epoch 49/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0381 - mse: 0.0018
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0406 - mse: 0.0021.0020 - val_loss: 0.0027 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0182
Epoch 50/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0406 - mse: 0.0021.0020 - val_loss: 0.0027 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0182
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0374 - mse: 0.0018.0020 - val_loss: 0.0027 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0182
Epoch 51/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0374 - mse: 0.0018.0020 - val_loss: 0.0027 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0182
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
 1/29 [>.............................] - ETA: 0s - loss: 0.0029 - mae: 0.0426 - mse: 0.0023.0020 - val_loss: 0.0027 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0182
Epoch 52/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0029 - mae: 0.0426 - mse: 0.0023.0020 - val_loss: 0.0027 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0182
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0411 - mse: 0.0022.0020 - val_loss: 0.0027 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0182
Epoch 53/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0401 - mse: 0.0021.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0182
Epoch 54/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0401 - mse: 0.0021.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0182
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0402 - mse: 0.0020.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0182
Epoch 55/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0402 - mse: 0.0020.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0182
25/29 [========================>.....] - ETA: 0s - loss: 0.0027 - mae: 0.0398 - mse: 0.0020.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0182
Epoch 56/200
25/29 [========================>.....] - ETA: 0s - loss: 0.0027 - mae: 0.0398 - mse: 0.0020.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0182
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0020.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0182
Epoch 57/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0020.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0182
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
Epoch 58/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0020.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0182
Epoch 58: ReduceLROnPlateau reducing learning rate to 0.00909408275038004.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0182
Epoch 59/200
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0020
29/29 [==============================] - 1s 36ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0091
Epoch 60/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0402 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0091
Epoch 61/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0382 - mse: 0.0019
29/29 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0091
Epoch 62/200
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0091
Epoch 63/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0020
29/29 [==============================] - 1s 35ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0091
Epoch 64/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0403 - mse: 0.0021
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0091
Epoch 65/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0398 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 46ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0091
Epoch 66/200
29/29 [==============================] - 1s 41ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0091
Epoch 67/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0020
29/29 [==============================] - 1s 43ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0091
Epoch 68/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0391 - mse: 0.0020
Epoch 68: ReduceLROnPlateau reducing learning rate to 0.00454704137519002.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 42ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0091
Epoch 69/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0045
Epoch 70/200
29/29 [==============================] - 1s 44ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0045
Epoch 71/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0406 - mse: 0.0021
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0045
Epoch 72/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0377 - mse: 0.0019
29/29 [==============================] - 1s 35ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0045
Epoch 73/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0020
29/29 [==============================] - 1s 49ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0045
Epoch 74/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0383 - mse: 0.0019
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0045
Epoch 75/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0400 - mse: 0.0020
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0045
Epoch 76/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0381 - mse: 0.0019
29/29 [==============================] - 1s 35ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0045
Epoch 77/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0373 - mse: 0.0018
29/29 [==============================] - 1s 36ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0045
Epoch 78/200
25/29 [========================>.....] - ETA: 0s - loss: 0.0026 - mae: 0.0395 - mse: 0.0020
Epoch 78: ReduceLROnPlateau reducing learning rate to 0.00227352068759501.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 34ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0045
Epoch 79/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0023
Epoch 80/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0391 - mse: 0.0020
29/29 [==============================] - 1s 46ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0023
Epoch 81/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0418 - mse: 0.0022
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 46ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0023
Epoch 82/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0375 - mse: 0.0019
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0023
Epoch 83/200
29/29 [==============================] - 1s 36ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0023
Epoch 84/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0391 - mse: 0.0020
29/29 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0023
Epoch 85/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0413 - mse: 0.0022
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0023
Epoch 86/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0395 - mse: 0.0020
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0023
Epoch 87/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0405 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0023
Epoch 88/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0373 - mse: 0.0019
Epoch 88: ReduceLROnPlateau reducing learning rate to 0.001136760343797505.
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0023
Epoch 89/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0393 - mse: 0.0020
29/29 [==============================] - 1s 48ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0011
Epoch 90/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0390 - mse: 0.0020
29/29 [==============================] - 1s 46ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0011
Epoch 91/200
26/29 [=========================>....] - ETA: 0s - loss: 0.0026 - mae: 0.0395 - mse: 0.0020
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0011
Epoch 92/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0407 - mse: 0.0021
29/29 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0011
Epoch 93/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0406 - mse: 0.0021
29/29 [==============================] - 1s 36ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0011
Epoch 94/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0419 - mse: 0.0022
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0011
Epoch 95/200
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 35ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0011
Epoch 96/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0368 - mse: 0.0018
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0011
Epoch 97/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0021
29/29 [==============================] - 1s 43ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0011
Epoch 98/200
27/29 [==========================>...] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
Epoch 98: ReduceLROnPlateau reducing learning rate to 0.0005683801718987525.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 42ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0011
Epoch 99/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0391 - mse: 0.0020
29/29 [==============================] - 1s 36ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 5.6838e-04
Epoch 100/200
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 5.6838e-04
Epoch 101/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0392 - mse: 0.0020
29/29 [==============================] - 1s 42ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 5.6838e-04
Epoch 102/200
29/29 [==============================] - 1s 36ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 5.6838e-04
Epoch 103/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0381 - mse: 0.0019
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 5.6838e-04
Epoch 104/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 5.6838e-04
Epoch 105/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0413 - mse: 0.0021
29/29 [==============================] - 1s 43ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 5.6838e-04
Epoch 106/200
27/29 [==========================>...] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 5.6838e-04
Epoch 107/200
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 5.6838e-04
Epoch 108/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0408 - mse: 0.0021
Epoch 108: ReduceLROnPlateau reducing learning rate to 0.0002841900859493762.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 5.6838e-04
Epoch 109/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0409 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 2.8419e-04
Epoch 110/200
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 2.8419e-04
Epoch 111/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 36ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 2.8419e-04
Epoch 112/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0385 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 2.8419e-04
Epoch 113/200
29/29 [==============================] - 1s 44ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 2.8419e-04
Epoch 114/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0394 - mse: 0.0021
29/29 [==============================] - 1s 35ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 2.8419e-04
Epoch 115/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0402 - mse: 0.0021
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 2.8419e-04
Epoch 116/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0373 - mse: 0.0019
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 2.8419e-04
Epoch 117/200
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 2.8419e-04
Epoch 118/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0386 - mse: 0.0019
Epoch 118: ReduceLROnPlateau reducing learning rate to 0.0001420950429746881.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 2.8419e-04
Epoch 119/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0391 - mse: 0.0020
29/29 [==============================] - 1s 35ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.4210e-04
Epoch 120/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0403 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.4210e-04
Epoch 121/200
29/29 [==============================] - 1s 44ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.4210e-04
Epoch 122/200
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 42ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.4210e-04
Epoch 123/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0378 - mse: 0.0018
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.4210e-04
Epoch 124/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0401 - mse: 0.0021
29/29 [==============================] - 1s 43ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.4210e-04
Epoch 125/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0403 - mse: 0.0021
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.4210e-04
Epoch 126/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0406 - mse: 0.0022
29/29 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.4210e-04
Epoch 127/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.4210e-04
Epoch 128/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0392 - mse: 0.0020
Epoch 128: ReduceLROnPlateau reducing learning rate to 7.104752148734406e-05.
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.4210e-04
Epoch 129/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0385 - mse: 0.0019
29/29 [==============================] - 1s 41ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 7.1048e-05
Epoch 130/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0020
29/29 [==============================] - 1s 50ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 7.1048e-05
Epoch 131/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 41ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 7.1048e-05
Epoch 132/200
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 7.1048e-05
Epoch 133/200
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 7.1048e-05
Epoch 134/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0381 - mse: 0.0019
29/29 [==============================] - 1s 36ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 7.1048e-05
Epoch 135/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0394 - mse: 0.0020
29/29 [==============================] - 1s 46ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 7.1048e-05
Epoch 136/200
25/29 [========================>.....] - ETA: 0s - loss: 0.0026 - mae: 0.0394 - mse: 0.0020
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 7.1048e-05
Epoch 137/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 7.1048e-05
Epoch 138/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0021
Epoch 138: ReduceLROnPlateau reducing learning rate to 3.552376074367203e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 36ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 7.1048e-05
Epoch 139/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0373 - mse: 0.0019
29/29 [==============================] - 1s 51ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 3.5524e-05
Epoch 140/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0414 - mse: 0.0022
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 3.5524e-05
Epoch 141/200
22/29 [=====================>........] - ETA: 0s - loss: 0.0026 - mae: 0.0395 - mse: 0.0020
29/29 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 3.5524e-05
Epoch 142/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0394 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 3.5524e-05
Epoch 143/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0410 - mse: 0.0022
29/29 [==============================] - 1s 41ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 3.5524e-05
Epoch 144/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0391 - mse: 0.0020
29/29 [==============================] - 1s 41ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 3.5524e-05
Epoch 145/200
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 3.5524e-05
Epoch 146/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0393 - mse: 0.0021
29/29 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 3.5524e-05
Epoch 147/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0395 - mse: 0.0021
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 3.5524e-05
Epoch 148/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0384 - mse: 0.0020
Epoch 148: ReduceLROnPlateau reducing learning rate to 1.7761880371836014e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 49ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 3.5524e-05
Epoch 149/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0385 - mse: 0.0019
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.7762e-05
Epoch 150/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0398 - mse: 0.0021
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.7762e-05
Epoch 151/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0416 - mse: 0.0022
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.7762e-05
Epoch 152/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0028 - mae: 0.0415 - mse: 0.0022
29/29 [==============================] - 1s 35ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.7762e-05
Epoch 153/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0398 - mse: 0.0021
29/29 [==============================] - 1s 43ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.7762e-05
Epoch 154/200
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 36ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.7762e-05
Epoch 155/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0023 - mae: 0.0369 - mse: 0.0018
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.7762e-05
Epoch 156/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0398 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 45ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.7762e-05
Epoch 157/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0374 - mse: 0.0019
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.7762e-05
Epoch 158/200
26/29 [=========================>....] - ETA: 0s - loss: 0.0026 - mae: 0.0395 - mse: 0.0020
Epoch 158: ReduceLROnPlateau reducing learning rate to 1e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.7762e-05
Epoch 159/200
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
Epoch 160/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
Epoch 161/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0392 - mse: 0.0020
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
Epoch 162/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0412 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
Epoch 163/200
29/29 [==============================] - 1s 45ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
Epoch 164/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0398 - mse: 0.0020
29/29 [==============================] - 1s 48ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
Epoch 165/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
Epoch 166/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0377 - mse: 0.0018
29/29 [==============================] - 2s 56ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
Epoch 167/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0396 - mse: 0.0020
29/29 [==============================] - 2s 56ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
29/29 [==============================] - 2s 54ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 2s 54ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0405 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0405 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
27/29 [==========================>...] - ETA: 0s - loss: 0.0026 - mae: 0.0394 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
27/29 [==========================>...] - ETA: 0s - loss: 0.0026 - mae: 0.0394 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
28/29 [===========================>..] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 46ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 46ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0381 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0381 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0388 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0406 - mse: 0.0021.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0406 - mse: 0.0021.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 45ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 45ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0386 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0386 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0400 - mse: 0.0021.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0382 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0382 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0370 - mse: 0.0018.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0370 - mse: 0.0018.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 37ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0395 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 46ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 46ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 35ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 35ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_120144-xzpxeu8p\files\model-best)... Done. 0.0s
 1/29 [>.............................] - ETA: 0s - loss: 0.0027 - mae: 0.0408 - mse: 0.0022.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 41ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 41ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0410 - mse: 0.0021.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0410 - mse: 0.0021.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 44ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
29/29 [==============================] - 1s 44ms/step - loss: 0.0026 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0394 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
 1/29 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0394 - mse: 0.0020.0020 - val_loss: 0.0026 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 1.0000e-05
38/38 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0103 - val_mae: 0.0909 - val_mse: 0.0101 - lr: 0.03640e-05
Epoch 33/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0103 - val_mae: 0.0909 - val_mse: 0.0101 - lr: 0.0364
Epoch 34/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0103 - val_mae: 0.0909 - val_mse: 0.0101 - lr: 0.03640e-05
Epoch 35/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0103 - val_mae: 0.0909 - val_mse: 0.0101 - lr: 0.0364
Epoch 36/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0103 - val_mae: 0.0909 - val_mse: 0.0101 - lr: 0.0364
Epoch 37/200
31/38 [=======================>......] - ETA: 0s - loss: 0.0103 - mae: 0.0908 - mse: 0.0101
Epoch 37: ReduceLROnPlateau reducing learning rate to 0.01818816550076008.
38/38 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0103 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0364
Epoch 38/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0182
Epoch 39/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0182
Epoch 40/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0182
Epoch 41/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0182
Epoch 42/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0182
Epoch 43/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0182
Epoch 44/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0182
Epoch 45/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0182
Epoch 46/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0182
Epoch 47/200
31/38 [=======================>......] - ETA: 0s - loss: 0.0103 - mae: 0.0913 - mse: 0.0101
Epoch 47: ReduceLROnPlateau reducing learning rate to 0.00909408275038004.
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0909 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0182
Epoch 48/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0091
Epoch 49/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0091
Epoch 50/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0091
Epoch 51/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0091
Epoch 52/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0091
Epoch 53/200
38/38 [==============================] - 0s 5ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0091
Epoch 54/200
38/38 [==============================] - 0s 5ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0091
Epoch 55/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0091
Epoch 56/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0091
Epoch 57/200
34/38 [=========================>....] - ETA: 0s - loss: 0.0102 - mae: 0.0907 - mse: 0.0100
Epoch 57: ReduceLROnPlateau reducing learning rate to 0.00454704137519002.
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0091
Epoch 58/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0045
Epoch 59/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0045
Epoch 60/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0101 - lr: 0.0045
Epoch 61/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0045
Epoch 62/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0045
Epoch 63/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0045
Epoch 64/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0045
Epoch 65/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0045
Epoch 66/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0045
Epoch 67/200
29/38 [=====================>........] - ETA: 0s - loss: 0.0102 - mae: 0.0908 - mse: 0.0101
Epoch 67: ReduceLROnPlateau reducing learning rate to 0.00227352068759501.
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0045
Epoch 68/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0023
Epoch 69/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0023
Epoch 70/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0023
Epoch 71/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0023
Epoch 72/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0023
Epoch 73/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0023
Epoch 74/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0023
Epoch 75/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0023
Epoch 76/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0023
Epoch 77/200
29/38 [=====================>........] - ETA: 0s - loss: 0.0102 - mae: 0.0908 - mse: 0.0101
Epoch 77: ReduceLROnPlateau reducing learning rate to 0.001136760343797505.
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0023
Epoch 78/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0011
Epoch 79/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0011
Epoch 80/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0011
Epoch 81/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0011
Epoch 82/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0011
Epoch 83/200
38/38 [==============================] - 0s 2ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0011
Epoch 84/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0011
Epoch 85/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0011
Epoch 86/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0011
Epoch 87/200
38/38 [==============================] - ETA: 0s - loss: 0.0102 - mae: 0.0908 - mse: 0.0101
Epoch 87: ReduceLROnPlateau reducing learning rate to 0.0005683801718987525.
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 0.0011
Epoch 88/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 5.6838e-04
Epoch 89/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 5.6838e-04
Epoch 90/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 5.6838e-04
Epoch 91/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 5.6838e-04
Epoch 92/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 5.6838e-04
Epoch 93/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 5.6838e-04
Epoch 94/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 5.6838e-04
Epoch 95/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 5.6838e-04
Epoch 96/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 5.6838e-04
Epoch 97/200
27/38 [====================>.........] - ETA: 0s - loss: 0.0102 - mae: 0.0908 - mse: 0.0101
Epoch 97: ReduceLROnPlateau reducing learning rate to 0.0002841900859493762.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0020s). Check your callbacks.
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 5.6838e-04
Epoch 98/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 2.8419e-04
Epoch 99/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 2.8419e-04
Epoch 100/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 2.8419e-04
Epoch 101/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 2.8419e-04
Epoch 102/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 2.8419e-04
Epoch 103/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 2.8419e-04
Epoch 104/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 2.8419e-04
Epoch 105/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 2.8419e-04
Epoch 106/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 2.8419e-04
Epoch 107/200
32/38 [========================>.....] - ETA: 0s - loss: 0.0102 - mae: 0.0908 - mse: 0.0101
Epoch 107: ReduceLROnPlateau reducing learning rate to 0.0001420950429746881.
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 2.8419e-04
Epoch 108/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.4210e-04
Epoch 109/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.4210e-04
Epoch 110/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.4210e-04
Epoch 111/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.4210e-04
Epoch 112/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.4210e-04
Epoch 113/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.4210e-04
Epoch 114/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.4210e-04
Epoch 115/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.4210e-04
Epoch 116/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.4210e-04
Epoch 117/200
35/38 [==========================>...] - ETA: 0s - loss: 0.0102 - mae: 0.0909 - mse: 0.0101
Epoch 117: ReduceLROnPlateau reducing learning rate to 7.104752148734406e-05.
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.4210e-04
Epoch 118/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 7.1048e-05
Epoch 119/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 7.1048e-05
Epoch 120/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 7.1048e-05
Epoch 121/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 7.1048e-05
Epoch 122/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 7.1048e-05
Epoch 123/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 7.1048e-05
Epoch 124/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 7.1048e-05
Epoch 125/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 7.1048e-05
Epoch 126/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 7.1048e-05
Epoch 127/200
33/38 [=========================>....] - ETA: 0s - loss: 0.0102 - mae: 0.0908 - mse: 0.0101
Epoch 127: ReduceLROnPlateau reducing learning rate to 3.552376074367203e-05.
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 7.1048e-05
Epoch 128/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 3.5524e-05
Epoch 129/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 3.5524e-05
Epoch 130/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 3.5524e-05
Epoch 131/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 3.5524e-05
Epoch 132/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 3.5524e-05
Epoch 133/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 3.5524e-05
Epoch 134/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 3.5524e-05
Epoch 135/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 3.5524e-05
Epoch 136/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 3.5524e-05
Epoch 137/200
35/38 [==========================>...] - ETA: 0s - loss: 0.0102 - mae: 0.0910 - mse: 0.0101
Epoch 137: ReduceLROnPlateau reducing learning rate to 1.7761880371836014e-05.
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 3.5524e-05
Epoch 138/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.7762e-05
Epoch 139/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.7762e-05
Epoch 140/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.7762e-05
Epoch 141/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.7762e-05
Epoch 142/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.7762e-05
Epoch 143/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.7762e-05
Epoch 144/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.7762e-05
Epoch 145/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.7762e-05
Epoch 146/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.7762e-05
Epoch 147/200
37/38 [============================>.] - ETA: 0s - loss: 0.0102 - mae: 0.0907 - mse: 0.0100
Epoch 147: ReduceLROnPlateau reducing learning rate to 1e-05.
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.7762e-05
Epoch 148/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 149/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 150/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 151/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 152/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 153/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 154/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 155/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 156/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 157/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 158/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 159/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 160/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 161/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 162/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 163/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 164/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 165/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 166/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 167/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 168/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 169/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 170/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 171/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 172/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 173/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 174/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 175/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 176/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 177/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 178/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 179/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 180/200
38/38 [==============================] - 0s 2ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 181/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 182/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 183/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 184/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 185/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 186/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 187/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 188/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 189/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 190/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 191/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 192/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 193/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 194/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 195/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 196/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 197/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 198/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 199/200
38/38 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 200/200
38/38 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0908 - val_mse: 0.0100 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.07275266159696196LR_[38]HN_136BS_10P_val_mseM_200epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
29/29 [==============================] - 1s 9ms/step - loss: 0.7007 - mae: 0.2027 - mse: 0.0606 - val_loss: 0.5400 - val_mae: 0.1532 - val_mse: 0.0264 - lr: 0.0728
Epoch 2/200
29/29 [==============================] - 0s 3ms/step - loss: 0.4463 - mae: 0.1525 - mse: 0.0251 - val_loss: 0.3578 - val_mae: 0.1532 - val_mse: 0.0249 - lr: 0.0728
Epoch 3/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2975 - mae: 0.1525 - mse: 0.0247 - val_loss: 0.2404 - val_mae: 0.1532 - val_mse: 0.0249 - lr: 0.0728
Epoch 4/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2013 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.1645 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0728
Epoch 5/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1392 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.1154 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0728
Epoch 6/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0989 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0836 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0728
Epoch 7/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0729 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0631 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0728
Epoch 8/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0560 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0497 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0728
Epoch 9/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0451 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0411 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 10/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0380 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0355 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 11/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0334 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0318 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 12/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0304 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0294 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 13/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0284 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0278 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 14/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0271 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0268 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 15/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0262 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0261 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 16/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0257 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0257 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 17/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0253 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0254 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 18/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0251 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0252 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 19/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0249 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0250 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 20/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0248 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 21/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0247 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0728
Epoch 22/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0728
Epoch 23/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0250 - mae: 0.1541 - mse: 0.0249
Epoch 23: ReduceLROnPlateau reducing learning rate to 0.03637633100152016.
29/29 [==============================] - 0s 4ms/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0728
Epoch 24/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0364
Epoch 25/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0364
Epoch 26/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0364
Epoch 27/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0364
Epoch 28/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0364
Epoch 29/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0364
Epoch 30/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0364
Epoch 31/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0364
Epoch 32/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0364
Epoch 33/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0229 - mae: 0.1474 - mse: 0.0229
Epoch 33: ReduceLROnPlateau reducing learning rate to 0.01818816550076008.
29/29 [==============================] - 0s 4ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0364
Epoch 34/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0182
Epoch 35/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0182
Epoch 36/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0182
Epoch 37/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0182
Epoch 38/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0182
Epoch 39/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0182
Epoch 40/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0182
Epoch 41/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0182
Epoch 42/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0182
Epoch 43/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0237 - mae: 0.1497 - mse: 0.0236
Epoch 43: ReduceLROnPlateau reducing learning rate to 0.00909408275038004.
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0182
Epoch 44/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0091
Epoch 45/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0091
Epoch 46/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0091
Epoch 47/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0091
Epoch 48/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0091
Epoch 49/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0091
Epoch 50/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0091
Epoch 51/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0091
Epoch 52/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0091
Epoch 53/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0244 - mae: 0.1524 - mse: 0.0244
Epoch 53: ReduceLROnPlateau reducing learning rate to 0.00454704137519002.
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0091
Epoch 54/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0045
Epoch 55/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0045
Epoch 56/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0045
Epoch 57/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0045
Epoch 58/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0045
Epoch 59/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0045
Epoch 60/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0045
Epoch 61/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0045
Epoch 62/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0045
Epoch 63/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0244 - mae: 0.1523 - mse: 0.0244
Epoch 63: ReduceLROnPlateau reducing learning rate to 0.00227352068759501.
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0045
Epoch 64/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0023
Epoch 65/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0023
Epoch 66/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0023
Epoch 67/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0023
Epoch 68/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0023
Epoch 69/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0023
Epoch 70/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0023
Epoch 71/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0023
Epoch 72/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0023
Epoch 73/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0241 - mae: 0.1511 - mse: 0.0241
Epoch 73: ReduceLROnPlateau reducing learning rate to 0.001136760343797505.
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0023
Epoch 74/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0011
Epoch 75/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0011
Epoch 76/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0011
Epoch 77/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0011
Epoch 78/200
29/29 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0011
Epoch 79/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0011
Epoch 80/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0011
Epoch 81/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0011
Epoch 82/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0011
Epoch 83/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0241 - mae: 0.1513 - mse: 0.0241
Epoch 83: ReduceLROnPlateau reducing learning rate to 0.0005683801718987525.
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0011
Epoch 84/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.6838e-04
Epoch 85/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.6838e-04
Epoch 86/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.6838e-04
Epoch 87/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.6838e-04
Epoch 88/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.6838e-04
Epoch 89/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.6838e-04
Epoch 90/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.6838e-04
Epoch 91/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.6838e-04
Epoch 92/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.6838e-04
Epoch 93/200
24/29 [=======================>......] - ETA: 0s - loss: 0.0245 - mae: 0.1528 - mse: 0.0245
Epoch 93: ReduceLROnPlateau reducing learning rate to 0.0002841900859493762.
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 5.6838e-04
Epoch 94/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.8419e-04
Epoch 95/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.8419e-04
Epoch 96/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.8419e-04
Epoch 97/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.8419e-04
Epoch 98/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.8419e-04
Epoch 99/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.8419e-04
Epoch 100/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.8419e-04
Epoch 101/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.8419e-04
Epoch 102/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.8419e-04
Epoch 103/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0253 - mae: 0.1553 - mse: 0.0253
Epoch 103: ReduceLROnPlateau reducing learning rate to 0.0001420950429746881.
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 2.8419e-04
Epoch 104/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.4210e-04
Epoch 105/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.4210e-04
Epoch 106/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.4210e-04
Epoch 107/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.4210e-04
Epoch 108/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.4210e-04
Epoch 109/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.4210e-04
Epoch 110/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.4210e-04
Epoch 111/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.4210e-04
Epoch 112/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.4210e-04
Epoch 113/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0240 - mae: 0.1507 - mse: 0.0240
Epoch 113: ReduceLROnPlateau reducing learning rate to 7.104752148734406e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.4210e-04
Epoch 114/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.1048e-05
Epoch 115/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.1048e-05
Epoch 116/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.1048e-05
Epoch 117/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.1048e-05
Epoch 118/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.1048e-05
Epoch 119/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.1048e-05
Epoch 120/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.1048e-05
Epoch 121/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.1048e-05
Epoch 122/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.1048e-05
Epoch 123/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0249 - mae: 0.1539 - mse: 0.0248
Epoch 123: ReduceLROnPlateau reducing learning rate to 3.552376074367203e-05.
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.1048e-05
Epoch 124/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5524e-05
Epoch 125/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5524e-05
Epoch 126/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5524e-05
Epoch 127/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5524e-05
Epoch 128/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5524e-05
Epoch 129/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5524e-05
Epoch 130/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5524e-05
Epoch 131/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5524e-05
Epoch 132/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5524e-05
Epoch 133/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0243 - mae: 0.1520 - mse: 0.0243
Epoch 133: ReduceLROnPlateau reducing learning rate to 1.7761880371836014e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5524e-05
Epoch 134/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7762e-05
Epoch 135/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7762e-05
Epoch 136/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7762e-05
Epoch 137/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7762e-05
Epoch 138/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7762e-05
Epoch 139/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7762e-05
Epoch 140/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7762e-05
Epoch 141/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7762e-05
Epoch 142/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7762e-05
Epoch 143/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0239 - mae: 0.1509 - mse: 0.0239
Epoch 143: ReduceLROnPlateau reducing learning rate to 1e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7762e-05
Epoch 144/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 145/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 146/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 147/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 148/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 149/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 150/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 151/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 152/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 153/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 154/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 155/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 156/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 157/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 158/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 159/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 160/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 161/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 162/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 163/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 164/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 165/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 166/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 167/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 168/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 169/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 170/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 171/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 172/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 173/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 174/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 175/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 176/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 177/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 178/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 179/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 180/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 181/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 182/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 183/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 184/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 185/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 186/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 187/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 188/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 189/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 190/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 191/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 192/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 193/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 194/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 195/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 196/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 197/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 198/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 199/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 200/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.07275266159696196LR_[38]HN_136BS_10P_val_mseM_200epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
29/29 [==============================] - 1s 9ms/step - loss: 0.8164 - mae: 0.2161 - mse: 0.0633 - val_loss: 0.6459 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0728
Epoch 2/200
29/29 [==============================] - 0s 3ms/step - loss: 0.5366 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.4328 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0728
Epoch 3/200
29/29 [==============================] - 0s 3ms/step - loss: 0.3620 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.2951 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0728
Epoch 4/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2493 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.2061 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0728
Epoch 5/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1764 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.1486 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0728
Epoch 6/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1292 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.1113 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0728
Epoch 7/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0987 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0873 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0728
Epoch 8/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0790 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0717 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0728
Epoch 9/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0662 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0616 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0728
Epoch 10/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0579 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0550 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0728
Epoch 11/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0526 - mae: 0.2025 - mse: 0.0422
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.03637633100152016.
29/29 [==============================] - 0s 4ms/step - loss: 0.0525 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0507 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0728
Epoch 12/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0495 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0490 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0364
Epoch 13/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0481 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0478 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0364
Epoch 14/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0471 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0469 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0364
Epoch 15/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0462 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0461 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0364
Epoch 16/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0455 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0454 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0364
Epoch 17/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0449 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0449 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0364
Epoch 18/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0444 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0445 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0364
Epoch 19/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0441 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0442 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0364
Epoch 20/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0437 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0439 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0364
Epoch 21/200
13/29 [============>.................] - ETA: 0s - loss: 0.0433 - mae: 0.2019 - mse: 0.0419
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.01818816550076008.
29/29 [==============================] - 0s 5ms/step - loss: 0.0435 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0436 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0364
Epoch 22/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0433 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0435 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0182
Epoch 23/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0432 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0434 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0182
Epoch 24/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0431 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0434 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0182
Epoch 25/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0431 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0433 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0182
Epoch 26/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0430 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0432 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0182
Epoch 27/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0429 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0432 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0182
Epoch 28/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0429 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0431 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0182
Epoch 29/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0428 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0431 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0182
Epoch 30/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0428 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0430 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0182
Epoch 31/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0427 - mae: 0.2026 - mse: 0.0422
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00909408275038004.
29/29 [==============================] - 0s 5ms/step - loss: 0.0427 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0430 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0182
Epoch 32/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0427 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0430 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0091
Epoch 33/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0427 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0429 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0091
Epoch 34/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0427 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0429 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0091
Epoch 35/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0426 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0429 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0091
Epoch 36/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0426 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0429 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0091
Epoch 37/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0426 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0429 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0091
Epoch 38/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0426 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0429 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0091
Epoch 39/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0426 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0091
Epoch 40/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0426 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0091
Epoch 41/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0447 - mae: 0.2079 - mse: 0.0443
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00454704137519002.
29/29 [==============================] - 0s 3ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0091
Epoch 42/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0045
Epoch 43/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0045
Epoch 44/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0045
Epoch 45/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0045
Epoch 46/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0045
Epoch 47/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0045
Epoch 48/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0045
Epoch 49/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0045
Epoch 50/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0045
Epoch 51/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0423 - mae: 0.2020 - mse: 0.0420
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00227352068759501.
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0045
Epoch 52/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0023
Epoch 53/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0023
Epoch 54/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0023
Epoch 55/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0023
Epoch 56/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0023
Epoch 57/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0023
Epoch 58/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0023
Epoch 59/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0023
Epoch 60/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0023
Epoch 61/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0406 - mae: 0.1978 - mse: 0.0403
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.001136760343797505.
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0023
Epoch 62/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0011
Epoch 63/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0011
Epoch 64/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0011
Epoch 65/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0011
Epoch 66/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0011
Epoch 67/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0011
Epoch 68/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0011
Epoch 69/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0011
Epoch 70/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0011
Epoch 71/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0425 - mae: 0.2025 - mse: 0.0422
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0005683801718987525.
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0011
Epoch 72/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 5.6838e-04
Epoch 73/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 5.6838e-04
Epoch 74/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 5.6838e-04
Epoch 75/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 5.6838e-04
Epoch 76/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 5.6838e-04
Epoch 77/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 5.6838e-04
Epoch 78/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 5.6838e-04
Epoch 79/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 5.6838e-04
Epoch 80/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 5.6838e-04
Epoch 81/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0428 - mae: 0.2031 - mse: 0.0425
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0002841900859493762.
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 5.6838e-04
Epoch 82/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.8419e-04
Epoch 83/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.8419e-04
Epoch 84/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.8419e-04
Epoch 85/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.8419e-04
Epoch 86/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.8419e-04
Epoch 87/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.8419e-04
Epoch 88/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.8419e-04
Epoch 89/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.8419e-04
Epoch 90/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.8419e-04
Epoch 91/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0418 - mae: 0.2005 - mse: 0.0416
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0001420950429746881.
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.8419e-04
Epoch 92/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.4210e-04
Epoch 93/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.4210e-04
Epoch 94/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.4210e-04
Epoch 95/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.4210e-04
Epoch 96/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.4210e-04
Epoch 97/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.4210e-04
Epoch 98/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.4210e-04
Epoch 99/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.4210e-04
Epoch 100/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.4210e-04
Epoch 101/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0436 - mae: 0.2053 - mse: 0.0433
Epoch 101: ReduceLROnPlateau reducing learning rate to 7.104752148734406e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.4210e-04
Epoch 102/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.1048e-05
Epoch 103/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.1048e-05
Epoch 104/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.1048e-05
Epoch 105/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.1048e-05
Epoch 106/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.1048e-05
Epoch 107/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.1048e-05
Epoch 108/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.1048e-05
Epoch 109/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.1048e-05
Epoch 110/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.1048e-05
Epoch 111/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0437 - mae: 0.2056 - mse: 0.0434
Epoch 111: ReduceLROnPlateau reducing learning rate to 3.552376074367203e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.1048e-05
Epoch 112/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5524e-05
Epoch 113/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5524e-05
Epoch 114/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5524e-05
Epoch 115/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5524e-05
Epoch 116/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5524e-05
Epoch 117/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5524e-05
Epoch 118/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5524e-05
Epoch 119/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5524e-05
Epoch 120/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5524e-05
Epoch 121/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0425 - mae: 0.2028 - mse: 0.0423
Epoch 121: ReduceLROnPlateau reducing learning rate to 1.7761880371836014e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5524e-05
Epoch 122/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7762e-05
Epoch 123/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7762e-05
Epoch 124/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7762e-05
Epoch 125/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7762e-05
Epoch 126/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7762e-05
Epoch 127/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7762e-05
Epoch 128/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7762e-05
Epoch 129/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7762e-05
Epoch 130/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7762e-05
Epoch 131/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0424 - mae: 0.2024 - mse: 0.0421
Epoch 131: ReduceLROnPlateau reducing learning rate to 1e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7762e-05
Epoch 132/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 133/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 134/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 135/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 136/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 137/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 138/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 139/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 140/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 141/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 142/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 143/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 144/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 145/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 146/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 147/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 148/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 149/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 150/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 151/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 152/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 153/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 154/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 155/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 156/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 157/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 158/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 159/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 160/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 161/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 162/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 163/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 164/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 165/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 166/200
29/29 [==============================] - 0s 6ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 167/200
29/29 [==============================] - 0s 6ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 168/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 169/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 170/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 171/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 172/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 173/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 174/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 175/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 176/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 177/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 178/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 179/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 180/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 181/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 182/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 183/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 184/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 185/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 186/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 187/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 188/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 189/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 190/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 191/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 192/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 193/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 194/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 195/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 196/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 197/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 198/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 199/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 200/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.07275266159696196LR_[38]HN_136BS_10P_val_mseM_200epochs/model_4.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
29/29 [==============================] - 1s 12ms/step - loss: 0.7051 - mae: 0.2535 - mse: 0.0811 - val_loss: 0.5654 - val_mae: 0.2532 - val_mse: 0.0656 - lr: 0.0728
Epoch 2/200
29/29 [==============================] - 0s 4ms/step - loss: 0.4746 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.3887 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 3/200
29/29 [==============================] - 0s 3ms/step - loss: 0.3299 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.2745 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 4/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2363 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.2006 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 5/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1759 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.1529 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 6/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1368 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.1220 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 7/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1115 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.1021 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 8/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0951 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0891 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 9/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0845 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0808 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 10/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0777 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0754 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 11/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0732 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0718 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 12/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0709 - mae: 0.2515 - mse: 0.0644
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.03637633100152016.
29/29 [==============================] - 0s 4ms/step - loss: 0.0703 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0696 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0728
Epoch 13/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0687 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0686 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0364
Epoch 14/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0680 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0680 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0364
Epoch 15/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0674 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0675 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0364
Epoch 16/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0669 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0671 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0364
Epoch 17/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0666 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0667 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0364
Epoch 18/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0663 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0665 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0364
Epoch 19/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0660 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0663 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0364
Epoch 20/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0658 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0661 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0364
Epoch 21/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0657 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0659 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0364
Epoch 22/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0657 - mae: 0.2528 - mse: 0.0650
Epoch 22: ReduceLROnPlateau reducing learning rate to 0.01818816550076008.
29/29 [==============================] - 0s 4ms/step - loss: 0.0655 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0658 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0364
Epoch 23/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0654 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0657 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0182
Epoch 24/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0654 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0657 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0182
Epoch 25/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0653 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0657 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0182
Epoch 26/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0653 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0656 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0182
Epoch 27/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0653 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0656 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0182
Epoch 28/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0652 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0656 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0182
Epoch 29/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0652 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0655 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0182
Epoch 30/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0652 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0655 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0182
Epoch 31/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0652 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0655 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0182
Epoch 32/200
27/29 [==========================>...] - ETA: 0s - loss: 0.0651 - mae: 0.2525 - mse: 0.0649
Epoch 32: ReduceLROnPlateau reducing learning rate to 0.00909408275038004.
29/29 [==============================] - 0s 4ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0655 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0182
Epoch 33/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0655 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0091
Epoch 34/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0655 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0091
Epoch 35/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0091
Epoch 36/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0091
Epoch 37/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0091
Epoch 38/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0091
Epoch 39/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0091
Epoch 40/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0091
Epoch 41/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0091
Epoch 42/200
29/29 [==============================] - ETA: 0s - loss: 0.0651 - mae: 0.2525 - mse: 0.0649
Epoch 42: ReduceLROnPlateau reducing learning rate to 0.00454704137519002.
29/29 [==============================] - 0s 5ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0091
Epoch 43/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0045
Epoch 44/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0045
Epoch 45/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0045
Epoch 46/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0045
Epoch 47/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0045
Epoch 48/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0045
Epoch 49/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0045
Epoch 50/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0045
Epoch 51/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0045
Epoch 52/200
25/29 [========================>.....] - ETA: 0s - loss: 0.0651 - mae: 0.2526 - mse: 0.0650
Epoch 52: ReduceLROnPlateau reducing learning rate to 0.00227352068759501.
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0045
Epoch 53/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0023
Epoch 54/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0023
Epoch 55/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0023
Epoch 56/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0023
Epoch 57/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0023
Epoch 58/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0023
Epoch 59/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0023
Epoch 60/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0023
Epoch 61/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0023
Epoch 62/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0656 - mae: 0.2535 - mse: 0.0655
Epoch 62: ReduceLROnPlateau reducing learning rate to 0.001136760343797505.
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0023
Epoch 63/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 64/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 65/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 66/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 67/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 68/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 69/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 70/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 71/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 72/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0647 - mae: 0.2518 - mse: 0.0646
Epoch 72: ReduceLROnPlateau reducing learning rate to 0.0005683801718987525.
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 73/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.6838e-04
Epoch 74/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.6838e-04
Epoch 75/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.6838e-04
Epoch 76/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.6838e-04
Epoch 77/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.6838e-04
Epoch 78/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.6838e-04
Epoch 79/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.6838e-04
Epoch 80/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.6838e-04
Epoch 81/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.6838e-04
Epoch 82/200
23/29 [======================>.......] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.0649
Epoch 82: ReduceLROnPlateau reducing learning rate to 0.0002841900859493762.
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.6838e-04
Epoch 83/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.8419e-04
Epoch 84/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.8419e-04
Epoch 85/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.8419e-04
Epoch 86/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.8419e-04
Epoch 87/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.8419e-04
Epoch 88/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.8419e-04
Epoch 89/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.8419e-04
Epoch 90/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.8419e-04
Epoch 91/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.8419e-04
Epoch 92/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0654 - mae: 0.2536 - mse: 0.0654
Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0001420950429746881.
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.8419e-04
Epoch 93/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.4210e-04
Epoch 94/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.4210e-04
Epoch 95/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.4210e-04
Epoch 96/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.4210e-04
Epoch 97/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.4210e-04
Epoch 98/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.4210e-04
Epoch 99/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.4210e-04
Epoch 100/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.4210e-04
Epoch 101/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.4210e-04
Epoch 102/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0635 - mae: 0.2498 - mse: 0.0634
Epoch 102: ReduceLROnPlateau reducing learning rate to 7.104752148734406e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.4210e-04
Epoch 103/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.1048e-05
Epoch 104/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.1048e-05
Epoch 105/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.1048e-05
Epoch 106/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.1048e-05
Epoch 107/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.1048e-05
Epoch 108/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.1048e-05
Epoch 109/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.1048e-05
Epoch 110/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.1048e-05
Epoch 111/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.1048e-05
Epoch 112/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0653 - mae: 0.2531 - mse: 0.0652
Epoch 112: ReduceLROnPlateau reducing learning rate to 3.552376074367203e-05.
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.1048e-05
Epoch 113/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.5524e-05
Epoch 114/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.5524e-05
Epoch 115/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.5524e-05
Epoch 116/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.5524e-05
Epoch 117/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.5524e-05
Epoch 118/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.5524e-05
Epoch 119/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.5524e-05
Epoch 120/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.5524e-05
Epoch 121/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.5524e-05
Epoch 122/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0664 - mae: 0.2552 - mse: 0.0664
Epoch 122: ReduceLROnPlateau reducing learning rate to 1.7761880371836014e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.5524e-05
Epoch 123/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.7762e-05
Epoch 124/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.7762e-05
Epoch 125/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.7762e-05
Epoch 126/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.7762e-05
Epoch 127/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.7762e-05
Epoch 128/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.7762e-05
Epoch 129/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.7762e-05
Epoch 130/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.7762e-05
Epoch 131/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.7762e-05
Epoch 132/200
26/29 [=========================>....] - ETA: 0s - loss: 0.0650 - mae: 0.2526 - mse: 0.0650
Epoch 132: ReduceLROnPlateau reducing learning rate to 1e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.7762e-05
Epoch 133/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 134/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 135/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 136/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 137/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 138/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 139/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 140/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 141/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 142/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 143/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 144/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 145/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 146/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 147/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 148/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 149/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 150/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 151/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 152/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 153/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 154/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 155/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 156/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 157/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 158/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 159/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 160/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 161/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 162/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 163/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 164/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 165/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 166/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 167/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 168/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 169/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 170/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 171/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 172/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 173/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 174/200
25/29 [========================>.....] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.0649
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0023s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0043s). Check your callbacks.
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 175/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 176/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 177/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 178/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 179/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 180/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 181/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 182/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 183/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 184/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 185/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 186/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 187/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 188/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 189/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 190/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 191/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 192/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 193/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 194/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 195/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 196/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 197/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 198/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 199/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 200/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.07275266159696196LR_[38]HN_136BS_10P_val_mseM_200epochs/model_5.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
29/29 [==============================] - 1s 8ms/step - loss: 0.7345 - mae: 0.3025 - mse: 0.0992 - val_loss: 0.6015 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 2/200
29/29 [==============================] - 0s 3ms/step - loss: 0.5091 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.4218 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 3/200
29/29 [==============================] - 0s 2ms/step - loss: 0.3620 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.3057 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 4/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2669 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.2307 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 5/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2054 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1822 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 6/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1657 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1508 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 7/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1400 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1305 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 8/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1234 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1174 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 9/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1126 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1089 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 10/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1057 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1034 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 11/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1019 - mae: 0.3008 - mse: 0.0916
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.03637633100152016.
29/29 [==============================] - 0s 3ms/step - loss: 0.1012 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0998 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0728
Epoch 12/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0986 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0984 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0364
Epoch 13/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0975 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0974 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0364
Epoch 14/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0966 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0966 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0364
Epoch 15/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0959 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0960 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0364
Epoch 16/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0953 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0954 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0364
Epoch 17/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0948 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0950 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0364
Epoch 18/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0944 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0947 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0364
Epoch 19/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0941 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0944 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0364
Epoch 20/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0939 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0942 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0364
Epoch 21/200
29/29 [==============================] - ETA: 0s - loss: 0.0936 - mae: 0.3025 - mse: 0.0927
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.01818816550076008.
29/29 [==============================] - 0s 4ms/step - loss: 0.0936 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0940 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0364
Epoch 22/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0935 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0939 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0182
Epoch 23/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0934 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0938 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0182
Epoch 24/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0934 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0937 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0182
Epoch 25/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0933 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0937 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0182
Epoch 26/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0932 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0936 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0182
Epoch 27/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0932 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0936 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0182
Epoch 28/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0931 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0935 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0182
Epoch 29/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0931 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0935 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0182
Epoch 30/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0931 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0935 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0182
Epoch 31/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0945 - mae: 0.3050 - mse: 0.0941
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00909408275038004.
29/29 [==============================] - 0s 3ms/step - loss: 0.0930 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0934 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0182
Epoch 32/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0930 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0934 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0091
Epoch 33/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0930 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0934 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0091
Epoch 34/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0930 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0934 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0091
Epoch 35/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0930 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0934 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0091
Epoch 36/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0930 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0934 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0091
Epoch 37/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0934 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0091
Epoch 38/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0091
Epoch 39/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0091
Epoch 40/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0091
Epoch 41/200
28/29 [===========================>..] - ETA: 0s - loss: 0.0929 - mae: 0.3025 - mse: 0.0927
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00454704137519002.
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0091
Epoch 42/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0045
Epoch 43/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0045
Epoch 44/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0045
Epoch 45/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0045
Epoch 46/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0045
Epoch 47/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0045
Epoch 48/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0045
Epoch 49/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0045
Epoch 50/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0045
Epoch 51/200
21/29 [====================>.........] - ETA: 0s - loss: 0.0926 - mae: 0.3021 - mse: 0.0924
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00227352068759501.
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0045
Epoch 52/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0023
Epoch 53/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0023
Epoch 54/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0023
Epoch 55/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0023
Epoch 56/200
29/29 [==============================] - 0s 6ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0023
Epoch 57/200
29/29 [==============================] - 0s 6ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0023
Epoch 58/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0023
Epoch 59/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0023
Epoch 60/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0023
Epoch 61/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0929 - mae: 0.3028 - mse: 0.0928
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.001136760343797505.
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0023
Epoch 62/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 63/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 64/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 65/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 66/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 67/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 68/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 69/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 70/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 71/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0930 - mae: 0.3027 - mse: 0.0928
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0005683801718987525.
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 72/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.6838e-04
Epoch 73/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.6838e-04
Epoch 74/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.6838e-04
Epoch 75/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.6838e-04
Epoch 76/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.6838e-04
Epoch 77/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.6838e-04
Epoch 78/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.6838e-04
Epoch 79/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.6838e-04
Epoch 80/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.6838e-04
Epoch 81/200
27/29 [==========================>...] - ETA: 0s - loss: 0.0928 - mae: 0.3024 - mse: 0.0926
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0002841900859493762.
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.6838e-04
Epoch 82/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.8419e-04
Epoch 83/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.8419e-04
Epoch 84/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.8419e-04
Epoch 85/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.8419e-04
Epoch 86/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.8419e-04
Epoch 87/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.8419e-04
Epoch 88/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.8419e-04
Epoch 89/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.8419e-04
Epoch 90/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.8419e-04
Epoch 91/200
29/29 [==============================] - ETA: 0s - loss: 0.0928 - mae: 0.3025 - mse: 0.0927
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0001420950429746881.
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.8419e-04
Epoch 92/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.4210e-04
Epoch 93/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.4210e-04
Epoch 94/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.4210e-04
Epoch 95/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.4210e-04
Epoch 96/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.4210e-04
Epoch 97/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.4210e-04
Epoch 98/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.4210e-04
Epoch 99/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.4210e-04
Epoch 100/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.4210e-04
Epoch 101/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0939 - mae: 0.3042 - mse: 0.0938
Epoch 101: ReduceLROnPlateau reducing learning rate to 7.104752148734406e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.4210e-04
Epoch 102/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.1048e-05
Epoch 103/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.1048e-05
Epoch 104/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.1048e-05
Epoch 105/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.1048e-05
Epoch 106/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.1048e-05
Epoch 107/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.1048e-05
Epoch 108/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.1048e-05
Epoch 109/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.1048e-05
Epoch 110/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.1048e-05
Epoch 111/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0923 - mae: 0.3017 - mse: 0.0922
Epoch 111: ReduceLROnPlateau reducing learning rate to 3.552376074367203e-05.
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.1048e-05
Epoch 112/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.5524e-05
Epoch 113/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.5524e-05
Epoch 114/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.5524e-05
Epoch 115/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.5524e-05
Epoch 116/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.5524e-05
Epoch 117/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.5524e-05
Epoch 118/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.5524e-05
Epoch 119/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.5524e-05
Epoch 120/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.5524e-05
Epoch 121/200
29/29 [==============================] - ETA: 0s - loss: 0.0928 - mae: 0.3025 - mse: 0.0927
Epoch 121: ReduceLROnPlateau reducing learning rate to 1.7761880371836014e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.5524e-05
Epoch 122/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.7762e-05
Epoch 123/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.7762e-05
Epoch 124/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.7762e-05
Epoch 125/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.7762e-05
Epoch 126/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.7762e-05
Epoch 127/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.7762e-05
Epoch 128/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.7762e-05
Epoch 129/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.7762e-05
Epoch 130/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.7762e-05
Epoch 131/200
 1/29 [>.............................] - ETA: 0s - loss: 0.0912 - mae: 0.2995 - mse: 0.0910
Epoch 131: ReduceLROnPlateau reducing learning rate to 1e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.7762e-05
Epoch 132/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 133/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 134/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 135/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 136/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 137/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 138/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 139/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 140/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 141/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 142/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 143/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 144/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 145/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 146/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 147/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 148/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 149/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 150/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 151/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 152/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 153/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 154/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 155/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 156/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 157/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 158/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 159/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 160/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 161/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 162/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 163/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 164/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 165/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 166/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 167/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 168/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 169/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 170/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 171/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 172/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 173/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 174/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 175/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 176/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 177/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 178/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 179/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 180/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 181/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 182/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 183/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 184/200
29/29 [==============================] - 0s 5ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 185/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 186/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 187/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 188/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 189/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 190/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 191/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 192/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 193/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 194/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 195/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 196/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 197/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 198/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 199/200
29/29 [==============================] - 0s 4ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 200/200
29/29 [==============================] - 0s 3ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.07275266159696196LR_[38]HN_136BS_10P_val_mseM_200epochs/model_6.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
29/29 [==============================] - 1s 8ms/step - loss: 0.8072 - mae: 0.3525 - mse: 0.1292 - val_loss: 0.6682 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 2/200
29/29 [==============================] - 0s 3ms/step - loss: 0.5696 - mae: 0.3525 - mse: 0.1255 - val_loss: 0.4765 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 3/200
29/29 [==============================] - 0s 3ms/step - loss: 0.4125 - mae: 0.3525 - mse: 0.1255 - val_loss: 0.3525 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 4/200
29/29 [==============================] - 0s 3ms/step - loss: 0.3110 - mae: 0.3525 - mse: 0.1255 - val_loss: 0.2724 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 5/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2454 - mae: 0.3525 - mse: 0.1255 - val_loss: 0.2207 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 6/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2030 - mae: 0.3525 - mse: 0.1255 - val_loss: 0.1872 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 7/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1756 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1656 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 8/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1579 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1516 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 9/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1465 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1425 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 10/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1391 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1367 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 11/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1366 - mae: 0.3531 - mse: 0.1258
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.03637633100152016.
29/29 [==============================] - 0s 4ms/step - loss: 0.1343 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1329 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0728
Epoch 12/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1316 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1314 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0364
Epoch 13/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1304 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1303 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0364
Epoch 14/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1294 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1295 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0364
Epoch 15/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1287 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1288 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0364
Epoch 16/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1281 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1283 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0364
Epoch 17/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1276 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1278 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0364
Epoch 18/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1272 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1275 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0364
Epoch 19/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1268 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1272 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0364
Epoch 20/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1266 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1269 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0364
Epoch 21/200
28/29 [===========================>..] - ETA: 0s - loss: 0.1264 - mae: 0.3526 - mse: 0.1255
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.01818816550076008.
29/29 [==============================] - 0s 4ms/step - loss: 0.1264 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1267 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0364
Epoch 22/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1262 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1266 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0182
Epoch 23/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1261 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1266 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0182
Epoch 24/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1261 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1265 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0182
Epoch 25/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1260 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1265 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0182
Epoch 26/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1259 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1264 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0182
Epoch 27/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1259 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1264 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0182
Epoch 28/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1259 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1263 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0182
Epoch 29/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1258 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1263 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0182
Epoch 30/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1258 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1262 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0182
Epoch 31/200
27/29 [==========================>...] - ETA: 0s - loss: 0.1257 - mae: 0.3525 - mse: 0.1254
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00909408275038004.
29/29 [==============================] - 0s 5ms/step - loss: 0.1257 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1262 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0182
Epoch 32/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1257 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1262 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0091
Epoch 33/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1257 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1262 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0091
Epoch 34/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1257 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1262 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0091
Epoch 35/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1257 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1262 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0091
Epoch 36/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1257 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0091
Epoch 37/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1257 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0091
Epoch 38/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0091
Epoch 39/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0091
Epoch 40/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0091
Epoch 41/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1247 - mae: 0.3510 - mse: 0.1245
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00454704137519002.
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0091
Epoch 42/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0045
Epoch 43/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0045
Epoch 44/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0045
Epoch 45/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0045
Epoch 46/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0045
Epoch 47/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0045
Epoch 48/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0045
Epoch 49/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0045
Epoch 50/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0045
Epoch 51/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1243 - mae: 0.3505 - mse: 0.1242
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00227352068759501.
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0045
Epoch 52/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0023
Epoch 53/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0023
Epoch 54/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0023
Epoch 55/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0023
Epoch 56/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0023
Epoch 57/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0023
Epoch 58/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0023
Epoch 59/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0023
Epoch 60/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0023
Epoch 61/200
28/29 [===========================>..] - ETA: 0s - loss: 0.1256 - mae: 0.3525 - mse: 0.1254
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.001136760343797505.
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0023
Epoch 62/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 63/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 64/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 65/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 66/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 67/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 68/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 69/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 70/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 71/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1253 - mae: 0.3522 - mse: 0.1251
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0005683801718987525.
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 72/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.6838e-04
Epoch 73/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.6838e-04
Epoch 74/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.6838e-04
Epoch 75/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.6838e-04
Epoch 76/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.6838e-04
Epoch 77/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.6838e-04
Epoch 78/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.6838e-04
Epoch 79/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.6838e-04
Epoch 80/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.6838e-04
Epoch 81/200
28/29 [===========================>..] - ETA: 0s - loss: 0.1255 - mae: 0.3525 - mse: 0.1254
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0002841900859493762.
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.6838e-04
Epoch 82/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.8419e-04
Epoch 83/200
29/29 [==============================] - 0s 6ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.8419e-04
Epoch 84/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.8419e-04
Epoch 85/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.8419e-04
Epoch 86/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.8419e-04
Epoch 87/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.8419e-04
Epoch 88/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.8419e-04
Epoch 89/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.8419e-04
Epoch 90/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.8419e-04
Epoch 91/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1248 - mae: 0.3514 - mse: 0.1247
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0001420950429746881.
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.8419e-04
Epoch 92/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.4210e-04
Epoch 93/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.4210e-04
Epoch 94/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.4210e-04
Epoch 95/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.4210e-04
Epoch 96/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.4210e-04
Epoch 97/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.4210e-04
Epoch 98/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.4210e-04
Epoch 99/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.4210e-04
Epoch 100/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.4210e-04
Epoch 101/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1273 - mae: 0.3550 - mse: 0.1272
Epoch 101: ReduceLROnPlateau reducing learning rate to 7.104752148734406e-05.
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.4210e-04
Epoch 102/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.1048e-05
Epoch 103/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.1048e-05
Epoch 104/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.1048e-05
Epoch 105/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.1048e-05
Epoch 106/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.1048e-05
Epoch 107/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.1048e-05
Epoch 108/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.1048e-05
Epoch 109/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.1048e-05
Epoch 110/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.1048e-05
Epoch 111/200
28/29 [===========================>..] - ETA: 0s - loss: 0.1256 - mae: 0.3526 - mse: 0.1255
Epoch 111: ReduceLROnPlateau reducing learning rate to 3.552376074367203e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.1048e-05
Epoch 112/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.5524e-05
Epoch 113/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.5524e-05
Epoch 114/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.5524e-05
Epoch 115/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.5524e-05
Epoch 116/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.5524e-05
Epoch 117/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.5524e-05
Epoch 118/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.5524e-05
Epoch 119/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.5524e-05
Epoch 120/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.5524e-05
Epoch 121/200
29/29 [==============================] - ETA: 0s - loss: 0.1255 - mae: 0.3525 - mse: 0.1254
Epoch 121: ReduceLROnPlateau reducing learning rate to 1.7761880371836014e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.5524e-05
Epoch 122/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.7762e-05
Epoch 123/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.7762e-05
Epoch 124/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.7762e-05
Epoch 125/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.7762e-05
Epoch 126/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.7762e-05
Epoch 127/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.7762e-05
Epoch 128/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.7762e-05
Epoch 129/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.7762e-05
Epoch 130/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.7762e-05
Epoch 131/200
28/29 [===========================>..] - ETA: 0s - loss: 0.1255 - mae: 0.3525 - mse: 0.1254
Epoch 131: ReduceLROnPlateau reducing learning rate to 1e-05.
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.7762e-05
Epoch 132/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 133/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 134/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 135/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 136/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 137/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 138/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 139/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 140/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 141/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 142/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 143/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 144/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 145/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 146/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 147/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 148/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 149/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 150/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 151/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 152/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 153/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 154/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 155/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 156/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 157/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 158/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 159/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 160/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 161/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 162/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 163/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 164/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 165/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 166/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 167/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 168/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 169/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 170/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 171/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 172/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 173/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 174/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 175/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 176/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 177/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 178/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 179/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 180/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 181/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 182/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 183/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 184/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 185/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 186/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 187/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 188/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 189/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 190/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 191/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 192/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 193/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 194/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 195/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 196/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 197/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 198/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 199/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 200/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.07275266159696196LR_[38]HN_136BS_10P_val_mseM_200epochs/model_7.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
29/29 [==============================] - 1s 9ms/step - loss: 0.8300 - mae: 0.4025 - mse: 0.1646 - val_loss: 0.6958 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 2/200
29/29 [==============================] - 0s 3ms/step - loss: 0.5989 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.5076 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 3/200
29/29 [==============================] - 0s 3ms/step - loss: 0.4448 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.3860 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 4/200
29/29 [==============================] - 0s 3ms/step - loss: 0.3452 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.3073 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 5/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2808 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.2565 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 6/200
29/29 [==============================] - 0s 5ms/step - loss: 0.2392 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.2237 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 7/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2123 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.2025 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 8/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1949 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1888 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 9/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1837 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1799 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 10/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1765 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1742 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 11/200
25/29 [========================>.....] - ETA: 0s - loss: 0.1722 - mae: 0.4028 - mse: 0.1634
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.03637633100152016.
29/29 [==============================] - 0s 5ms/step - loss: 0.1718 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1705 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0728
Epoch 12/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1692 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1690 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0364
Epoch 13/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1680 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1680 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0364
Epoch 14/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1671 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1672 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0364
Epoch 15/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1663 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1665 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0364
Epoch 16/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1657 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1660 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0364
Epoch 17/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1652 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1656 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0364
Epoch 18/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1648 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1652 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0364
Epoch 19/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1645 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1649 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0364
Epoch 20/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1643 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1647 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0364
Epoch 21/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1621 - mae: 0.4000 - mse: 0.1611
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.01818816550076008.
29/29 [==============================] - 0s 3ms/step - loss: 0.1641 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1645 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0364
Epoch 22/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1639 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1644 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0182
Epoch 23/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1638 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1643 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0182
Epoch 24/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1638 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1643 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0182
Epoch 25/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1637 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1642 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0182
Epoch 26/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1637 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1642 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0182
Epoch 27/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1636 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1641 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0182
Epoch 28/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1636 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1641 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0182
Epoch 29/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1635 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1641 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0182
Epoch 30/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1635 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1640 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0182
Epoch 31/200
23/29 [======================>.......] - ETA: 0s - loss: 0.1632 - mae: 0.4022 - mse: 0.1629
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00909408275038004.
29/29 [==============================] - 0s 5ms/step - loss: 0.1635 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1640 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0182
Epoch 32/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1634 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1640 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0091
Epoch 33/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1634 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1640 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0091
Epoch 34/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1634 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1640 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0091
Epoch 35/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1634 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0091
Epoch 36/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1634 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0091
Epoch 37/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1634 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0091
Epoch 38/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1634 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0091
Epoch 39/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1634 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0091
Epoch 40/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1634 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0091
Epoch 41/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1605 - mae: 0.3989 - mse: 0.1603
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00454704137519002.
29/29 [==============================] - 0s 5ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0091
Epoch 42/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0045
Epoch 43/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0045
Epoch 44/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0045
Epoch 45/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0045
Epoch 46/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0045
Epoch 47/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0045
Epoch 48/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0045
Epoch 49/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0045
Epoch 50/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0045
Epoch 51/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1612 - mae: 0.3998 - mse: 0.1611
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00227352068759501.
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1639 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0045
Epoch 52/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0023
Epoch 53/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0023
Epoch 54/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0023
Epoch 55/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0023
Epoch 56/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0023
Epoch 57/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0023
Epoch 58/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0023
Epoch 59/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0023
Epoch 60/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0023
Epoch 61/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1660 - mae: 0.4059 - mse: 0.1659
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.001136760343797505.
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0023
Epoch 62/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 63/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 64/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 65/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 66/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 67/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 68/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 69/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 70/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 71/200
27/29 [==========================>...] - ETA: 0s - loss: 0.1633 - mae: 0.4025 - mse: 0.1632
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0005683801718987525.
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 72/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.6838e-04
Epoch 73/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.6838e-04
Epoch 74/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.6838e-04
Epoch 75/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.6838e-04
Epoch 76/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.6838e-04
Epoch 77/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.6838e-04
Epoch 78/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.6838e-04
Epoch 79/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.6838e-04
Epoch 80/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.6838e-04
Epoch 81/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1619 - mae: 0.4009 - mse: 0.1618
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0002841900859493762.
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.6838e-04
Epoch 82/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.8419e-04
Epoch 83/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.8419e-04
Epoch 84/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.8419e-04
Epoch 85/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.8419e-04
Epoch 86/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.8419e-04
Epoch 87/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.8419e-04
Epoch 88/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.8419e-04
Epoch 89/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.8419e-04
Epoch 90/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.8419e-04
Epoch 91/200
29/29 [==============================] - ETA: 0s - loss: 0.1633 - mae: 0.4025 - mse: 0.1632
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0001420950429746881.
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.8419e-04
Epoch 92/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.4210e-04
Epoch 93/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.4210e-04
Epoch 94/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.4210e-04
Epoch 95/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.4210e-04
Epoch 96/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.4210e-04
Epoch 97/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.4210e-04
Epoch 98/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.4210e-04
Epoch 99/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.4210e-04
Epoch 100/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.4210e-04
Epoch 101/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1650 - mae: 0.4047 - mse: 0.1649
Epoch 101: ReduceLROnPlateau reducing learning rate to 7.104752148734406e-05.
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.4210e-04
Epoch 102/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 7.1048e-05
Epoch 103/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 7.1048e-05
Epoch 104/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 7.1048e-05
Epoch 105/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 7.1048e-05
Epoch 106/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 7.1048e-05
Epoch 107/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 7.1048e-05
Epoch 108/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 7.1048e-05
Epoch 109/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 7.1048e-05
Epoch 110/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 7.1048e-05
Epoch 111/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1614 - mae: 0.4001 - mse: 0.1613
Epoch 111: ReduceLROnPlateau reducing learning rate to 3.552376074367203e-05.
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 7.1048e-05
Epoch 112/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.5524e-05
Epoch 113/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.5524e-05
Epoch 114/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.5524e-05
Epoch 115/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.5524e-05
Epoch 116/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.5524e-05
Epoch 117/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.5524e-05
Epoch 118/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.5524e-05
Epoch 119/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.5524e-05
Epoch 120/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.5524e-05
Epoch 121/200
 1/29 [>.............................] - ETA: 0s - loss: 0.1643 - mae: 0.4039 - mse: 0.1642
Epoch 121: ReduceLROnPlateau reducing learning rate to 1.7761880371836014e-05.
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.5524e-05
Epoch 122/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.7762e-05
Epoch 123/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.7762e-05
Epoch 124/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.7762e-05
Epoch 125/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.7762e-05
Epoch 126/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.7762e-05
Epoch 127/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.7762e-05
Epoch 128/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.7762e-05
Epoch 129/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.7762e-05
Epoch 130/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.7762e-05
Epoch 131/200
25/29 [========================>.....] - ETA: 0s - loss: 0.1633 - mae: 0.4026 - mse: 0.1632
Epoch 131: ReduceLROnPlateau reducing learning rate to 1e-05.
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.7762e-05
Epoch 132/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 133/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 134/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 135/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 136/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 137/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 138/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 139/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 140/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 141/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 142/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 143/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 144/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 145/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 146/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 147/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 148/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 149/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 150/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 151/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 152/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 153/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 154/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 155/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 156/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 157/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 158/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 159/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 160/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 161/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 162/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 163/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 164/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 165/200
29/29 [==============================] - 0s 5ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 166/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 167/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 168/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 169/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 170/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 171/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 172/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 173/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 174/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 175/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 176/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 177/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 178/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 179/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 180/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 181/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 182/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 183/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 184/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 185/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 186/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 187/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 188/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 189/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 190/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 191/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 192/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 193/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 194/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 195/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 196/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 197/200
29/29 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 198/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 199/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 200/200
29/29 [==============================] - 0s 4ms/step - loss: 0.1633 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.07275266159696196LR_[38]HN_136BS_10P_val_mseM_200epochs/model_8.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
29/29 [==============================] - 1s 12ms/step - loss: 0.8168 - mae: 0.4525 - mse: 0.2062 - val_loss: 0.6948 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0728
Epoch 2/200
29/29 [==============================] - 0s 4ms/step - loss: 0.6058 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.5221 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0728
Epoch 3/200
29/29 [==============================] - 0s 4ms/step - loss: 0.4643 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.4104 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0728
Epoch 4/200
29/29 [==============================] - 0s 4ms/step - loss: 0.3729 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.3383 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0728
Epoch 5/200
29/29 [==============================] - 0s 5ms/step - loss: 0.3138 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2917 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0728
Epoch 6/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2756 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2615 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0728
Epoch 7/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2510 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2421 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0728
Epoch 8/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2350 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2295 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0728
Epoch 9/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2247 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2214 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0728
Epoch 10/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2181 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2161 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0728
Epoch 11/200
 1/29 [>.............................] - ETA: 0s - loss: 0.2182 - mae: 0.4557 - mse: 0.2086
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.03637633100152016.
29/29 [==============================] - 0s 3ms/step - loss: 0.2138 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2127 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0728
Epoch 12/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2114 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2114 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0364
Epoch 13/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2103 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2105 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0364
Epoch 14/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2095 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2097 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0364
Epoch 15/200
29/29 [==============================] - 0s 2ms/step - loss: 0.2088 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2091 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0364
Epoch 16/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2082 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2086 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0364
Epoch 17/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2078 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2082 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0364
Epoch 18/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2074 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2079 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0364
Epoch 19/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2071 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2076 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0364
Epoch 20/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2069 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2074 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0364
Epoch 21/200
 1/29 [>.............................] - ETA: 0s - loss: 0.2062 - mae: 0.4519 - mse: 0.2053
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.01818816550076008.
29/29 [==============================] - 0s 4ms/step - loss: 0.2067 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2073 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0364
Epoch 22/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2066 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2072 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0182
Epoch 23/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2065 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2071 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0182
Epoch 24/200
29/29 [==============================] - 0s 5ms/step - loss: 0.2065 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2071 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0182
Epoch 25/200
29/29 [==============================] - 0s 5ms/step - loss: 0.2064 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2070 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0182
Epoch 26/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2064 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2070 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0182
Epoch 27/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2063 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2069 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0182
Epoch 28/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2063 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2069 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0182
Epoch 29/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2062 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2068 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0182
Epoch 30/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2062 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2068 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0182
Epoch 31/200
 1/29 [>.............................] - ETA: 0s - loss: 0.2056 - mae: 0.4519 - mse: 0.2054
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00909408275038004.
29/29 [==============================] - 0s 4ms/step - loss: 0.2062 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2068 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0182
Epoch 32/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2062 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2068 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0091
Epoch 33/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2062 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2068 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0091
Epoch 34/200
29/29 [==============================] - 0s 5ms/step - loss: 0.2061 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2068 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0091
Epoch 35/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2061 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2067 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0091
Epoch 36/200
29/29 [==============================] - 0s 5ms/step - loss: 0.2061 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2067 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0091
Epoch 37/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2061 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2067 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0091
Epoch 38/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2061 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2067 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0091
Epoch 39/200
29/29 [==============================] - 0s 4ms/step - loss: 0.2061 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2067 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0091
Epoch 40/200
29/29 [==============================] - 0s 3ms/step - loss: 0.2061 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2067 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0091
Epoch 41/200

Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00227352068759501. 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05