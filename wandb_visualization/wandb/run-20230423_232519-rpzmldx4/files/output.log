Epoch 1/200
76/98 [======================>.......] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
98/98 [==============================] - 2s 13ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 2/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 3/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 4/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 5/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 6/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 7/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 8/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 9/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 10/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 11/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 12/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 13/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 14/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 15/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 16/200
78/98 [======================>.......] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.8578
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.010782640427350998.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0216
Epoch 17/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 18/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 19/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 20/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 21/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 22/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 23/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 24/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 25/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 26/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 27/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 28/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 29/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 30/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 31/200
78/98 [======================>.......] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.005391320213675499.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0108
Epoch 32/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 33/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 34/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 35/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 36/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 37/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 38/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 39/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 40/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 41/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 42/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 43/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 44/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 45/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 46/200
88/98 [=========================>....] - ETA: 0s - loss: 0.8570 - mae: 0.9247 - mse: 0.8570
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0026956601068377495.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0054
Epoch 47/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 48/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 49/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 50/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 51/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 52/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 53/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 54/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 55/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 56/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 57/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 58/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 59/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 60/200
24/98 [======>.......................] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.85838579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 61/200
74/98 [=====================>........] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0013478300534188747.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 62/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 63/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 64/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 65/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 66/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 67/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 68/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 69/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 70/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 71/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 72/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 73/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 74/200
58/98 [================>.............] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
24/98 [======>.......................] - ETA: 0s - loss: 0.8597 - mae: 0.9261 - mse: 0.85978579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 76/200
87/98 [=========================>....] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0006739150267094374.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0013
Epoch 77/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 78/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 79/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 80/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 81/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 82/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 83/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 84/200
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 85/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 86/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 87/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 88/200
79/98 [=======================>......] - ETA: 0s - loss: 0.8590 - mae: 0.9258 - mse: 0.8590
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 91/200
80/98 [=======================>......] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.8581
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0003369575133547187.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.7392e-04
Epoch 92/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.3696e-04
Epoch 93/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.3696e-04
Epoch 94/200
22/98 [=====>........................] - ETA: 0s - loss: 0.8548 - mae: 0.9235 - mse: 0.8548
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.3696e-04
Epoch 98/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.3696e-04
Epoch 99/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.3696e-04
Epoch 100/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.3696e-04
Epoch 101/200
 1/98 [..............................] - ETA: 0s - loss: 0.8653 - mae: 0.9292 - mse: 0.8653
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.3696e-04
Epoch 106/200
77/98 [======================>.......] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.8574
Epoch 106: ReduceLROnPlateau reducing learning rate to 0.00016847875667735934.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.3696e-04
Epoch 107/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.6848e-04
Epoch 108/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.6848e-04
Epoch 109/200
 1/98 [..............................] - ETA: 0s - loss: 0.8704 - mae: 0.9319 - mse: 0.8704
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.6848e-04
Epoch 113/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.6848e-04
Epoch 114/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.6848e-04
Epoch 115/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.6848e-04
Epoch 116/200
24/98 [======>.......................] - ETA: 0s - loss: 0.8618 - mae: 0.9273 - mse: 0.8618
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.6848e-04
Epoch 121/200
75/98 [=====================>........] - ETA: 0s - loss: 0.8584 - mae: 0.9255 - mse: 0.8584
Epoch 121: ReduceLROnPlateau reducing learning rate to 8.423937833867967e-05.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.6848e-04
Epoch 122/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.4239e-05
Epoch 123/200
73/98 [=====================>........] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
98/98 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.4239e-05
Epoch 128/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.4239e-05
Epoch 129/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.4239e-05
Epoch 130/200
77/98 [======================>.......] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.4239e-05
Epoch 136/200
77/98 [======================>.......] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 136: ReduceLROnPlateau reducing learning rate to 4.2119689169339836e-05.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.4239e-05
Epoch 137/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2120e-05
Epoch 138/200
 1/98 [..............................] - ETA: 0s - loss: 0.8516 - mae: 0.9219 - mse: 0.8516
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2120e-05
Epoch 143/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2120e-05
Epoch 144/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2120e-05
Epoch 145/200
26/98 [======>.......................] - ETA: 0s - loss: 0.8607 - mae: 0.9267 - mse: 0.8607
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2120e-05
Epoch 151/200
98/98 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 151: ReduceLROnPlateau reducing learning rate to 2.1059844584669918e-05.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2120e-05
Epoch 152/200
27/98 [=======>......................] - ETA: 0s - loss: 0.8619 - mae: 0.9274 - mse: 0.8619
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1060e-05
Epoch 158/200
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1060e-05
Epoch 159/200
78/98 [======================>.......] - ETA: 0s - loss: 0.8588 - mae: 0.9257 - mse: 0.8588
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1060e-05
Epoch 166/200
78/98 [======================>.......] - ETA: 0s - loss: 0.8572 - mae: 0.9248 - mse: 0.8572
Epoch 166: ReduceLROnPlateau reducing learning rate to 1.0529922292334959e-05.
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1060e-05
Epoch 167/200
 1/98 [..............................] - ETA: 0s - loss: 0.8537 - mae: 0.9229 - mse: 0.8537
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0530e-05
Epoch 173/200
98/98 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0530e-05
Epoch 174/200
 1/98 [..............................] - ETA: 0s - loss: 0.8683 - mae: 0.9310 - mse: 0.8683
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0530e-05
Epoch 181/200
81/98 [=======================>......] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
52/98 [==============>...............] - ETA: 0s - loss: 0.8564 - mae: 0.9244 - mse: 0.85648579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0530e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.021605
 73/130 [===============>..............] - ETA: 0s - loss: 0.6439 - mae: 0.8004 - mse: 0.64396431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.021605
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.010805
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.010805
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.005405
  1/130 [..............................] - ETA: 0s - loss: 0.6479 - mae: 0.8031 - mse: 0.64796431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.005405
130/130 [==============================] - 0s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.005405
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.002705
  1/130 [..............................] - ETA: 0s - loss: 0.6591 - mae: 0.8093 - mse: 0.65916431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.002705
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.002705
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.001305
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.001305
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.001305
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 6.7392e-04
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 6.7392e-04
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
130/130 [==============================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 106/200==========================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 111/200==========================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 116/200==========================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 121/200==========================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 127/200==========================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 132/200==========================] - 0s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 136: ReduceLROnPlateau reducing learning rate to 4.2119689169339836e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 142/200duceLROnPlateau reducing learning rate to 4.2119689169339836e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 147/200duceLROnPlateau reducing learning rate to 4.2119689169339836e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 151: ReduceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 158/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 163/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 169/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 174/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 178/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 182/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 189/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 194/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 200/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 200/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 18/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 26/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 33/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 41/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 48/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 48/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 56/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 62/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 70/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0006739150267094374.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 84/200duceLROnPlateau reducing learning rate to 0.0006739150267094374.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 106/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 113/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 121/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 128/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 136/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 141/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 149/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 156/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 164/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 171/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 178/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 185/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 192/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 200/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 200/200uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 18/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 18/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 41/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 48/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 56/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 62/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 70/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 77/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 85/2000uceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0003369575133547187.5..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 106: ReduceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 112/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 120/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 127/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 134/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 141/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 149/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 156/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 164/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 171/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 179/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 186/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 194/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 200/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 200/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 200/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_232519-rpzmldx4\files\model-best)... Done. 0.0s
Epoch 12/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 19/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 27/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 34/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 42/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 49/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 57/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 64/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 72/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 79/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 87/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 94/2000duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 102/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 109/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 117/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 124/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 124/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 139/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 139/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 147/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 154/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 162/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 169/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 176/200duceLROnPlateau reducing learning rate to 0.00016847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 181: ReduceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 189/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 196/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 196/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 196/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/20000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 14/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 21/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 29/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 36/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 44/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 51/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 59/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 66/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 66/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 81/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 81/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 89/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 96/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 104/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 111/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 119/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 126/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 133/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 140/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 147/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 154/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 162/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 169/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 174/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 181/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 188/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 196/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 196/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 196/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 18/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 26/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 33/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 41/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 48/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 56/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 63/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 63/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 78/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 78/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 93/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 93/2000duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 101/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 108/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 116/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 122/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 130/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 136/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 143/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 150/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 156/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 161/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 167/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 175/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 181: ReduceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 189/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
Epoch 196/200duceLROnPlateau reducing learning rate to 1e-05.6847875667735934..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 3.3696e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0028s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0028s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 18/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 26/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 33/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 41/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 48/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 56/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 63/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 71/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 71/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 78/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 86/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 93/200ained_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 101/200ined_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 108/200ined_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 115/200ined_models/models_segments_overlap-cnn_adam_0.021565279987064503LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 121: ReduceLROnPlateau reducing learning rate to 8.423937833867967e-05.3LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 129/200duceLROnPlateau reducing learning rate to 8.423937833867967e-05.3LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 136: ReduceLROnPlateau reducing learning rate to 4.2119689169339836e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 144/200duceLROnPlateau reducing learning rate to 4.2119689169339836e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 151: ReduceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 157/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 163/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 170/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 178/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 185/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 193/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 200/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 200/200duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 18/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 26/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 26/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 33/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 41/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 48/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 56/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 62/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 70/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 77/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 85/2000duceLROnPlateau reducing learning rate to 2.1059844584669918e-05.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0003369575133547187.5.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0003369575133547187.5.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 106: ReduceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 112/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 119/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 126/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 134/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 141/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 148/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 155/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 163/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 170/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 177/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 184/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 192/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 199/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04
Epoch 199/200duceLROnPlateau reducing learning rate to 0.00016847875667735934.LR_[516]CHN_64CNNI_40BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 3.3696e-04