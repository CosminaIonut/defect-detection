Epoch 1/20
152/243 [=================>............] - ETA: 0s - loss: 0.0493 - mae: 0.0620 - mse: 0.0104
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0001s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143048-k0g8sci4\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0319 - mae: 0.0531 - mse: 0.0073 - val_loss: 0.0027 - val_mae: 0.0436 - val_mse: 0.0024 - lr: 0.0337
Epoch 2/20
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0378 - mse: 0.0020 - val_loss: 0.0023 - val_mae: 0.0362 - val_mse: 0.0020 - lr: 0.0337
Epoch 3/20
191/243 [======================>.......] - ETA: 0s - loss: 0.0023 - mae: 0.0377 - mse: 0.0020
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.016832269728183746.
243/243 [==============================] - 0s 954us/step - loss: 0.0023 - mae: 0.0376 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0454 - val_mse: 0.0025 - lr: 0.0337
Epoch 4/20
237/243 [============================>.] - ETA: 0s - loss: 0.0021 - mae: 0.0374 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0373 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0375 - val_mse: 0.0020 - lr: 0.0168
Epoch 5/20
194/243 [======================>.......] - ETA: 0s - loss: 0.0021 - mae: 0.0373 - mse: 0.0020
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.008416134864091873.
243/243 [==============================] - 0s 852us/step - loss: 0.0021 - mae: 0.0374 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0386 - val_mse: 0.0020 - lr: 0.0168
Epoch 6/20
172/243 [====================>.........] - ETA: 0s - loss: 0.0021 - mae: 0.0371 - mse: 0.0019
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004208067432045937.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143048-k0g8sci4\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0021 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0084
Epoch 7/20
175/243 [====================>.........] - ETA: 0s - loss: 0.0020 - mae: 0.0367 - mse: 0.0019
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0021040337160229683.
243/243 [==============================] - 0s 901us/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0362 - val_mse: 0.0020 - lr: 0.0042
Epoch 8/20
171/243 [====================>.........] - ETA: 0s - loss: 0.0020 - mae: 0.0371 - mse: 0.0019
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0010520168580114841.
243/243 [==============================] - 0s 908us/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0363 - val_mse: 0.0020 - lr: 0.0021
Epoch 9/20
179/243 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0368 - mse: 0.0019
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005260084290057421.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143048-k0g8sci4\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0378 - val_mse: 0.0020 - lr: 0.0011
Epoch 10/20
197/243 [=======================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0370 - mse: 0.0019
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00026300421450287104.
243/243 [==============================] - 0s 872us/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0371 - val_mse: 0.0020 - lr: 5.2601e-04
Epoch 11/20
150/243 [=================>............] - ETA: 0s - loss: 0.0020 - mae: 0.0371 - mse: 0.0019
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00013150210725143552.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143048-k0g8sci4\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0373 - val_mse: 0.0020 - lr: 2.6300e-04
Epoch 12/20
170/243 [===================>..........] - ETA: 0s - loss: 0.0021 - mae: 0.0374 - mse: 0.0019
Epoch 12: ReduceLROnPlateau reducing learning rate to 6.575105362571776e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143048-k0g8sci4\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0373 - val_mse: 0.0020 - lr: 1.3150e-04
Epoch 13/20
164/243 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0371 - mse: 0.0019
Epoch 13: ReduceLROnPlateau reducing learning rate to 3.287552681285888e-05.
243/243 [==============================] - 0s 958us/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0373 - val_mse: 0.0020 - lr: 6.5751e-05
Epoch 14/20
172/243 [====================>.........] - ETA: 0s - loss: 0.0020 - mae: 0.0369 - mse: 0.0019
Epoch 14: ReduceLROnPlateau reducing learning rate to 1.643776340642944e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143048-k0g8sci4\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143048-k0g8sci4\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0370 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0377 - val_mse: 0.0020 - lr: 3.2876e-05
Epoch 15/20
190/243 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0371 - mse: 0.0019
Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0375 - val_mse: 0.0020 - lr: 1.6438e-05
Epoch 16/20
154/243 [==================>...........] - ETA: 0s - loss: 0.0020 - mae: 0.0373 - mse: 0.0019
243/243 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0375 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 17/20
176/243 [====================>.........] - ETA: 0s - loss: 0.0020 - mae: 0.0369 - mse: 0.0019
243/243 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0375 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 18/20
166/243 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0368 - mse: 0.0019
243/243 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0375 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 19/20
191/243 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0371 - mse: 0.0019
243/243 [==============================] - 1s 4ms/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0375 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 20/20
185/243 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0369 - mse: 0.0019
243/243 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0376 - val_mse: 0.0020 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.033664537978604483LR_[32]HN_16BS_1P_val_lossM_20epochs/model_1.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
323/323 [==============================] - 1s 1ms/step - loss: 0.0287 - mae: 0.0956 - mse: 0.0123 - val_loss: 0.0106 - val_mae: 0.0902 - val_mse: 0.0102 - lr: 0.0337
Epoch 2/20
323/323 [==============================] - 0s 837us/step - loss: 0.0105 - mae: 0.0909 - mse: 0.0102 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0098 - lr: 0.0337
Epoch 3/20
275/323 [========================>.....] - ETA: 0s - loss: 0.0102 - mae: 0.0908 - mse: 0.0100
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.016832269728183746.
323/323 [==============================] - 0s 833us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0099 - val_loss: 0.0105 - val_mae: 0.0902 - val_mse: 0.0102 - lr: 0.0337
Epoch 4/20
323/323 [==============================] - 0s 855us/step - loss: 0.0099 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0097 - lr: 0.0168
Epoch 5/20
263/323 [=======================>......] - ETA: 0s - loss: 0.0098 - mae: 0.0903 - mse: 0.0097
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.008416134864091873.
323/323 [==============================] - 0s 862us/step - loss: 0.0099 - mae: 0.0904 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0904 - val_mse: 0.0097 - lr: 0.0168
Epoch 6/20
301/323 [==========================>...] - ETA: 0s - loss: 0.0098 - mae: 0.0903 - mse: 0.0096
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004208067432045937.
323/323 [==============================] - 0s 819us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0904 - val_mse: 0.0097 - lr: 0.0084
Epoch 7/20
282/323 [=========================>....] - ETA: 0s - loss: 0.0097 - mae: 0.0903 - mse: 0.0096
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0021040337160229683.
323/323 [==============================] - 0s 833us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0903 - val_mse: 0.0096 - lr: 0.0042
Epoch 8/20
323/323 [==============================] - 0s 932us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0021
Epoch 9/20
285/323 [=========================>....] - ETA: 0s - loss: 0.0097 - mae: 0.0903 - mse: 0.0096
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0010520168580114841.
323/323 [==============================] - 0s 882us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0021
Epoch 10/20
291/323 [==========================>...] - ETA: 0s - loss: 0.0097 - mae: 0.0903 - mse: 0.0096
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005260084290057421.
323/323 [==============================] - 0s 865us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0011
Epoch 11/20
286/323 [=========================>....] - ETA: 0s - loss: 0.0097 - mae: 0.0904 - mse: 0.0096
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00026300421450287104.
323/323 [==============================] - 0s 836us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 5.2601e-04
Epoch 12/20
263/323 [=======================>......] - ETA: 0s - loss: 0.0098 - mae: 0.0908 - mse: 0.0097
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00013150210725143552.
323/323 [==============================] - 0s 890us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 2.6300e-04
Epoch 13/20
301/323 [==========================>...] - ETA: 0s - loss: 0.0097 - mae: 0.0901 - mse: 0.0096
Epoch 13: ReduceLROnPlateau reducing learning rate to 6.575105362571776e-05.
323/323 [==============================] - 0s 818us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 1.3150e-04
Epoch 14/20
298/323 [==========================>...] - ETA: 0s - loss: 0.0097 - mae: 0.0904 - mse: 0.0096
Epoch 14: ReduceLROnPlateau reducing learning rate to 3.287552681285888e-05.
323/323 [==============================] - 0s 817us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 6.5751e-05
Epoch 15/20
282/323 [=========================>....] - ETA: 0s - loss: 0.0097 - mae: 0.0905 - mse: 0.0096
Epoch 15: ReduceLROnPlateau reducing learning rate to 1.643776340642944e-05.
323/323 [==============================] - 0s 834us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 3.2876e-05
Epoch 16/20
241/323 [=====================>........] - ETA: 0s - loss: 0.0097 - mae: 0.0904 - mse: 0.0096
Epoch 16: ReduceLROnPlateau reducing learning rate to 1e-05.
323/323 [==============================] - 0s 934us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 1.6438e-05
Epoch 17/20
323/323 [==============================] - 0s 836us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 1.0000e-05
Epoch 18/20
323/323 [==============================] - 0s 832us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 1.0000e-05
Epoch 19/20
323/323 [==============================] - 0s 884us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 1.0000e-05
Epoch 20/20
323/323 [==============================] - 0s 868us/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.033664537978604483LR_[32]HN_16BS_1P_val_lossM_20epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
243/243 [==============================] - 1s 1ms/step - loss: 0.0483 - mae: 0.1543 - mse: 0.0260 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0337
Epoch 2/20
243/243 [==============================] - 0s 974us/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0337
Epoch 3/20
175/243 [====================>.........] - ETA: 0s - loss: 0.0245 - mae: 0.1528 - mse: 0.0244
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.016832269728183746.
243/243 [==============================] - 0s 897us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0251 - val_mae: 0.1532 - val_mse: 0.0250 - lr: 0.0337
Epoch 4/20
176/243 [====================>.........] - ETA: 0s - loss: 0.0243 - mae: 0.1525 - mse: 0.0242
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.008416134864091873.
243/243 [==============================] - 0s 888us/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0168
Epoch 5/20
243/243 [==============================] - 0s 894us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0084
Epoch 6/20
185/243 [=====================>........] - ETA: 0s - loss: 0.0243 - mae: 0.1527 - mse: 0.0242
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004208067432045937.
243/243 [==============================] - 0s 890us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0084
Epoch 7/20
190/243 [======================>.......] - ETA: 0s - loss: 0.0241 - mae: 0.1522 - mse: 0.0241
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0021040337160229683.
243/243 [==============================] - 0s 837us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0042
Epoch 8/20
171/243 [====================>.........] - ETA: 0s - loss: 0.0241 - mae: 0.1521 - mse: 0.0240
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0010520168580114841.
243/243 [==============================] - 0s 842us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0021
Epoch 9/20
174/243 [====================>.........] - ETA: 0s - loss: 0.0243 - mae: 0.1528 - mse: 0.0242
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005260084290057421.
243/243 [==============================] - 0s 893us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0011
Epoch 10/20
177/243 [====================>.........] - ETA: 0s - loss: 0.0242 - mae: 0.1527 - mse: 0.0242
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00026300421450287104.
243/243 [==============================] - 0s 899us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 5.2601e-04
Epoch 11/20
179/243 [=====================>........] - ETA: 0s - loss: 0.0242 - mae: 0.1527 - mse: 0.0241
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00013150210725143552.
243/243 [==============================] - 0s 894us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 2.6300e-04
Epoch 12/20
176/243 [====================>.........] - ETA: 0s - loss: 0.0242 - mae: 0.1525 - mse: 0.0241
Epoch 12: ReduceLROnPlateau reducing learning rate to 6.575105362571776e-05.
243/243 [==============================] - 0s 882us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.3150e-04
Epoch 13/20
226/243 [==========================>...] - ETA: 0s - loss: 0.0242 - mae: 0.1526 - mse: 0.0241
Epoch 13: ReduceLROnPlateau reducing learning rate to 3.287552681285888e-05.
243/243 [==============================] - 0s 1ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 6.5751e-05
Epoch 14/20
187/243 [======================>.......] - ETA: 0s - loss: 0.0242 - mae: 0.1526 - mse: 0.0241
Epoch 14: ReduceLROnPlateau reducing learning rate to 1.643776340642944e-05.
243/243 [==============================] - 0s 912us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 3.2876e-05
Epoch 15/20
184/243 [=====================>........] - ETA: 0s - loss: 0.0243 - mae: 0.1529 - mse: 0.0242
Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 871us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.6438e-05
Epoch 16/20
243/243 [==============================] - 0s 890us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.0000e-05
Epoch 17/20
243/243 [==============================] - 0s 898us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.0000e-05
Epoch 18/20
243/243 [==============================] - 0s 965us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.0000e-05
Epoch 19/20
243/243 [==============================] - 0s 924us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.0000e-05
Epoch 20/20
243/243 [==============================] - 0s 887us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.033664537978604483LR_[32]HN_16BS_1P_val_lossM_20epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
243/243 [==============================] - 1s 1ms/step - loss: 0.0641 - mae: 0.2031 - mse: 0.0433 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0337
Epoch 2/20
162/243 [===================>..........] - ETA: 0s - loss: 0.0420 - mae: 0.2019 - mse: 0.0419
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.016832269728183746.
243/243 [==============================] - 0s 959us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0421 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0427 - lr: 0.0337
Epoch 3/20
243/243 [==============================] - 0s 888us/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0168
Epoch 4/20
243/243 [==============================] - 0s 892us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0168
Epoch 5/20
181/243 [=====================>........] - ETA: 0s - loss: 0.0421 - mae: 0.2026 - mse: 0.0420
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.008416134864091873.
243/243 [==============================] - 0s 894us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0168
Epoch 6/20
183/243 [=====================>........] - ETA: 0s - loss: 0.0419 - mae: 0.2024 - mse: 0.0419
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004208067432045937.
243/243 [==============================] - 0s 894us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0084
Epoch 7/20
192/243 [======================>.......] - ETA: 0s - loss: 0.0420 - mae: 0.2026 - mse: 0.0419
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0021040337160229683.
243/243 [==============================] - 0s 827us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0042
Epoch 8/20
192/243 [======================>.......] - ETA: 0s - loss: 0.0421 - mae: 0.2030 - mse: 0.0420
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0010520168580114841.
243/243 [==============================] - 0s 827us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0021
Epoch 9/20
192/243 [======================>.......] - ETA: 0s - loss: 0.0419 - mae: 0.2025 - mse: 0.0419
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005260084290057421.
243/243 [==============================] - 0s 828us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0011
Epoch 10/20
191/243 [======================>.......] - ETA: 0s - loss: 0.0418 - mae: 0.2023 - mse: 0.0417
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00026300421450287104.
243/243 [==============================] - 0s 893us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 5.2601e-04
Epoch 11/20
167/243 [===================>..........] - ETA: 0s - loss: 0.0418 - mae: 0.2023 - mse: 0.0418
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00013150210725143552.
243/243 [==============================] - 0s 969us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 2.6300e-04
Epoch 12/20
187/243 [======================>.......] - ETA: 0s - loss: 0.0418 - mae: 0.2024 - mse: 0.0418
Epoch 12: ReduceLROnPlateau reducing learning rate to 6.575105362571776e-05.
243/243 [==============================] - 0s 893us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.3150e-04
Epoch 13/20
197/243 [=======================>......] - ETA: 0s - loss: 0.0419 - mae: 0.2026 - mse: 0.0419
Epoch 13: ReduceLROnPlateau reducing learning rate to 3.287552681285888e-05.
243/243 [==============================] - 0s 891us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 6.5751e-05
Epoch 14/20
175/243 [====================>.........] - ETA: 0s - loss: 0.0419 - mae: 0.2025 - mse: 0.0418
Epoch 14: ReduceLROnPlateau reducing learning rate to 1.643776340642944e-05.
243/243 [==============================] - 0s 891us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 3.2876e-05
Epoch 15/20
170/243 [===================>..........] - ETA: 0s - loss: 0.0420 - mae: 0.2027 - mse: 0.0419
Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 894us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.6438e-05
Epoch 16/20
243/243 [==============================] - 0s 888us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.0000e-05
Epoch 17/20
243/243 [==============================] - 0s 915us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.0000e-05
Epoch 18/20
243/243 [==============================] - 0s 895us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.0000e-05
Epoch 19/20
243/243 [==============================] - 0s 890us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.0000e-05
Epoch 20/20
243/243 [==============================] - 0s 823us/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.033664537978604483LR_[32]HN_16BS_1P_val_lossM_20epochs/model_4.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
243/243 [==============================] - 1s 1ms/step - loss: 0.0866 - mae: 0.2526 - mse: 0.0656 - val_loss: 0.0655 - val_mae: 0.2532 - val_mse: 0.0654 - lr: 0.0337
Epoch 2/20
243/243 [==============================] - 0s 891us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0337
Epoch 3/20
197/243 [=======================>......] - ETA: 0s - loss: 0.0650 - mae: 0.2526 - mse: 0.0650
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.016832269728183746.
243/243 [==============================] - 0s 891us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0337
Epoch 4/20
184/243 [=====================>........] - ETA: 0s - loss: 0.0650 - mae: 0.2526 - mse: 0.0649
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.008416134864091873.
243/243 [==============================] - 0s 891us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0168
Epoch 5/20
183/243 [=====================>........] - ETA: 0s - loss: 0.0650 - mae: 0.2528 - mse: 0.0650
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.004208067432045937.
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0084
Epoch 6/20
177/243 [====================>.........] - ETA: 0s - loss: 0.0649 - mae: 0.2525 - mse: 0.0649
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0021040337160229683.
243/243 [==============================] - 0s 902us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0042
Epoch 7/20
182/243 [=====================>........] - ETA: 0s - loss: 0.0650 - mae: 0.2526 - mse: 0.0649
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0010520168580114841.
243/243 [==============================] - 0s 903us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0021
Epoch 8/20
181/243 [=====================>........] - ETA: 0s - loss: 0.0646 - mae: 0.2520 - mse: 0.0646
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005260084290057421.
243/243 [==============================] - 0s 892us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0011
Epoch 9/20
178/243 [====================>.........] - ETA: 0s - loss: 0.0649 - mae: 0.2524 - mse: 0.0648
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00026300421450287104.
243/243 [==============================] - 0s 894us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 5.2601e-04
Epoch 10/20
177/243 [====================>.........] - ETA: 0s - loss: 0.0650 - mae: 0.2526 - mse: 0.0649
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00013150210725143552.
243/243 [==============================] - 0s 891us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 2.6300e-04
Epoch 11/20
174/243 [====================>.........] - ETA: 0s - loss: 0.0649 - mae: 0.2524 - mse: 0.0649
Epoch 11: ReduceLROnPlateau reducing learning rate to 6.575105362571776e-05.
243/243 [==============================] - 0s 956us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.3150e-04
Epoch 12/20
190/243 [======================>.......] - ETA: 0s - loss: 0.0648 - mae: 0.2524 - mse: 0.0648
Epoch 12: ReduceLROnPlateau reducing learning rate to 3.287552681285888e-05.
243/243 [==============================] - 0s 875us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.5751e-05
Epoch 13/20
166/243 [===================>..........] - ETA: 0s - loss: 0.0650 - mae: 0.2526 - mse: 0.0650
Epoch 13: ReduceLROnPlateau reducing learning rate to 1.643776340642944e-05.
243/243 [==============================] - 0s 936us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.2876e-05
Epoch 14/20
191/243 [======================>.......] - ETA: 0s - loss: 0.0648 - mae: 0.2523 - mse: 0.0648
Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 826us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.6438e-05
Epoch 15/20
243/243 [==============================] - 0s 876us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 16/20
243/243 [==============================] - 0s 908us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 17/20
243/243 [==============================] - 0s 932us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 18/20
243/243 [==============================] - 0s 949us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 19/20
243/243 [==============================] - 0s 888us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 20/20
243/243 [==============================] - 0s 945us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.033664537978604483LR_[32]HN_16BS_1P_val_lossM_20epochs/model_5.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0017s). Check your callbacks.
243/243 [==============================] - 1s 1ms/step - loss: 0.1144 - mae: 0.3025 - mse: 0.0931 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0337
Epoch 2/20
176/243 [====================>.........] - ETA: 0s - loss: 0.0928 - mae: 0.3028 - mse: 0.0928
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.016832269728183746.
243/243 [==============================] - 0s 890us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0337
Epoch 3/20
166/243 [===================>..........] - ETA: 0s - loss: 0.0928 - mae: 0.3027 - mse: 0.0928
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.008416134864091873.
243/243 [==============================] - 0s 916us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0168
Epoch 4/20
182/243 [=====================>........] - ETA: 0s - loss: 0.0926 - mae: 0.3024 - mse: 0.0926
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.004208067432045937.
243/243 [==============================] - 0s 918us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0084
Epoch 5/20
242/243 [============================>.] - ETA: 0s - loss: 0.0927 - mae: 0.3025 - mse: 0.0927
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0021040337160229683.
243/243 [==============================] - 0s 923us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0042
Epoch 6/20
179/243 [=====================>........] - ETA: 0s - loss: 0.0928 - mae: 0.3027 - mse: 0.0928
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0010520168580114841.
243/243 [==============================] - 0s 903us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0021
Epoch 7/20
181/243 [=====================>........] - ETA: 0s - loss: 0.0927 - mae: 0.3026 - mse: 0.0927
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005260084290057421.
243/243 [==============================] - 0s 892us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0011
Epoch 8/20
182/243 [=====================>........] - ETA: 0s - loss: 0.0924 - mae: 0.3020 - mse: 0.0924
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00026300421450287104.
243/243 [==============================] - 0s 897us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.2601e-04
Epoch 9/20
190/243 [======================>.......] - ETA: 0s - loss: 0.0928 - mae: 0.3028 - mse: 0.0928
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00013150210725143552.
243/243 [==============================] - 0s 891us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.6300e-04
Epoch 10/20
192/243 [======================>.......] - ETA: 0s - loss: 0.0926 - mae: 0.3024 - mse: 0.0926
Epoch 10: ReduceLROnPlateau reducing learning rate to 6.575105362571776e-05.
243/243 [==============================] - 0s 829us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.3150e-04
Epoch 11/20
195/243 [=======================>......] - ETA: 0s - loss: 0.0927 - mae: 0.3026 - mse: 0.0927
Epoch 11: ReduceLROnPlateau reducing learning rate to 3.287552681285888e-05.
243/243 [==============================] - 0s 823us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.5751e-05
Epoch 12/20
172/243 [====================>.........] - ETA: 0s - loss: 0.0925 - mae: 0.3022 - mse: 0.0925
Epoch 12: ReduceLROnPlateau reducing learning rate to 1.643776340642944e-05.
243/243 [==============================] - 0s 896us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.2876e-05
Epoch 13/20
175/243 [====================>.........] - ETA: 0s - loss: 0.0927 - mae: 0.3026 - mse: 0.0927
Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 910us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.6438e-05
Epoch 14/20
243/243 [==============================] - 0s 826us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 15/20
243/243 [==============================] - 0s 829us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 16/20
243/243 [==============================] - 0s 890us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 17/20
243/243 [==============================] - 0s 895us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 18/20
243/243 [==============================] - 0s 890us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 19/20
243/243 [==============================] - 0s 897us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 20/20
243/243 [==============================] - 0s 821us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.033664537978604483LR_[32]HN_16BS_1P_val_lossM_20epochs/model_6.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
243/243 [==============================] - 1s 1ms/step - loss: 0.1445 - mae: 0.3525 - mse: 0.1256 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0337
Epoch 2/20
198/243 [=======================>......] - ETA: 0s - loss: 0.1253 - mae: 0.3524 - mse: 0.1253
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.016832269728183746.
243/243 [==============================] - 0s 919us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0337
Epoch 3/20
183/243 [=====================>........] - ETA: 0s - loss: 0.1255 - mae: 0.3525 - mse: 0.1254
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.008416134864091873.
243/243 [==============================] - 0s 888us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0168
Epoch 4/20
183/243 [=====================>........] - ETA: 0s - loss: 0.1256 - mae: 0.3527 - mse: 0.1256
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.004208067432045937.
243/243 [==============================] - 0s 891us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0084
Epoch 5/20
178/243 [====================>.........] - ETA: 0s - loss: 0.1253 - mae: 0.3524 - mse: 0.1253
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0021040337160229683.
243/243 [==============================] - 0s 890us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0042
Epoch 6/20
179/243 [=====================>........] - ETA: 0s - loss: 0.1256 - mae: 0.3528 - mse: 0.1256
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0010520168580114841.
243/243 [==============================] - 0s 890us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0021
Epoch 7/20
177/243 [====================>.........] - ETA: 0s - loss: 0.1253 - mae: 0.3524 - mse: 0.1253
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005260084290057421.
243/243 [==============================] - 0s 917us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0011
Epoch 8/20
176/243 [====================>.........] - ETA: 0s - loss: 0.1254 - mae: 0.3525 - mse: 0.1254
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00026300421450287104.
243/243 [==============================] - 0s 899us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.2601e-04
Epoch 9/20
176/243 [====================>.........] - ETA: 0s - loss: 0.1255 - mae: 0.3527 - mse: 0.1255
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00013150210725143552.
243/243 [==============================] - 0s 884us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.6300e-04
Epoch 10/20
173/243 [====================>.........] - ETA: 0s - loss: 0.1253 - mae: 0.3523 - mse: 0.1253
Epoch 10: ReduceLROnPlateau reducing learning rate to 6.575105362571776e-05.
243/243 [==============================] - 0s 894us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.3150e-04
Epoch 11/20
152/243 [=================>............] - ETA: 0s - loss: 0.1250 - mae: 0.3520 - mse: 0.1250
Epoch 11: ReduceLROnPlateau reducing learning rate to 3.287552681285888e-05.
243/243 [==============================] - 0s 888us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.5751e-05
Epoch 12/20
170/243 [===================>..........] - ETA: 0s - loss: 0.1251 - mae: 0.3521 - mse: 0.1251
Epoch 12: ReduceLROnPlateau reducing learning rate to 1.643776340642944e-05.
243/243 [==============================] - 0s 892us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.2876e-05
Epoch 13/20
175/243 [====================>.........] - ETA: 0s - loss: 0.1254 - mae: 0.3525 - mse: 0.1254
Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 917us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.6438e-05
Epoch 14/20
243/243 [==============================] - 0s 891us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 15/20
243/243 [==============================] - 0s 893us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 16/20
243/243 [==============================] - 0s 894us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 17/20
243/243 [==============================] - 0s 892us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 18/20
243/243 [==============================] - 0s 887us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 19/20
243/243 [==============================] - 0s 844us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 20/20
243/243 [==============================] - 0s 834us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.033664537978604483LR_[32]HN_16BS_1P_val_lossM_20epochs/model_7.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
243/243 [==============================] - 1s 1ms/step - loss: 0.1837 - mae: 0.4025 - mse: 0.1633 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0337
Epoch 2/20
183/243 [=====================>........] - ETA: 0s - loss: 0.1634 - mae: 0.4028 - mse: 0.1634
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.016832269728183746.
243/243 [==============================] - 0s 890us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0337
Epoch 3/20
180/243 [=====================>........] - ETA: 0s - loss: 0.1633 - mae: 0.4026 - mse: 0.1633
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.008416134864091873.
243/243 [==============================] - 0s 894us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0168
Epoch 4/20
171/243 [====================>.........] - ETA: 0s - loss: 0.1636 - mae: 0.4030 - mse: 0.1636
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.004208067432045937.
243/243 [==============================] - 0s 894us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0084
Epoch 5/20
167/243 [===================>..........] - ETA: 0s - loss: 0.1629 - mae: 0.4022 - mse: 0.1629
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0021040337160229683.
243/243 [==============================] - 0s 950us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0042
Epoch 6/20
191/243 [======================>.......] - ETA: 0s - loss: 0.1631 - mae: 0.4024 - mse: 0.1631
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0010520168580114841.
243/243 [==============================] - 0s 847us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0021
Epoch 7/20
172/243 [====================>.........] - ETA: 0s - loss: 0.1634 - mae: 0.4028 - mse: 0.1634
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005260084290057421.
243/243 [==============================] - 0s 899us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0011
Epoch 8/20
219/243 [==========================>...] - ETA: 0s - loss: 0.1632 - mae: 0.4025 - mse: 0.1632
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00026300421450287104.
243/243 [==============================] - 0s 822us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.2601e-04
Epoch 9/20
193/243 [======================>.......] - ETA: 0s - loss: 0.1629 - mae: 0.4022 - mse: 0.1629
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00013150210725143552.
243/243 [==============================] - 0s 889us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.6300e-04
Epoch 10/20
 85/243 [=========>....................] - ETA: 0s - loss: 0.1635 - mae: 0.4029 - mse: 0.1635
191/243 [======================>.......] - ETA: 0s - loss: 0.1631 - mae: 0.4025 - mse: 0.1631
Epoch 10: ReduceLROnPlateau reducing learning rate to 6.575105362571776e-05.
243/243 [==============================] - 0s 893us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.3150e-04
Epoch 11/20
190/243 [======================>.......] - ETA: 0s - loss: 0.1633 - mae: 0.4026 - mse: 0.1633
Epoch 11: ReduceLROnPlateau reducing learning rate to 3.287552681285888e-05.
243/243 [==============================] - 0s 902us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 6.5751e-05
Epoch 12/20
185/243 [=====================>........] - ETA: 0s - loss: 0.1631 - mae: 0.4025 - mse: 0.1631
Epoch 12: ReduceLROnPlateau reducing learning rate to 1.643776340642944e-05.
243/243 [==============================] - 0s 921us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 3.2876e-05
Epoch 13/20
185/243 [=====================>........] - ETA: 0s - loss: 0.1632 - mae: 0.4026 - mse: 0.1632
Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 890us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.6438e-05
Epoch 14/20
243/243 [==============================] - 0s 893us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 15/20
243/243 [==============================] - 0s 849us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 16/20
243/243 [==============================] - 0s 953us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 17/20
243/243 [==============================] - 0s 908us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 18/20
243/243 [==============================] - 0s 908us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 19/20
243/243 [==============================] - 0s 891us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 20/20
243/243 [==============================] - 0s 828us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.033664537978604483LR_[32]HN_16BS_1P_val_lossM_20epochs/model_8.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
243/243 [==============================] - 1s 1ms/step - loss: 0.2273 - mae: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0337
Epoch 2/20
184/243 [=====================>........] - ETA: 0s - loss: 0.2061 - mae: 0.4527 - mse: 0.2061
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.016832269728183746.
243/243 [==============================] - 0s 893us/step - loss: 0.2060 - mae: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0337
Epoch 3/20
177/243 [====================>.........] - ETA: 0s - loss: 0.2058 - mae: 0.4523 - mse: 0.2057
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.008416134864091873.
243/243 [==============================] - 0s 901us/step - loss: 0.2060 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0168
Epoch 4/20
171/243 [====================>.........] - ETA: 0s - loss: 0.2055 - mae: 0.4520 - mse: 0.2055
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.004208067432045937.
243/243 [==============================] - 0s 887us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0084
Epoch 5/20
164/243 [===================>..........] - ETA: 0s - loss: 0.2059 - mae: 0.4525 - mse: 0.2059
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0021040337160229683.
243/243 [==============================] - 0s 949us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0042
Epoch 6/20
183/243 [=====================>........] - ETA: 0s - loss: 0.2059 - mae: 0.4524 - mse: 0.2059
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0010520168580114841.
243/243 [==============================] - 0s 918us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0021
Epoch 7/20
186/243 [=====================>........] - ETA: 0s - loss: 0.2061 - mae: 0.4527 - mse: 0.2061
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005260084290057421.
243/243 [==============================] - 0s 891us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0011
Epoch 8/20
186/243 [=====================>........] - ETA: 0s - loss: 0.2060 - mae: 0.4527 - mse: 0.2060
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00026300421450287104.
243/243 [==============================] - 0s 890us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 5.2601e-04
Epoch 9/20
185/243 [=====================>........] - ETA: 0s - loss: 0.2063 - mae: 0.4529 - mse: 0.2063
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00013150210725143552.
243/243 [==============================] - 0s 891us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 2.6300e-04
Epoch 10/20
179/243 [=====================>........] - ETA: 0s - loss: 0.2059 - mae: 0.4524 - mse: 0.2059
Epoch 10: ReduceLROnPlateau reducing learning rate to 6.575105362571776e-05.
243/243 [==============================] - 0s 910us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.3150e-04
Epoch 11/20
200/243 [=======================>......] - ETA: 0s - loss: 0.2059 - mae: 0.4525 - mse: 0.2059
Epoch 11: ReduceLROnPlateau reducing learning rate to 3.287552681285888e-05.
243/243 [==============================] - 0s 873us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 6.5751e-05
Epoch 12/20
173/243 [====================>.........] - ETA: 0s - loss: 0.2057 - mae: 0.4523 - mse: 0.2057
Epoch 12: ReduceLROnPlateau reducing learning rate to 1.643776340642944e-05.
243/243 [==============================] - 0s 852us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 3.2876e-05
Epoch 13/20
168/243 [===================>..........] - ETA: 0s - loss: 0.2060 - mae: 0.4526 - mse: 0.2060
Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 962us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.6438e-05
Epoch 14/20
243/243 [==============================] - 0s 888us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 15/20
243/243 [==============================] - 0s 893us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 16/20
243/243 [==============================] - 0s 892us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 17/20
243/243 [==============================] - 0s 901us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 18/20
243/243 [==============================] - 0s 892us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 19/20
243/243 [==============================] - 0s 893us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 20/20
243/243 [==============================] - 0s 893us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.033664537978604483LR_[32]HN_16BS_1P_val_lossM_20epochs/model_9.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])