C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow_probability\python\layers\util.py:99: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  loc = add_variable_fn(
C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow_probability\python\layers\util.py:109: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  untransformed_scale = add_variable_fn(
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_1 (InputLayer)        [(None, 8)]               0
 dense_flipout (DenseFlipout  (None, 8)                144
 )
 dense_flipout_1 (DenseFlipo  (None, 36)               648
 ut)
 dense_flipout_2 (DenseFlipo  (None, 2)                148
 ut)
 distribution_lambda (Distri  ((None, 1),              0
 butionLambda)                (None, 1))
=================================================================
Total params: 940
Trainable params: 940
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
11/11 [==============================] - 2s 13ms/step - loss: 1.3721
Epoch 2/200
11/11 [==============================] - 0s 11ms/step - loss: 1.3420
Epoch 3/200
11/11 [==============================] - 0s 12ms/step - loss: 1.3341
Epoch 4/200
11/11 [==============================] - 0s 10ms/step - loss: 1.3290
Epoch 5/200
11/11 [==============================] - 0s 12ms/step - loss: 1.3223
Epoch 6/200
11/11 [==============================] - 0s 12ms/step - loss: 1.3220
Epoch 7/200
11/11 [==============================] - 0s 11ms/step - loss: 1.3147
Epoch 8/200
11/11 [==============================] - 0s 10ms/step - loss: 1.3106
Epoch 9/200
11/11 [==============================] - 0s 10ms/step - loss: 1.3067
Epoch 10/200
11/11 [==============================] - 0s 12ms/step - loss: 1.3005
Epoch 11/200
11/11 [==============================] - 0s 12ms/step - loss: 1.2950
Epoch 12/200
11/11 [==============================] - 0s 12ms/step - loss: 1.2897
Epoch 13/200
11/11 [==============================] - 0s 12ms/step - loss: 1.2851
Epoch 14/200
11/11 [==============================] - 0s 12ms/step - loss: 1.2803
Epoch 15/200
11/11 [==============================] - 0s 12ms/step - loss: 1.2738
Epoch 16/200
11/11 [==============================] - 0s 12ms/step - loss: 1.2683
Epoch 17/200
11/11 [==============================] - 0s 12ms/step - loss: 1.2664
Epoch 18/200
11/11 [==============================] - 0s 10ms/step - loss: 1.2599
Epoch 19/200
11/11 [==============================] - 0s 10ms/step - loss: 1.2526
Epoch 20/200
11/11 [==============================] - 0s 10ms/step - loss: 1.2476
Epoch 21/200
11/11 [==============================] - 0s 10ms/step - loss: 1.2380
Epoch 22/200
11/11 [==============================] - 0s 10ms/step - loss: 1.2294
Epoch 23/200
11/11 [==============================] - 0s 11ms/step - loss: 1.2209
Epoch 24/200
11/11 [==============================] - 0s 11ms/step - loss: 1.2092
Epoch 25/200
11/11 [==============================] - 0s 11ms/step - loss: 1.1981
Epoch 26/200
11/11 [==============================] - 0s 12ms/step - loss: 1.1842
Epoch 27/200
11/11 [==============================] - 0s 12ms/step - loss: 1.1709
Epoch 28/200
11/11 [==============================] - 0s 10ms/step - loss: 1.1493
Epoch 29/200
11/11 [==============================] - 0s 10ms/step - loss: 1.1345
Epoch 30/200
11/11 [==============================] - 0s 10ms/step - loss: 1.1123
Epoch 31/200
11/11 [==============================] - 0s 11ms/step - loss: 1.0869
Epoch 32/200
11/11 [==============================] - 0s 10ms/step - loss: 1.0646
Epoch 33/200
11/11 [==============================] - 0s 10ms/step - loss: 1.0363
Epoch 34/200
11/11 [==============================] - 0s 10ms/step - loss: 1.0002
Epoch 35/200
11/11 [==============================] - 0s 10ms/step - loss: 1.0021
Epoch 36/200
11/11 [==============================] - 0s 12ms/step - loss: 0.9730
Epoch 37/200
11/11 [==============================] - 0s 12ms/step - loss: 0.9411
Epoch 38/200
11/11 [==============================] - 0s 12ms/step - loss: 0.9048
Epoch 39/200
11/11 [==============================] - 0s 12ms/step - loss: 0.8771
Epoch 40/200
11/11 [==============================] - 0s 11ms/step - loss: 0.8690
Epoch 41/200
11/11 [==============================] - 0s 10ms/step - loss: 0.8130
Epoch 42/200
11/11 [==============================] - 0s 8ms/step - loss: 0.8529
Epoch 43/200
11/11 [==============================] - 0s 12ms/step - loss: 0.7960
Epoch 44/200
11/11 [==============================] - 0s 10ms/step - loss: 0.8237
Epoch 45/200
11/11 [==============================] - 0s 10ms/step - loss: 0.8125
Epoch 46/200
11/11 [==============================] - 0s 10ms/step - loss: 0.8035
Epoch 47/200
Epoch 56/200=========================] - 0s 10ms/step - loss: 0.7447
Epoch 48/200
11/11 [==============================] - 0s 10ms/step - loss: 0.7436
Epoch 49/200
11/11 [==============================] - 0s 10ms/step - loss: 0.6955
Epoch 50/200
11/11 [==============================] - 0s 8ms/step - loss: 0.7117
Epoch 51/200
11/11 [==============================] - 0s 10ms/step - loss: 0.7157
Epoch 52/200
11/11 [==============================] - 0s 10ms/step - loss: 0.6526
Epoch 53/200
11/11 [==============================] - 0s 9ms/step - loss: 0.6857
Epoch 54/200
11/11 [==============================] - 0s 8ms/step - loss: 0.6807
Epoch 55/200
11/11 [==============================] - 0s 10ms/step - loss: 0.6440
Epoch 56/200=========================] - 0s 10ms/step - loss: 0.7447
11/11 [==============================] - 0s 10ms/step - loss: 0.5858
Epoch 57/200
11/11 [==============================] - 0s 10ms/step - loss: 0.6157
Epoch 58/200
11/11 [==============================] - 0s 12ms/step - loss: 0.5468
Epoch 59/200
11/11 [==============================] - 0s 9ms/step - loss: 0.6140
Epoch 60/200
11/11 [==============================] - 0s 10ms/step - loss: 0.5420
Epoch 61/200
11/11 [==============================] - 0s 10ms/step - loss: 0.5751
Epoch 62/200
11/11 [==============================] - 0s 11ms/step - loss: 0.5385
Epoch 63/200
11/11 [==============================] - 0s 10ms/step - loss: 0.5472
Epoch 64/200
11/11 [==============================] - 0s 10ms/step - loss: 0.5216
Epoch 65/200
11/11 [==============================] - 0s 12ms/step - loss: 0.5445
Epoch 66/200
11/11 [==============================] - 0s 13ms/step - loss: 0.4858
Epoch 67/200
11/11 [==============================] - 0s 12ms/step - loss: 0.4739
Epoch 68/200
11/11 [==============================] - 0s 12ms/step - loss: 0.4772
Epoch 69/200
11/11 [==============================] - 0s 12ms/step - loss: 0.4319
Epoch 70/200
11/11 [==============================] - 0s 12ms/step - loss: 0.4710
Epoch 71/200
11/11 [==============================] - 0s 12ms/step - loss: 0.4403
Epoch 72/200
11/11 [==============================] - 0s 13ms/step - loss: 0.4039
Epoch 73/200
11/11 [==============================] - 0s 12ms/step - loss: 0.3944
Epoch 74/200
11/11 [==============================] - 0s 12ms/step - loss: 0.3917
Epoch 75/200
11/11 [==============================] - 0s 14ms/step - loss: 0.3766
Epoch 76/200
11/11 [==============================] - 0s 12ms/step - loss: 0.3596
Epoch 77/200
11/11 [==============================] - 0s 13ms/step - loss: 0.3639
Epoch 78/200
11/11 [==============================] - 0s 13ms/step - loss: 0.3672
Epoch 79/200
11/11 [==============================] - 0s 13ms/step - loss: 0.3279
Epoch 80/200
 6/11 [===============>..............] - ETA: 0s - loss: 0.3240
11/11 [==============================] - 0s 15ms/step - loss: 0.2563
Epoch 84/200
11/11 [==============================] - 0s 12ms/step - loss: 0.2607
Epoch 85/200
11/11 [==============================] - 0s 12ms/step - loss: 0.2876
Epoch 86/200
11/11 [==============================] - 0s 12ms/step - loss: 0.2712
Epoch 87/200
11/11 [==============================] - 0s 13ms/step - loss: 0.2224
Epoch 88/200
11/11 [==============================] - 0s 12ms/step - loss: 0.2646
Epoch 89/200
11/11 [==============================] - 0s 12ms/step - loss: 0.2259
Epoch 90/200
11/11 [==============================] - 0s 12ms/step - loss: 0.2300
Epoch 91/200
11/11 [==============================] - 0s 13ms/step - loss: 0.1835
Epoch 92/200
11/11 [==============================] - 0s 12ms/step - loss: 0.1854
Epoch 93/200
11/11 [==============================] - 0s 12ms/step - loss: 0.2083
Epoch 94/200
11/11 [==============================] - 0s 11ms/step - loss: 0.1871
Epoch 95/200
11/11 [==============================] - ETA: 0s - loss: 0.1642
11/11 [==============================] - 0s 11ms/step - loss: 0.1145
Epoch 102/200
11/11 [==============================] - 0s 14ms/step - loss: 0.0847
Epoch 103/200
11/11 [==============================] - 0s 15ms/step - loss: 0.0581
Epoch 104/200
11/11 [==============================] - 0s 12ms/step - loss: 0.0919
Epoch 105/200
11/11 [==============================] - 0s 12ms/step - loss: 0.0773
Epoch 106/200
11/11 [==============================] - 0s 13ms/step - loss: 0.0297
Epoch 107/200
11/11 [==============================] - 0s 12ms/step - loss: 0.0231
Epoch 108/200
11/11 [==============================] - 0s 12ms/step - loss: 0.0600
Epoch 109/200
11/11 [==============================] - 0s 13ms/step - loss: 0.0174
Epoch 110/200
 6/11 [===============>..............] - ETA: 0s - loss: 0.1529
11/11 [==============================] - 0s 15ms/step - loss: -0.0651
Epoch 120/200
11/11 [==============================] - 0s 15ms/step - loss: -0.0801
Epoch 121/200
11/11 [==============================] - 0s 13ms/step - loss: -0.0409
Epoch 122/200
11/11 [==============================] - 0s 15ms/step - loss: -0.0986
Epoch 123/200
11/11 [==============================] - 0s 14ms/step - loss: -0.0756
Epoch 124/200
 1/11 [=>............................] - ETA: 0s - loss: -0.1267
 5/11 [============>.................] - ETA: 0s - loss: -0.1929.0651
11/11 [==============================] - 0s 15ms/step - loss: -0.2229
11/11 [==============================] - 0s 15ms/step - loss: -0.3399
 1/11 [=>............................] - ETA: 0s - loss: -0.4944.3399
11/11 [==============================] - 0s 15ms/step - loss: -0.4351
11/11 [==============================] - 0s 15ms/step - loss: -0.6120
11/11 [==============================] - 0s 15ms/step - loss: -0.6120
11/11 [==============================] - 0s 15ms/step - loss: -0.6120
11/11 [==============================] - 0s 15ms/step - loss: -0.6120
11/11 [==============================] - 0s 15ms/step - loss: -0.6120
11/11 [==============================] - 0s 15ms/step - loss: -0.6120
 [0.04003521]========================] - 0s 15ms/step - loss: -0.6120
 [0.06432328]
 [0.03618606]
 [0.05430653]
 [0.04597025]
 [0.0550665 ]
 [0.07675606]
 [0.04099194]
 [0.03987861]
 [0.05655667]
 [0.04850494]
 [0.06773654]
 [0.06264553]
 [0.0610693 ]
 [0.04498332]
 [0.05047519]
 [0.05960939]
 [0.03725566]
 [0.05566656]
 [0.03937023]
 [0.02397649]
 [0.04981161]
 [0.05428458]
 [0.06224415]
 [0.04182815]
 [0.07001838]
 [0.04666324]
 [0.0310437 ]
 [0.06685074]
 [0.05537619]
 [0.06500288]
 [0.04507769]
 [0.04816664]
 [0.06032456]
 [0.05685567]
 [0.06555984]
 [0.04003521]========================] - 0s 15ms/step - loss: -0.6120
 [0.04636859]
 [0.05051609]
 [0.03829295]
 [0.03907057]
 [0.07757394]
 [0.0574649 ]
 [0.06408912]
 [0.0587375 ]
 [0.03556553]
 [0.03917733]
 [0.0467672 ]
 [0.03631208]
 [0.03911815]
 [0.0671535 ]
 [0.06834366]
 [0.0554237 ]
 [0.05878427]
 [0.05661909]
 [0.0414002 ]
 [0.05478653]
 [0.06812376]
 [0.06975035]
 [0.02417456]
 [0.05487984]
 [0.0627826 ]
 [0.0560215 ]
 [0.06206071]
 [0.04143807]
 [0.03540341]
 [0.07315037]
 [0.06741657]
 [0.04916548]
 [0.05071513]
 [0.04947865]
 [0.0463419 ]
 [0.06194112]
 [0.07200748]
 [0.04362388]
 [0.06101433]
 [0.03756498]
 [0.05989599]
 [0.0314064 ]
 [0.0575263 ]
 [0.04601899]
 [0.06515749]
 [0.05708014]
 [0.05389563]
 [0.04762781]
 [0.04880987]
 [0.06915724]
 [0.06417269]
 [0.04604039]
 [0.03149692]
 [0.05346928]
 [0.05896212]
 [0.06316365]
 [0.05845344]
 [0.04899502]
 [0.06890246]
 [0.04799052]
 [0.05633702]
 [0.0321307 ]
 [0.04412079]
 [0.04449775]
 [0.06722266]
 [0.05297885]
 [0.07615697]
 [0.06647661]
 [0.0419828 ]
 [0.04462546]
 [0.05004142]
 [0.06765146]
 [0.04155738]
 [0.06345738]
 [0.06438907]
 [0.04507244]
 [0.05574469]
 [0.06565125]
 [0.06014782]
 [0.06350637]
 [0.05771685]
 [0.03500428]
 [0.06099901]
 [0.04692397]
 [0.05007277]
 [0.05186008]
 [0.05518017]
 [0.07251917]
 [0.03770242]
 [0.04971164]
 [0.04201769]
 [0.07056948]
 [0.05108473]
 [0.05465351]
 [0.03716203]
 [0.05987152]
 [0.03714453]
 [0.04196614]
 [0.06038072]
 [0.05184089]
 [0.0367549 ]
 [0.04983514]
 [0.04683314]
 [0.03067873]
 [0.05680451]
 [0.05921023]
 [0.06153501]
 [0.06331217]
 [0.05687855]
 [0.06206375]
 [0.05826712]
 [0.04539634]
 [0.03696394]
 [0.07228192]
 [0.0558992 ]
 [0.07613877]
 [0.05527125]
 [0.06058187]
 [0.0464348 ]
 [0.03541613]
 [0.05759949]
 [0.04292095]
 [0.04099212]
 [0.03670623]
 [0.0356143 ]
 [0.04775138]
 [0.06408014]
 [0.06210236]
 [0.0525222 ]
 [0.04207368]
 [0.05354831]
 [0.06880601]
 [0.06735185]
 [0.07349209]
 [0.03425914]
 [0.05819098]
 [0.04909578]
 [0.04540007]
 [0.05745136]
 [0.05795895]
 [0.06132258]
 [0.05778203]
 [0.06261319]
 [0.07392365]
 [0.06069658]
 [0.0561338 ]
 [0.0592798 ]
 [0.05617257]
 [0.05011263]
 [0.04688117]
 [0.03286357]
 [0.05040369]
 [0.05481089]
 [0.04799531]
 [0.07240016]
 [0.05386401]
 [0.01554166]
 [0.06098879]
 [0.04830289]
 [0.03425093]
 [0.04408906]
 [0.0768979 ]
 [0.04204619]
 [0.03582502]
 [0.05873716]
 [0.05902081]
 [0.03624206]
 [0.03764593]
 [0.05325518]
 [0.04212045]
 [0.05281815]
 [0.05542616]
 [0.04094819]
 [0.06688335]
 [0.06134366]
 [0.05345974]
 [0.04321084]
 [0.04410466]
 [0.03958104]
 [0.06462321]
 [0.05806411]
 [0.0395308 ]
 [0.05876078]
 [0.06161593]
 [0.04447874]
 [0.06318612]
 [0.02997597]
 [0.04337883]
 [0.0625302 ]
 [0.04664089]
 [0.04992093]
 [0.04779681]
 [0.05290369]
 [0.06277714]
 [0.07769332]
 [0.05324204]
 [0.05601861]
 [0.06236927]
 [0.06210204]
 [0.05320368]
 [0.04840214]
 [0.05009157]
 [0.05386088]
 [0.04922438]
 [0.05342429]
 [0.03890198]
 [0.06003399]
 [0.03865905]
 [0.04487103]
 [0.05735884]
 [0.04710093]
 [0.0502938 ]
 [0.04857969]
 [0.04673981]
 [0.06995674]
 [0.04933543]
 [0.03591854]
 [0.05482251]
 [0.03535659]
 [0.05500644]
 [0.06130329]
 [0.053505  ]
 [0.06872637]
 [0.0371766 ]
 [0.03432291]
 [0.07539808]
 [0.04415254]
 [0.03867296]
 [0.05019763]
 [0.06560764]
 [0.04373174]
 [0.0604781 ]
 [0.06482039]
 [0.04211741]
 [0.03174089]
 [0.03507712]
 [0.05504837]
 [0.05080092]
 [0.05750785]
 [0.06103026]
 [0.06286559]
 [0.04447033]
 [0.06757038]
 [0.06061378]
 [0.04472729]
 [0.06984209]
 [0.06590326]
 [0.03224846]
 [0.06060219]
 [0.05312477]
 [0.04437638]
 [0.058174  ]
 [0.05288504]
 [0.03258509]
 [0.03884883]
 [0.05082828]
 [0.05242375]]
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow_probability\python\layers\util.py:99: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  loc = add_variable_fn(
C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow_probability\python\layers\util.py:109: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  untransformed_scale = add_variable_fn(
[1.11372882]
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_2 (InputLayer)        [(None, 8)]               0
 dense_flipout_3 (DenseFlipo  (None, 8)                144
 ut)
 dense_flipout_4 (DenseFlipo  (None, 36)               648
 ut)
 dense_flipout_5 (DenseFlipo  (None, 2)                148
 ut)
 distribution_lambda_1 (Dist  ((None, 1),              0
 ributionLambda)              (None, 1))
=================================================================
Total params: 940
Trainable params: 940
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
20/20 [==============================] - 2s 16ms/step - loss: 0.9769
Epoch 2/200
20/20 [==============================] - 0s 15ms/step - loss: 0.9590
Epoch 3/200
20/20 [==============================] - 0s 15ms/step - loss: 0.9489
Epoch 4/200
20/20 [==============================] - 0s 15ms/step - loss: 0.9419
Epoch 5/200
20/20 [==============================] - 0s 15ms/step - loss: 0.9338
Epoch 6/200
20/20 [==============================] - 0s 15ms/step - loss: 0.9271
Epoch 7/200
20/20 [==============================] - 0s 15ms/step - loss: 0.9157
Epoch 8/200
20/20 [==============================] - 0s 14ms/step - loss: 0.9045
Epoch 9/200
20/20 [==============================] - 0s 14ms/step - loss: 0.8891
Epoch 10/200
20/20 [==============================] - 0s 15ms/step - loss: 0.8666
Epoch 11/200
20/20 [==============================] - 0s 14ms/step - loss: 0.8404
Epoch 12/200
20/20 [==============================] - 0s 15ms/step - loss: 0.8015
Epoch 13/200
20/20 [==============================] - 0s 15ms/step - loss: 0.7483
Epoch 14/200
20/20 [==============================] - 0s 14ms/step - loss: 0.6967
Epoch 15/200
20/20 [==============================] - 0s 15ms/step - loss: 0.6481
Epoch 16/200
20/20 [==============================] - 0s 14ms/step - loss: 0.6239
Epoch 17/200
20/20 [==============================] - 0s 15ms/step - loss: 0.5728
Epoch 18/200
20/20 [==============================] - 0s 15ms/step - loss: 0.5665
Epoch 19/200
20/20 [==============================] - 0s 15ms/step - loss: 0.5152
Epoch 20/200
20/20 [==============================] - 0s 14ms/step - loss: 0.4819
Epoch 21/200
20/20 [==============================] - 0s 15ms/step - loss: 0.4474
Epoch 22/200
20/20 [==============================] - 0s 14ms/step - loss: 0.4026
Epoch 23/200
20/20 [==============================] - 0s 15ms/step - loss: 0.3827
Epoch 24/200
20/20 [==============================] - 0s 14ms/step - loss: 0.3764
Epoch 25/200
20/20 [==============================] - 0s 15ms/step - loss: 0.3507
Epoch 26/200
20/20 [==============================] - 0s 14ms/step - loss: 0.3451
Epoch 27/200
20/20 [==============================] - 0s 15ms/step - loss: 0.3199
Epoch 28/200
20/20 [==============================] - 0s 14ms/step - loss: 0.3060
Epoch 29/200
20/20 [==============================] - 0s 15ms/step - loss: 0.2507
Epoch 30/200
20/20 [==============================] - 0s 15ms/step - loss: 0.2298
Epoch 31/200
20/20 [==============================] - 0s 15ms/step - loss: 0.2060
Epoch 32/200
20/20 [==============================] - 0s 14ms/step - loss: 0.1986
Epoch 33/200
20/20 [==============================] - 0s 14ms/step - loss: 0.1611
Epoch 34/200
20/20 [==============================] - 0s 15ms/step - loss: 0.1420
Epoch 35/200
20/20 [==============================] - 0s 14ms/step - loss: 0.1945
Epoch 36/200
20/20 [==============================] - 0s 13ms/step - loss: 0.1421
Epoch 37/200
20/20 [==============================] - 0s 15ms/step - loss: 0.1195
Epoch 38/200
20/20 [==============================] - 0s 15ms/step - loss: 0.0856
Epoch 39/200
20/20 [==============================] - 0s 15ms/step - loss: 0.0562
Epoch 40/200
20/20 [==============================] - 0s 14ms/step - loss: -0.0058
Epoch 41/200
20/20 [==============================] - 0s 14ms/step - loss: 0.0100
Epoch 42/200
20/20 [==============================] - 0s 14ms/step - loss: 0.0087
Epoch 43/200
20/20 [==============================] - 0s 14ms/step - loss: -0.0140
Epoch 44/200
20/20 [==============================] - 0s 15ms/step - loss: -0.0528
Epoch 45/200
20/20 [==============================] - 0s 14ms/step - loss: -0.0529
Epoch 46/200
20/20 [==============================] - 0s 15ms/step - loss: -0.0774
Epoch 47/200
20/20 [==============================] - 0s 13ms/step - loss: -0.0607
Epoch 48/200
20/20 [==============================] - 0s 15ms/step - loss: -0.1193
Epoch 49/200
20/20 [==============================] - 0s 14ms/step - loss: -0.1345
Epoch 50/200
20/20 [==============================] - 0s 14ms/step - loss: -0.1119
Epoch 51/200
20/20 [==============================] - 0s 14ms/step - loss: -0.1602
Epoch 52/200
20/20 [==============================] - 0s 14ms/step - loss: -0.1857
Epoch 53/200
20/20 [==============================] - 0s 15ms/step - loss: -0.1854
Epoch 54/200
18/20 [==========================>...] - ETA: 0s - loss: -0.2171
Traceback (most recent call last):
  File "C:\MyDocuments\Disertatie\segments\wandb_visualization\overlap\wrapper_test_and_train.py", line 369, in <module>
    train_models_and_save_bnn(n_members,data,test_size,"BNN")
  File "C:\MyDocuments\Disertatie\segments\wandb_visualization\overlap\test_bnn.py", line 26, in train_models_and_save_bnn
    mybnn.train_bnn(x_train, y_train, train_env)
  File "C:\MyDocuments\Disertatie\segments\model_training\overlap\bayesian_models.py", line 101, in train_bnn
    history = self.model.fit(X, Y, batch_size=batch_size, epochs=epochs, verbose=verbose,
  File "C:\Users\cosmi\AppData\Roaming\Python\Python39\site-packages\wandb\integration\keras\keras.py", line 174, in new_v2
    return old_v2(*args, **kwargs)
  File "C:\Users\cosmi\AppData\Roaming\Python\Python39\site-packages\wandb\integration\keras\keras.py", line 174, in new_v2
    return old_v2(*args, **kwargs)
  File "C:\Users\cosmi\AppData\Roaming\Python\Python39\site-packages\wandb\integration\keras\keras.py", line 174, in new_v2
    return old_v2(*args, **kwargs)
  File "C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\keras\utils\traceback_utils.py", line 65, in error_handler
    return fn(*args, **kwargs)
  File "C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\keras\engine\training.py", line 1564, in fit
    tmp_logs = self.train_function(iterator)
  File "C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow\python\util\traceback_utils.py", line 150, in error_handler
    return fn(*args, **kwargs)
  File "C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow\python\eager\def_function.py", line 915, in __call__
    result = self._call(*args, **kwds)
  File "C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow\python\eager\def_function.py", line 947, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow\python\eager\function.py", line 2496, in __call__
    return graph_function._call_flat(
  File "C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow\python\eager\function.py", line 1862, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow\python\eager\function.py", line 499, in call
    outputs = execute.execute(
  File "C:\Users\cosmi\anaconda3\envs\tf2.12\lib\site-packages\tensorflow\python\eager\execute.py", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
20/20 [==============================] - 0s 15ms/step - loss: -0.2061
Epoch 55/200
20/20 [==============================] - 0s 15ms/step - loss: -0.2365
Epoch 56/200
20/20 [==============================] - 0s 14ms/step - loss: -0.2137
Epoch 57/200
14/20 [====================>.........] - ETA: 0s - loss: -0.2638