Epoch 1/20
 62/122 [==============>...............] - ETA: 0s - loss: 0.1035 - mae: 0.0849 - mse: 0.0188
122/122 [==============================] - 1s 8ms/step - loss: 0.0545 - mae: 0.0622 - mse: 0.0105 - val_loss: 0.0026 - val_mae: 0.0390 - val_mse: 0.0020 - lr: 0.0273
Epoch 2/20
 80/122 [==================>...........] - ETA: 0s - loss: 0.0024 - mae: 0.0376 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0025 - mae: 0.0379 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0367 - val_mse: 0.0020 - lr: 0.0273
Epoch 3/20
 63/122 [==============>...............] - ETA: 0s - loss: 0.0023 - mae: 0.0376 - mse: 0.0020
122/122 [==============================] - 1s 7ms/step - loss: 0.0023 - mae: 0.0377 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0373 - val_mse: 0.0020 - lr: 0.0273
Epoch 4/20
 70/122 [================>.............] - ETA: 0s - loss: 0.0022 - mae: 0.0377 - mse: 0.0020
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0136351203545928.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0023 - mae: 0.0376 - mse: 0.0021 - val_loss: 0.0022 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0273
Epoch 5/20
 68/122 [===============>..............] - ETA: 0s - loss: 0.0022 - mae: 0.0372 - mse: 0.0020
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0068175601772964.
122/122 [==============================] - 0s 954us/step - loss: 0.0022 - mae: 0.0374 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0422 - val_mse: 0.0023 - lr: 0.0136
Epoch 6/20
 86/122 [====================>.........] - ETA: 0s - loss: 0.0021 - mae: 0.0373 - mse: 0.0019
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0034087800886482.
122/122 [==============================] - 1s 6ms/step - loss: 0.0021 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0365 - val_mse: 0.0019 - lr: 0.0068
Epoch 7/20
 75/122 [=================>............] - ETA: 0s - loss: 0.0021 - mae: 0.0368 - mse: 0.0019
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0017043900443241.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0021 - mae: 0.0370 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0365 - val_mse: 0.0019 - lr: 0.0034
Epoch 8/20
 76/122 [=================>............] - ETA: 0s - loss: 0.0021 - mae: 0.0370 - mse: 0.0019
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00085219502216205.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0021 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0382 - val_mse: 0.0020 - lr: 0.0017
Epoch 9/20
 64/122 [==============>...............] - ETA: 0s - loss: 0.0020 - mae: 0.0368 - mse: 0.0019
122/122 [==============================] - 1s 6ms/step - loss: 0.0021 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0373 - val_mse: 0.0019 - lr: 8.5220e-04
Epoch 10/20
100/122 [=======================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0366 - mse: 0.0019
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.000426097511081025.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0021 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0376 - val_mse: 0.0019 - lr: 8.5220e-04
Epoch 11/20
 69/122 [===============>..............] - ETA: 0s - loss: 0.0020 - mae: 0.0368 - mse: 0.0019
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002130487555405125.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0376 - val_mse: 0.0019 - lr: 4.2610e-04
Epoch 12/20
 96/122 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0368 - mse: 0.0019
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00010652437777025625.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 7ms/step - loss: 0.0020 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0376 - val_mse: 0.0019 - lr: 2.1305e-04
Epoch 13/20
 56/122 [============>.................] - ETA: 0s - loss: 0.0021 - mae: 0.0375 - mse: 0.0019
Epoch 13: ReduceLROnPlateau reducing learning rate to 5.3262188885128126e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0376 - val_mse: 0.0019 - lr: 1.0652e-04
Epoch 14/20
 94/122 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0367 - mse: 0.0019
Epoch 14: ReduceLROnPlateau reducing learning rate to 2.6631094442564063e-05.
122/122 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0373 - val_mse: 0.0019 - lr: 5.3262e-05
Epoch 15/20
 70/122 [================>.............] - ETA: 0s - loss: 0.0020 - mae: 0.0366 - mse: 0.0019
Epoch 15: ReduceLROnPlateau reducing learning rate to 1.3315547221282031e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0374 - val_mse: 0.0019 - lr: 2.6631e-05
Epoch 16/20
 74/122 [=================>............] - ETA: 0s - loss: 0.0021 - mae: 0.0371 - mse: 0.0019
Epoch 16: ReduceLROnPlateau reducing learning rate to 1e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0373 - val_mse: 0.0019 - lr: 1.3316e-05
Epoch 17/20
 83/122 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0369 - mse: 0.0019
122/122 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0368 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0373 - val_mse: 0.0019 - lr: 1.0000e-05
Epoch 18/20
 66/122 [===============>..............] - ETA: 0s - loss: 0.0020 - mae: 0.0368 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143155-gb5ex13s\files\model-best)... Done. 0.0s
122/122 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0368 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0373 - val_mse: 0.0019 - lr: 1.0000e-05
Epoch 19/20
 70/122 [================>.............] - ETA: 0s - loss: 0.0020 - mae: 0.0369 - mse: 0.0019
122/122 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0373 - val_mse: 0.0019 - lr: 1.0000e-05
Epoch 20/20
122/122 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0369 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0373 - val_mse: 0.0019 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.02727024080423971LR_[23]HN_32BS_1P_val_lossM_20epochs/model_1.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
162/162 [==============================] - 1s 3ms/step - loss: 0.0420 - mae: 0.0988 - mse: 0.0136 - val_loss: 0.0107 - val_mae: 0.0919 - val_mse: 0.0103 - lr: 0.0273
Epoch 2/20
162/162 [==============================] - 0s 930us/step - loss: 0.0106 - mae: 0.0910 - mse: 0.0102 - val_loss: 0.0103 - val_mae: 0.0902 - val_mse: 0.0100 - lr: 0.0273
Epoch 3/20
162/162 [==============================] - 0s 885us/step - loss: 0.0103 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0911 - val_mse: 0.0100 - lr: 0.0273
Epoch 4/20
 73/162 [============>.................] - ETA: 0s - loss: 0.0102 - mae: 0.0898 - mse: 0.0100
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0136351203545928.
162/162 [==============================] - 0s 1ms/step - loss: 0.0103 - mae: 0.0908 - mse: 0.0101 - val_loss: 0.0107 - val_mae: 0.0931 - val_mse: 0.0105 - lr: 0.0273
Epoch 5/20
162/162 [==============================] - 0s 848us/step - loss: 0.0101 - mae: 0.0906 - mse: 0.0099 - val_loss: 0.0100 - val_mae: 0.0902 - val_mse: 0.0098 - lr: 0.0136
Epoch 6/20
162/162 [==============================] - 0s 916us/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0099 - val_mae: 0.0902 - val_mse: 0.0097 - lr: 0.0136
Epoch 7/20
103/162 [==================>...........] - ETA: 0s - loss: 0.0100 - mae: 0.0910 - mse: 0.0099
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0068175601772964.
162/162 [==============================] - 0s 914us/step - loss: 0.0099 - mae: 0.0904 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0903 - val_mse: 0.0097 - lr: 0.0136
Epoch 8/20
 80/162 [=============>................] - ETA: 0s - loss: 0.0097 - mae: 0.0895 - mse: 0.0095
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0034087800886482.
162/162 [==============================] - 0s 951us/step - loss: 0.0099 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0904 - val_mse: 0.0097 - lr: 0.0068
Epoch 9/20
162/162 [==============================] - 0s 915us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0903 - val_mse: 0.0097 - lr: 0.0034
Epoch 10/20
 77/162 [=============>................] - ETA: 0s - loss: 0.0098 - mae: 0.0898 - mse: 0.0096
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0017043900443241.
162/162 [==============================] - 0s 951us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0034
Epoch 11/20
 95/162 [================>.............] - ETA: 0s - loss: 0.0099 - mae: 0.0906 - mse: 0.0097
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00085219502216205.
162/162 [==============================] - 0s 931us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0017
Epoch 12/20
 73/162 [============>.................] - ETA: 0s - loss: 0.0097 - mae: 0.0898 - mse: 0.0096
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.000426097511081025.
162/162 [==============================] - 0s 934us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 8.5220e-04
Epoch 13/20
 88/162 [===============>..............] - ETA: 0s - loss: 0.0099 - mae: 0.0906 - mse: 0.0098
Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002130487555405125.
162/162 [==============================] - 0s 950us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 4.2610e-04
Epoch 14/20
 91/162 [===============>..............] - ETA: 0s - loss: 0.0097 - mae: 0.0898 - mse: 0.0096
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00010652437777025625.
162/162 [==============================] - 0s 914us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 2.1305e-04
Epoch 15/20
 83/162 [==============>...............] - ETA: 0s - loss: 0.0099 - mae: 0.0907 - mse: 0.0097
Epoch 15: ReduceLROnPlateau reducing learning rate to 5.3262188885128126e-05.
162/162 [==============================] - 0s 1ms/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 1.0652e-04
Epoch 16/20
 80/162 [=============>................] - ETA: 0s - loss: 0.0097 - mae: 0.0897 - mse: 0.0096
Epoch 16: ReduceLROnPlateau reducing learning rate to 2.6631094442564063e-05.
162/162 [==============================] - 0s 942us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 5.3262e-05
Epoch 17/20
 96/162 [================>.............] - ETA: 0s - loss: 0.0098 - mae: 0.0903 - mse: 0.0097
Epoch 17: ReduceLROnPlateau reducing learning rate to 1.3315547221282031e-05.
162/162 [==============================] - 0s 917us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 2.6631e-05
Epoch 18/20
 72/162 [============>.................] - ETA: 0s - loss: 0.0097 - mae: 0.0897 - mse: 0.0096
Epoch 18: ReduceLROnPlateau reducing learning rate to 1e-05.
162/162 [==============================] - 0s 948us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 1.3316e-05
Epoch 19/20
162/162 [==============================] - 0s 1ms/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 1.0000e-05
Epoch 20/20
162/162 [==============================] - 0s 947us/step - loss: 0.0098 - mae: 0.0903 - mse: 0.0097 - val_loss: 0.0098 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.02727024080423971LR_[23]HN_32BS_1P_val_lossM_20epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
122/122 [==============================] - 0s 2ms/step - loss: 0.0647 - mae: 0.1576 - mse: 0.0283 - val_loss: 0.0251 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0273
Epoch 2/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0249 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0273
Epoch 3/20
122/122 [==============================] - 0s 951us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0273
Epoch 4/20
 92/122 [=====================>........] - ETA: 0s - loss: 0.0246 - mae: 0.1526 - mse: 0.0244
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0136351203545928.
122/122 [==============================] - 0s 961us/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0252 - val_mae: 0.1532 - val_mse: 0.0249 - lr: 0.0273
Epoch 5/20
 70/122 [================>.............] - ETA: 0s - loss: 0.0243 - mae: 0.1523 - mse: 0.0242
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0068175601772964.
122/122 [==============================] - 0s 956us/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0136
Epoch 6/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0068
Epoch 7/20
 84/122 [===================>..........] - ETA: 0s - loss: 0.0243 - mae: 0.1525 - mse: 0.0241
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0034087800886482.
122/122 [==============================] - 0s 1ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0068
Epoch 8/20
101/122 [=======================>......] - ETA: 0s - loss: 0.0242 - mae: 0.1525 - mse: 0.0241
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0017043900443241.
122/122 [==============================] - 0s 967us/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0034
Epoch 9/20
 94/122 [======================>.......] - ETA: 0s - loss: 0.0243 - mae: 0.1527 - mse: 0.0242
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00085219502216205.
122/122 [==============================] - 0s 949us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0017
Epoch 10/20
 71/122 [================>.............] - ETA: 0s - loss: 0.0241 - mae: 0.1521 - mse: 0.0240
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.000426097511081025.
122/122 [==============================] - 0s 1ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 8.5220e-04
Epoch 11/20
 89/122 [====================>.........] - ETA: 0s - loss: 0.0243 - mae: 0.1526 - mse: 0.0242
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002130487555405125.
122/122 [==============================] - 0s 932us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 4.2610e-04
Epoch 12/20
 81/122 [==================>...........] - ETA: 0s - loss: 0.0242 - mae: 0.1525 - mse: 0.0241
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00010652437777025625.
122/122 [==============================] - 0s 1ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 2.1305e-04
Epoch 13/20
 89/122 [====================>.........] - ETA: 0s - loss: 0.0243 - mae: 0.1528 - mse: 0.0242
Epoch 13: ReduceLROnPlateau reducing learning rate to 5.3262188885128126e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.0652e-04
Epoch 14/20
 98/122 [=======================>......] - ETA: 0s - loss: 0.0243 - mae: 0.1526 - mse: 0.0241
Epoch 14: ReduceLROnPlateau reducing learning rate to 2.6631094442564063e-05.
122/122 [==============================] - 0s 880us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 5.3262e-05
Epoch 15/20
 90/122 [=====================>........] - ETA: 0s - loss: 0.0242 - mae: 0.1525 - mse: 0.0241
Epoch 15: ReduceLROnPlateau reducing learning rate to 1.3315547221282031e-05.
122/122 [==============================] - 0s 960us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 2.6631e-05
Epoch 16/20
 82/122 [===================>..........] - ETA: 0s - loss: 0.0243 - mae: 0.1528 - mse: 0.0242
Epoch 16: ReduceLROnPlateau reducing learning rate to 1e-05.
122/122 [==============================] - 0s 874us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.3316e-05
Epoch 17/20
122/122 [==============================] - 0s 947us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.0000e-05
Epoch 18/20
122/122 [==============================] - 0s 951us/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.0000e-05
Epoch 19/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.0000e-05
Epoch 20/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.02727024080423971LR_[23]HN_32BS_1P_val_lossM_20epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
122/122 [==============================] - 0s 2ms/step - loss: 0.0816 - mae: 0.2038 - mse: 0.0447 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0273
Epoch 2/20
 71/122 [================>.............] - ETA: 0s - loss: 0.0427 - mae: 0.2029 - mse: 0.0424
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0136351203545928.
122/122 [==============================] - 0s 1ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0273
Epoch 3/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0136
Epoch 4/20
122/122 [==============================] - 0s 928us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0136
Epoch 5/20
 86/122 [====================>.........] - ETA: 0s - loss: 0.0421 - mae: 0.2022 - mse: 0.0420
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0068175601772964.
122/122 [==============================] - 0s 949us/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0421 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0136
Epoch 6/20
 87/122 [====================>.........] - ETA: 0s - loss: 0.0422 - mae: 0.2026 - mse: 0.0420
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0034087800886482.
122/122 [==============================] - 0s 1ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0068
Epoch 7/20
 97/122 [======================>.......] - ETA: 0s - loss: 0.0421 - mae: 0.2025 - mse: 0.0420
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0017043900443241.
122/122 [==============================] - 0s 955us/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0034
Epoch 8/20
122/122 [==============================] - 0s 964us/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0017
Epoch 9/20
 87/122 [====================>.........] - ETA: 0s - loss: 0.0421 - mae: 0.2025 - mse: 0.0420
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00085219502216205.
122/122 [==============================] - 0s 1ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0017
Epoch 10/20
 71/122 [================>.............] - ETA: 0s - loss: 0.0421 - mae: 0.2027 - mse: 0.0420
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.000426097511081025.
122/122 [==============================] - 0s 1ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 8.5220e-04
Epoch 11/20
 94/122 [======================>.......] - ETA: 0s - loss: 0.0419 - mae: 0.2021 - mse: 0.0418
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002130487555405125.
122/122 [==============================] - 0s 964us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 4.2610e-04
Epoch 12/20
 66/122 [===============>..............] - ETA: 0s - loss: 0.0418 - mae: 0.2020 - mse: 0.0417
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00010652437777025625.
122/122 [==============================] - 0s 1ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 2.1305e-04
Epoch 13/20
 85/122 [===================>..........] - ETA: 0s - loss: 0.0420 - mae: 0.2025 - mse: 0.0419
Epoch 13: ReduceLROnPlateau reducing learning rate to 5.3262188885128126e-05.
122/122 [==============================] - 0s 982us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.0652e-04
Epoch 14/20
 82/122 [===================>..........] - ETA: 0s - loss: 0.0420 - mae: 0.2026 - mse: 0.0419
Epoch 14: ReduceLROnPlateau reducing learning rate to 2.6631094442564063e-05.
122/122 [==============================] - 0s 935us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 5.3262e-05
Epoch 15/20
104/122 [========================>.....] - ETA: 0s - loss: 0.0420 - mae: 0.2024 - mse: 0.0419
Epoch 15: ReduceLROnPlateau reducing learning rate to 1.3315547221282031e-05.
122/122 [==============================] - 0s 958us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 2.6631e-05
Epoch 16/20
 95/122 [======================>.......] - ETA: 0s - loss: 0.0420 - mae: 0.2024 - mse: 0.0419
Epoch 16: ReduceLROnPlateau reducing learning rate to 1e-05.
122/122 [==============================] - 0s 956us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.3316e-05
Epoch 17/20
122/122 [==============================] - 0s 959us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.0000e-05
Epoch 18/20
122/122 [==============================] - 0s 998us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.0000e-05
Epoch 19/20
122/122 [==============================] - 0s 997us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.0000e-05
Epoch 20/20
122/122 [==============================] - 0s 917us/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.02727024080423971LR_[23]HN_32BS_1P_val_lossM_20epochs/model_4.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
122/122 [==============================] - 0s 2ms/step - loss: 0.0999 - mae: 0.2526 - mse: 0.0667 - val_loss: 0.0657 - val_mae: 0.2532 - val_mse: 0.0656 - lr: 0.0273
Epoch 2/20
122/122 [==============================] - 0s 955us/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0273
Epoch 3/20
 96/122 [======================>.......] - ETA: 0s - loss: 0.0649 - mae: 0.2524 - mse: 0.0649
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0136351203545928.
122/122 [==============================] - 0s 956us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0273
Epoch 4/20
 88/122 [====================>.........] - ETA: 0s - loss: 0.0650 - mae: 0.2526 - mse: 0.0650
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0068175601772964.
122/122 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0654 - lr: 0.0136
Epoch 5/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0068
Epoch 6/20
 77/122 [=================>............] - ETA: 0s - loss: 0.0649 - mae: 0.2525 - mse: 0.0649
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0034087800886482.
122/122 [==============================] - 0s 969us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0068
Epoch 7/20
 89/122 [====================>.........] - ETA: 0s - loss: 0.0649 - mae: 0.2526 - mse: 0.0649
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0017043900443241.
122/122 [==============================] - 0s 957us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0034
Epoch 8/20
 83/122 [===================>..........] - ETA: 0s - loss: 0.0651 - mae: 0.2529 - mse: 0.0650
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00085219502216205.
122/122 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0017
Epoch 9/20
 87/122 [====================>.........] - ETA: 0s - loss: 0.0648 - mae: 0.2523 - mse: 0.0648
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.000426097511081025.
122/122 [==============================] - 0s 955us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 8.5220e-04
Epoch 10/20
 80/122 [==================>...........] - ETA: 0s - loss: 0.0649 - mae: 0.2525 - mse: 0.0648
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002130487555405125.
122/122 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 4.2610e-04
Epoch 11/20
 98/122 [=======================>......] - ETA: 0s - loss: 0.0646 - mae: 0.2520 - mse: 0.0646
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010652437777025625.
122/122 [==============================] - 0s 953us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 2.1305e-04
Epoch 12/20
 94/122 [======================>.......] - ETA: 0s - loss: 0.0649 - mae: 0.2525 - mse: 0.0649
Epoch 12: ReduceLROnPlateau reducing learning rate to 5.3262188885128126e-05.
122/122 [==============================] - 0s 955us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 1.0652e-04
Epoch 13/20
 88/122 [====================>.........] - ETA: 0s - loss: 0.0649 - mae: 0.2526 - mse: 0.0649
Epoch 13: ReduceLROnPlateau reducing learning rate to 2.6631094442564063e-05.
122/122 [==============================] - 0s 957us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 5.3262e-05
Epoch 14/20
 82/122 [===================>..........] - ETA: 0s - loss: 0.0650 - mae: 0.2527 - mse: 0.0650
Epoch 14: ReduceLROnPlateau reducing learning rate to 1.3315547221282031e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 2.6631e-05
Epoch 15/20
 83/122 [===================>..........] - ETA: 0s - loss: 0.0650 - mae: 0.2528 - mse: 0.0650
Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 1.3316e-05
Epoch 16/20
122/122 [==============================] - 0s 956us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 1.0000e-05
Epoch 17/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 1.0000e-05
Epoch 18/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 1.0000e-05
Epoch 19/20
122/122 [==============================] - 0s 944us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 1.0000e-05
Epoch 20/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.02727024080423971LR_[23]HN_32BS_1P_val_lossM_20epochs/model_5.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
122/122 [==============================] - 0s 2ms/step - loss: 0.1252 - mae: 0.3025 - mse: 0.0935 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0932 - lr: 0.0273
Epoch 2/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0273
Epoch 3/20
 94/122 [======================>.......] - ETA: 0s - loss: 0.0926 - mae: 0.3024 - mse: 0.0926
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0136351203545928.
122/122 [==============================] - 0s 957us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0273
Epoch 4/20
 83/122 [===================>..........] - ETA: 0s - loss: 0.0927 - mae: 0.3026 - mse: 0.0927
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0068175601772964.
122/122 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0136
Epoch 5/20
 84/122 [===================>..........] - ETA: 0s - loss: 0.0927 - mae: 0.3025 - mse: 0.0927
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0034087800886482.
122/122 [==============================] - 0s 953us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0068
Epoch 6/20
 75/122 [=================>............] - ETA: 0s - loss: 0.0925 - mae: 0.3022 - mse: 0.0925
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0017043900443241.
122/122 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0034
Epoch 7/20
 95/122 [======================>.......] - ETA: 0s - loss: 0.0927 - mae: 0.3025 - mse: 0.0927
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00085219502216205.
122/122 [==============================] - 0s 924us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0017
Epoch 8/20
 87/122 [====================>.........] - ETA: 0s - loss: 0.0926 - mae: 0.3025 - mse: 0.0926
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.000426097511081025.
122/122 [==============================] - 0s 963us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 8.5220e-04
Epoch 9/20
 76/122 [=================>............] - ETA: 0s - loss: 0.0926 - mae: 0.3024 - mse: 0.0926
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002130487555405125.
122/122 [==============================] - 0s 976us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 4.2610e-04
Epoch 10/20
 92/122 [=====================>........] - ETA: 0s - loss: 0.0926 - mae: 0.3024 - mse: 0.0926
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00010652437777025625.
122/122 [==============================] - 0s 980us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.1305e-04
Epoch 11/20
 87/122 [====================>.........] - ETA: 0s - loss: 0.0928 - mae: 0.3028 - mse: 0.0928
Epoch 11: ReduceLROnPlateau reducing learning rate to 5.3262188885128126e-05.
122/122 [==============================] - 0s 956us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0652e-04
Epoch 12/20
106/122 [=========================>....] - ETA: 0s - loss: 0.0927 - mae: 0.3025 - mse: 0.0927
Epoch 12: ReduceLROnPlateau reducing learning rate to 2.6631094442564063e-05.
122/122 [==============================] - 0s 962us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 5.3262e-05
Epoch 13/20
 70/122 [================>.............] - ETA: 0s - loss: 0.0930 - mae: 0.3031 - mse: 0.0930
Epoch 13: ReduceLROnPlateau reducing learning rate to 1.3315547221282031e-05.
122/122 [==============================] - 0s 958us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 2.6631e-05
Epoch 14/20
 61/122 [==============>...............] - ETA: 0s - loss: 0.0926 - mae: 0.3024 - mse: 0.0926
Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.3316e-05
Epoch 15/20
122/122 [==============================] - 0s 956us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 16/20
122/122 [==============================] - 0s 959us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 17/20
122/122 [==============================] - 0s 995us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 18/20
122/122 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 19/20
122/122 [==============================] - 0s 999us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 20/20
122/122 [==============================] - 0s 963us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.02727024080423971LR_[23]HN_32BS_1P_val_lossM_20epochs/model_6.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
122/122 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3525 - mse: 0.1259 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0273
Epoch 2/20
 82/122 [===================>..........] - ETA: 0s - loss: 0.1251 - mae: 0.3520 - mse: 0.1251
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0136351203545928.
122/122 [==============================] - 0s 914us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0273
Epoch 3/20
104/122 [========================>.....] - ETA: 0s - loss: 0.1255 - mae: 0.3525 - mse: 0.1254
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0068175601772964.
122/122 [==============================] - 0s 928us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0136
Epoch 4/20
 93/122 [=====================>........] - ETA: 0s - loss: 0.1254 - mae: 0.3525 - mse: 0.1254
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0034087800886482.
122/122 [==============================] - 0s 961us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0068
Epoch 5/20
 70/122 [================>.............] - ETA: 0s - loss: 0.1257 - mae: 0.3529 - mse: 0.1257
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0017043900443241.
122/122 [==============================] - 0s 951us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0034
Epoch 6/20
 66/122 [===============>..............] - ETA: 0s - loss: 0.1247 - mae: 0.3515 - mse: 0.1247
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00085219502216205.
122/122 [==============================] - 0s 995us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0017
Epoch 7/20
 87/122 [====================>.........] - ETA: 0s - loss: 0.1257 - mae: 0.3529 - mse: 0.1257
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.000426097511081025.
122/122 [==============================] - 0s 970us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 8.5220e-04
Epoch 8/20
 85/122 [===================>..........] - ETA: 0s - loss: 0.1252 - mae: 0.3521 - mse: 0.1252
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002130487555405125.
122/122 [==============================] - 0s 1ms/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 4.2610e-04
Epoch 9/20
109/122 [=========================>....] - ETA: 0s - loss: 0.1254 - mae: 0.3524 - mse: 0.1254
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010652437777025625.
122/122 [==============================] - 0s 890us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.1305e-04
Epoch 10/20
 80/122 [==================>...........] - ETA: 0s - loss: 0.1254 - mae: 0.3525 - mse: 0.1254
Epoch 10: ReduceLROnPlateau reducing learning rate to 5.3262188885128126e-05.
122/122 [==============================] - 0s 999us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0652e-04
Epoch 11/20
 73/122 [================>.............] - ETA: 0s - loss: 0.1253 - mae: 0.3524 - mse: 0.1253
Epoch 11: ReduceLROnPlateau reducing learning rate to 2.6631094442564063e-05.
122/122 [==============================] - 0s 947us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 5.3262e-05
Epoch 12/20
 66/122 [===============>..............] - ETA: 0s - loss: 0.1251 - mae: 0.3521 - mse: 0.1251
Epoch 12: ReduceLROnPlateau reducing learning rate to 1.3315547221282031e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 2.6631e-05
Epoch 13/20
 87/122 [====================>.........] - ETA: 0s - loss: 0.1254 - mae: 0.3525 - mse: 0.1254
Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.
122/122 [==============================] - 0s 956us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.3316e-05
Epoch 14/20
122/122 [==============================] - 0s 967us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 15/20
122/122 [==============================] - 0s 946us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 16/20
122/122 [==============================] - 0s 961us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 17/20
122/122 [==============================] - 0s 1000us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 18/20
122/122 [==============================] - 0s 1ms/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 19/20
122/122 [==============================] - 0s 951us/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 20/20
122/122 [==============================] - 0s 1ms/step - loss: 0.1254 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.02727024080423971LR_[23]HN_32BS_1P_val_lossM_20epochs/model_7.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
122/122 [==============================] - 0s 2ms/step - loss: 0.1986 - mae: 0.4025 - mse: 0.1633 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0273
Epoch 2/20
 86/122 [====================>.........] - ETA: 0s - loss: 0.1635 - mae: 0.4029 - mse: 0.1635
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0136351203545928.
122/122 [==============================] - 0s 999us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0273
Epoch 3/20
105/122 [========================>.....] - ETA: 0s - loss: 0.1633 - mae: 0.4027 - mse: 0.1633
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0068175601772964.
122/122 [==============================] - 0s 952us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0136
Epoch 4/20
 86/122 [====================>.........] - ETA: 0s - loss: 0.1634 - mae: 0.4028 - mse: 0.1634
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0034087800886482.
122/122 [==============================] - 0s 956us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0068
Epoch 5/20
 90/122 [=====================>........] - ETA: 0s - loss: 0.1632 - mae: 0.4025 - mse: 0.1632
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0017043900443241.
122/122 [==============================] - 0s 960us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0034
Epoch 6/20
 84/122 [===================>..........] - ETA: 0s - loss: 0.1635 - mae: 0.4029 - mse: 0.1635
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00085219502216205.
122/122 [==============================] - 0s 1ms/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0017
Epoch 7/20
102/122 [========================>.....] - ETA: 0s - loss: 0.1631 - mae: 0.4024 - mse: 0.1631
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.000426097511081025.
122/122 [==============================] - 0s 957us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 8.5220e-04
Epoch 8/20
 92/122 [=====================>........] - ETA: 0s - loss: 0.1632 - mae: 0.4025 - mse: 0.1632
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002130487555405125.
122/122 [==============================] - 0s 956us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 4.2610e-04
Epoch 9/20
 80/122 [==================>...........] - ETA: 0s - loss: 0.1632 - mae: 0.4025 - mse: 0.1632
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010652437777025625.
122/122 [==============================] - 0s 1ms/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.1305e-04
Epoch 10/20
 65/122 [==============>...............] - ETA: 0s - loss: 0.1634 - mae: 0.4028 - mse: 0.1634
Epoch 10: ReduceLROnPlateau reducing learning rate to 5.3262188885128126e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0652e-04
Epoch 11/20
 94/122 [======================>.......] - ETA: 0s - loss: 0.1632 - mae: 0.4025 - mse: 0.1632
Epoch 11: ReduceLROnPlateau reducing learning rate to 2.6631094442564063e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 5.3262e-05
Epoch 12/20
 97/122 [======================>.......] - ETA: 0s - loss: 0.1633 - mae: 0.4027 - mse: 0.1633
Epoch 12: ReduceLROnPlateau reducing learning rate to 1.3315547221282031e-05.
122/122 [==============================] - 0s 995us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 2.6631e-05
Epoch 13/20
 75/122 [=================>............] - ETA: 0s - loss: 0.1631 - mae: 0.4025 - mse: 0.1631
Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.
122/122 [==============================] - 0s 957us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.3316e-05
Epoch 14/20
122/122 [==============================] - 0s 1ms/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 15/20
122/122 [==============================] - 0s 1ms/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 16/20
122/122 [==============================] - 0s 958us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 17/20
122/122 [==============================] - 0s 959us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 18/20
122/122 [==============================] - 0s 956us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 19/20
122/122 [==============================] - 0s 981us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
Epoch 20/20
122/122 [==============================] - 0s 978us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.02727024080423971LR_[23]HN_32BS_1P_val_lossM_20epochs/model_8.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
122/122 [==============================] - 0s 2ms/step - loss: 0.2371 - mae: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0273
Epoch 2/20
104/122 [========================>.....] - ETA: 0s - loss: 0.2061 - mae: 0.4527 - mse: 0.2061
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0136351203545928.
122/122 [==============================] - 0s 955us/step - loss: 0.2060 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0273
Epoch 3/20
 77/122 [=================>............] - ETA: 0s - loss: 0.2060 - mae: 0.4526 - mse: 0.2060
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0068175601772964.
122/122 [==============================] - 0s 1ms/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0136
Epoch 4/20
 76/122 [=================>............] - ETA: 0s - loss: 0.2061 - mae: 0.4527 - mse: 0.2061
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0034087800886482.
122/122 [==============================] - 0s 1ms/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0068
Epoch 5/20
 95/122 [======================>.......] - ETA: 0s - loss: 0.2060 - mae: 0.4526 - mse: 0.2060
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0017043900443241.
122/122 [==============================] - 0s 980us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0034
Epoch 6/20
 85/122 [===================>..........] - ETA: 0s - loss: 0.2058 - mae: 0.4523 - mse: 0.2058
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00085219502216205.
122/122 [==============================] - 0s 1ms/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0017
Epoch 7/20
 97/122 [======================>.......] - ETA: 0s - loss: 0.2058 - mae: 0.4523 - mse: 0.2058
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.000426097511081025.
122/122 [==============================] - 0s 953us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 8.5220e-04
Epoch 8/20
 88/122 [====================>.........] - ETA: 0s - loss: 0.2057 - mae: 0.4522 - mse: 0.2057
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002130487555405125.
122/122 [==============================] - 0s 967us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 4.2610e-04
Epoch 9/20
 64/122 [==============>...............] - ETA: 0s - loss: 0.2058 - mae: 0.4523 - mse: 0.2058
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010652437777025625.
122/122 [==============================] - 0s 1ms/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 2.1305e-04
Epoch 10/20
 85/122 [===================>..........] - ETA: 0s - loss: 0.2055 - mae: 0.4520 - mse: 0.2055
Epoch 10: ReduceLROnPlateau reducing learning rate to 5.3262188885128126e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0652e-04
Epoch 11/20
101/122 [=======================>......] - ETA: 0s - loss: 0.2059 - mae: 0.4525 - mse: 0.2059
Epoch 11: ReduceLROnPlateau reducing learning rate to 2.6631094442564063e-05.
122/122 [==============================] - 0s 958us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 5.3262e-05
Epoch 12/20
 96/122 [======================>.......] - ETA: 0s - loss: 0.2059 - mae: 0.4524 - mse: 0.2059
Epoch 12: ReduceLROnPlateau reducing learning rate to 1.3315547221282031e-05.
122/122 [==============================] - 0s 961us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 2.6631e-05
Epoch 13/20
 87/122 [====================>.........] - ETA: 0s - loss: 0.2061 - mae: 0.4527 - mse: 0.2061
Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.
122/122 [==============================] - 0s 1ms/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.3316e-05
Epoch 14/20
122/122 [==============================] - 0s 969us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 15/20
122/122 [==============================] - 0s 994us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 16/20
122/122 [==============================] - 0s 955us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 17/20
122/122 [==============================] - 0s 1ms/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 18/20
122/122 [==============================] - 0s 966us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 19/20
122/122 [==============================] - 0s 940us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
Epoch 20/20
122/122 [==============================] - 0s 1ms/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.02727024080423971LR_[23]HN_32BS_1P_val_lossM_20epochs/model_9.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0001s vs `on_train_batch_end` time: 0.0002s). Check your callbacks.