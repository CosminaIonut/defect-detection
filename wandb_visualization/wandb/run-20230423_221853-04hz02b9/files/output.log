Epoch 1/200
185/243 [=====================>........] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
243/243 [==============================] - 1s 5ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 2/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 3/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 4/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 5/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 6/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 7/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 8/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 9/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 10/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 11/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 12/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 13/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 14/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 15/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 16/200
180/243 [=====================>........] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.8581
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.04874482378363609.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0975
Epoch 17/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 18/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 19/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 20/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 21/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 22/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 23/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 24/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 25/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 26/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 27/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 28/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 29/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 30/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 31/200
203/243 [========================>.....] - ETA: 0s - loss: 0.8571 - mae: 0.9248 - mse: 0.8571
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.024372411891818047.
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0487
Epoch 32/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 33/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 34/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 35/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 36/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 37/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 38/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 39/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 40/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 41/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 42/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 43/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 44/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 45/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 46/200
234/243 [===========================>..] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.012186205945909023.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0244
Epoch 47/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 48/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 49/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 50/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 51/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 52/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 53/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 54/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 55/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 56/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 57/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 58/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 59/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 60/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 61/200
233/243 [===========================>..] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.006093102972954512.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0122
Epoch 62/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0061
Epoch 63/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0061
Epoch 64/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0061
Epoch 65/200
126/243 [==============>...............] - ETA: 0s - loss: 0.8593 - mae: 0.9260 - mse: 0.8593
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0061
Epoch 68/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0061
Epoch 69/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0061
Epoch 70/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0061
Epoch 71/200
 69/243 [=======>......................] - ETA: 0s - loss: 0.8562 - mae: 0.9242 - mse: 0.8562
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0061
Epoch 75/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0061
Epoch 76/200
207/243 [========================>.....] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.003046551486477256.
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0061
Epoch 77/200
 72/243 [=======>......................] - ETA: 0s - loss: 0.8557 - mae: 0.9240 - mse: 0.8557
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 81/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 82/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 83/200
 68/243 [=======>......................] - ETA: 0s - loss: 0.8603 - mae: 0.9265 - mse: 0.8603
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 88/200
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0030
Epoch 89/200
211/243 [=========================>....] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 94/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 95/200
149/243 [=================>............] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
Epoch 101/200
  1/243 [..............................] - ETA: 0s - loss: 0.8728 - mae: 0.9334 - mse: 0.8728
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0015
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.6164e-04
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.6164e-04
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.8082e-04
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.8082e-04
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.8082e-04
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.9041e-04
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.9041e-04
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.5205e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.5205e-05
220/243 [==========================>...] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.85758579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.5205e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.7602e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.7602e-05
158/243 [==================>...........] - ETA: 0s - loss: 0.8588 - mae: 0.9257 - mse: 0.85888579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.7602e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.3801e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.3801e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1901e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1901e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1901e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0975e-05
153/323 [=============>................] - ETA: 0s - loss: 0.6405 - mae: 0.7982 - mse: 0.64056431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0975e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0487e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0487e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0487e-05
296/323 [==========================>...] - ETA: 0s - loss: 0.6435 - mae: 0.8001 - mse: 0.64356431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0487e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0244e-05
  1/323 [..............................] - ETA: 5s - loss: 0.6689 - mae: 0.8161 - mse: 0.66896431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0244e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0244e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 57/200===========================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 66/200duceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 83/200duceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 119/200uceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 132/200uceLROnPlateau reducing learning rate to 0.006093102972954512.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 136: ReduceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 142/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 146/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 150/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 155/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 160/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 165/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 170/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 174/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 179/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 183/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
Epoch 196/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0122e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200rained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 16/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 22/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 29/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 35/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 35/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 42/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 53/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 63/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 98/200duceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 109/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 115/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 120/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 126/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 131/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 136/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 153/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 158/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 164/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 180/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 190/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 195/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/20000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 9/20000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 15/2000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 20/2000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 25/2000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 31/2000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 36/2000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 41/2000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 46/2000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 51/2000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 56/2000uceLROnPlateau reducing learning rate to 0.001523275743238628.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 66/200duceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 71/200duceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 111/200uceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 0.006093102972954512.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 0.000380818935809657.9707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 127/200duceLROnPlateau reducing learning rate to 0.000380818935809657.9707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 132/200duceLROnPlateau reducing learning rate to 0.000380818935809657.9707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 136: ReduceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 142/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 146/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 151/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 156/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 166/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 171/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 177/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 181: ReduceLROnPlateau reducing learning rate to 2.380118348810356e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 187/200duceLROnPlateau reducing learning rate to 2.380118348810356e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 2.380118348810356e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 196: ReduceLROnPlateau reducing learning rate to 1.190059174405178e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
222/243 [==========================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
222/243 [==========================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 2/200========================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 9/200========================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 14/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 18/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 23/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 28/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 33/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 38/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 43/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 48/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 53/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 58/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 63/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 68/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 73/200=======================>...] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.003046551486477256.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 0.003046551486477256.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 87/200duceLROnPlateau reducing learning rate to 0.003046551486477256.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.003046551486477256.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 0.003046551486477256.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.003046551486477256.4752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 106: ReduceLROnPlateau reducing learning rate to 0.000761637871619314.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 112/200duceLROnPlateau reducing learning rate to 0.000761637871619314.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 117/200duceLROnPlateau reducing learning rate to 0.000761637871619314.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 127/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 132/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 136/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 141/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 146/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 151/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 156/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 161/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 166/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 171/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 177/200duceLROnPlateau reducing learning rate to 0.000380818935809657.752 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 181: ReduceLROnPlateau reducing learning rate to 2.380118348810356e-05.52 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 186/200duceLROnPlateau reducing learning rate to 2.380118348810356e-05.52 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 2.380118348810356e-05.52 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 196: ReduceLROnPlateau reducing learning rate to 1.190059174405178e-05.52 - mse: 0.2277  I_16BS_15P_val_lossM_200epochs/model_2.h5 val_mse: 0.6442 - lr: 0.0122e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200rained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 16/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 16/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 22/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 27/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 42/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 52/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 63/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 69/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 0.012186205945909023.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 83/200duceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 98/200duceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 108/200uceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 113/200uceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 119/200uceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 129/200uceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 132/200uceLROnPlateau reducing learning rate to 0.003046551486477256.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 136: ReduceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 142/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 147/200duceLROnPlateau reducing learning rate to 0.0001904094679048285.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 151: ReduceLROnPlateau reducing learning rate to 9.520473395241424e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 157/200duceLROnPlateau reducing learning rate to 9.520473395241424e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 162/200duceLROnPlateau reducing learning rate to 9.520473395241424e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 166: ReduceLROnPlateau reducing learning rate to 4.760236697620712e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 172/200duceLROnPlateau reducing learning rate to 4.760236697620712e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 177/200duceLROnPlateau reducing learning rate to 4.760236697620712e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 181: ReduceLROnPlateau reducing learning rate to 2.380118348810356e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 187/200duceLROnPlateau reducing learning rate to 2.380118348810356e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 192/200duceLROnPlateau reducing learning rate to 2.380118348810356e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 196/200duceLROnPlateau reducing learning rate to 2.380118348810356e-05.707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_5.h5 val_mse: 0.6442 - lr: 0.0122e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0028s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0028s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200rained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 16/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 16/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 22/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 45/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 45/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 71/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 77/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 91/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 111/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 126/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 139/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 145/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 150/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 155/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 160/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 165/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 180/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 190/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 195/200uceLROnPlateau reducing learning rate to 0.024372411891818047.09707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 val_mse: 0.6442 - lr: 0.0122e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200rained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 9/200rained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 15/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 21/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 26/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 31/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 36/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 41/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 46/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 51/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 56/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 61/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 65/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 70/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 75/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 80/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 85/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 90/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 95/200ained_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 100/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 105/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 110/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 115/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 120/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 125/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 130/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 135/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 140/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 144/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 149/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 155/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 160/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 165/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 170/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 175/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 180/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 186/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 191/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 196/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 200/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 200/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/20000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 9/20000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 16/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 21/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 26/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 31/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 36/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 41/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 46/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 51/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 55/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 61/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 64/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 70/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 75/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 79/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 85/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 90/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 94/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 99/2000ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 104/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 109/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 114/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 119/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 124/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 130/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 134/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 139/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 144/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 149/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 154/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 160/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 165/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 170/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 175/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 179/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 184/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 190/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 195/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 200/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05
Epoch 200/200ined_models/models_segments_overlap-cnn_rmsprop_0.09748964730809707LR_[34]CHN_8CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 val_mse: 0.6442 - lr: 0.0122e-05