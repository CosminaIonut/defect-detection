Y [[0.096]
 [0.014]
 [0.026]
 ...
 [0.02 ]
 [0.074]
 [0.086]]
X [[0.00293123 0.00242012 0.00202833 ... 0.00123472 0.00110405 0.0010347 ]
 [0.01003065 0.00520174 0.00222913 ... 0.00047567 0.00160212 0.00299861]
 [0.0651209  0.0624661  0.06012098 ... 0.0531939  0.05098064 0.04881471]
 ...
 [0.0141577  0.00361839 0.00010273 ... 0.00797822 0.00888544 0.00702019]
 [0.01199593 0.00969102 0.00783776 ... 0.00348408 0.0024832  0.00170184]
 [0.02677437 0.00450759 0.00090166 ... 0.01828678 0.01288733 0.00463391]]
Epoch 1/100
71/86 [=======================>......] - ETA: 0s - loss: 2.7090 - mae: 0.0353 - mse: 0.0037
wandb: WARNING The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.
86/86 [==============================] - 2s 17ms/step - loss: 2.2747 - mae: 0.0339 - mse: 0.0033 - val_loss: 0.0523 - val_mae: 0.0408 - val_mse: 0.0024 - lr: 0.0100
Epoch 2/100
56/86 [==================>...........] - ETA: 0s - loss: 0.0586 - mae: 0.0498 - mse: 0.0186
86/86 [==============================] - 1s 12ms/step - loss: 0.0492 - mae: 0.0432 - mse: 0.0128 - val_loss: 0.0327 - val_mae: 0.0260 - val_mse: 8.9798e-04 - lr: 0.0100
Epoch 3/100
86/86 [==============================] - 0s 2ms/step - loss: 0.0348 - mae: 0.0267 - mse: 0.0010 - val_loss: 0.0354 - val_mae: 0.0268 - val_mse: 9.7103e-04 - lr: 0.0100
Epoch 4/100
86/86 [==============================] - 0s 2ms/step - loss: 0.0351 - mae: 0.0267 - mse: 0.0010 - val_loss: 0.0335 - val_mae: 0.0275 - val_mse: 0.0011 - lr: 0.0100
Epoch 5/100
68/86 [======================>.......] - ETA: 0s - loss: 0.0348 - mae: 0.0256 - mse: 9.2249e-04
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.
86/86 [==============================] - 0s 2ms/step - loss: 0.0348 - mae: 0.0256 - mse: 9.2253e-04 - val_loss: 0.0351 - val_mae: 0.0266 - val_mse: 9.8143e-04 - lr: 0.0100
Epoch 6/100
60/86 [===================>..........] - ETA: 0s - loss: 0.0062 - mae: 0.0254 - mse: 8.6744e-04
86/86 [==============================] - 1s 12ms/step - loss: 0.0069 - mae: 0.0251 - mse: 8.5096e-04 - val_loss: 0.0082 - val_mae: 0.0259 - val_mse: 8.9833e-04 - lr: 0.0050
Epoch 7/100
86/86 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0250 - mse: 8.4667e-04 - val_loss: 0.0093 - val_mae: 0.0290 - val_mse: 0.0012 - lr: 0.0050
Epoch 8/100
79/86 [==========================>...] - ETA: 0s - loss: 0.0088 - mae: 0.0251 - mse: 8.5137e-04
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.
86/86 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0250 - mse: 8.4397e-04 - val_loss: 0.0089 - val_mae: 0.0257 - val_mse: 8.7029e-04 - lr: 0.0050
Epoch 9/100
86/86 [==============================] - ETA: 0s - loss: 0.0024 - mae: 0.0248 - mse: 8.2953e-04
86/86 [==============================] - 1s 11ms/step - loss: 0.0024 - mae: 0.0248 - mse: 8.2953e-04 - val_loss: 0.0029 - val_mae: 0.0266 - val_mse: 9.4921e-04 - lr: 0.0025
Epoch 10/100
63/86 [====================>.........] - ETA: 0s - loss: 0.0028 - mae: 0.0248 - mse: 8.2447e-04
86/86 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0249 - mse: 8.3114e-04 - val_loss: 0.0029 - val_mae: 0.0258 - val_mse: 8.7680e-04 - lr: 0.0025
Epoch 11/100
72/86 [========================>.....] - ETA: 0s - loss: 0.0028 - mae: 0.0247 - mse: 8.2338e-04
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.
86/86 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0248 - mse: 8.2987e-04 - val_loss: 0.0030 - val_mae: 0.0267 - val_mse: 9.9664e-04 - lr: 0.0025
Epoch 12/100
71/86 [=======================>......] - ETA: 0s - loss: 0.0012 - mae: 0.0249 - mse: 8.3255e-04
86/86 [==============================] - 1s 13ms/step - loss: 0.0012 - mae: 0.0248 - mse: 8.2645e-04 - val_loss: 0.0014 - val_mae: 0.0258 - val_mse: 8.7723e-04 - lr: 0.0012
Epoch 13/100
86/86 [==============================] - 1s 6ms/step - loss: 0.0013 - mae: 0.0248 - mse: 8.2426e-04 - val_loss: 0.0014 - val_mae: 0.0259 - val_mse: 9.0383e-04 - lr: 0.0012
Epoch 14/100
78/86 [==========================>...] - ETA: 0s - loss: 0.0013 - mae: 0.0248 - mse: 8.2703e-04
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.
86/86 [==============================] - 0s 6ms/step - loss: 0.0013 - mae: 0.0248 - mse: 8.2450e-04 - val_loss: 0.0014 - val_mae: 0.0261 - val_mse: 9.2842e-04 - lr: 0.0012
Epoch 15/100
72/86 [========================>.....] - ETA: 0s - loss: 9.1443e-04 - mae: 0.0248 - mse: 8.2568e-04
86/86 [==============================] - 3s 40ms/step - loss: 9.1799e-04 - mae: 0.0248 - mse: 8.2461e-04 - val_loss: 9.9244e-04 - val_mae: 0.0257 - val_mse: 8.6678e-04 - lr: 6.2500e-04
Epoch 16/100
82/86 [===========================>..] - ETA: 0s - loss: 9.4878e-04 - mae: 0.0248 - mse: 8.2464e-04
86/86 [==============================] - 4s 42ms/step - loss: 9.4655e-04 - mae: 0.0247 - mse: 8.2237e-04 - val_loss: 9.8999e-04 - val_mae: 0.0257 - val_mse: 8.6538e-04 - lr: 6.2500e-04
Epoch 17/100
77/86 [=========================>....] - ETA: 0s - loss: 9.4204e-04 - mae: 0.0247 - mse: 8.1719e-04
Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.
86/86 [==============================] - 1s 8ms/step - loss: 9.4547e-04 - mae: 0.0247 - mse: 8.2051e-04 - val_loss: 9.9105e-04 - val_mae: 0.0257 - val_mse: 8.6699e-04 - lr: 6.2500e-04
Epoch 18/100
80/86 [==========================>...] - ETA: 0s - loss: 8.4351e-04 - mae: 0.0248 - mse: 8.2125e-04
86/86 [==============================] - 4s 46ms/step - loss: 8.4314e-04 - mae: 0.0247 - mse: 8.1986e-04 - val_loss: 8.9873e-04 - val_mae: 0.0257 - val_mse: 8.6523e-04 - lr: 3.1250e-04
Epoch 19/100
79/86 [==========================>...] - ETA: 0s - loss: 8.5774e-04 - mae: 0.0248 - mse: 8.2669e-04
86/86 [==============================] - 3s 38ms/step - loss: 8.5144e-04 - mae: 0.0247 - mse: 8.2039e-04 - val_loss: 8.9614e-04 - val_mae: 0.0257 - val_mse: 8.6523e-04 - lr: 3.1250e-04
Epoch 20/100
85/86 [============================>.] - ETA: 0s - loss: 8.5063e-04 - mae: 0.0247 - mse: 8.1928e-04
Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.
86/86 [==============================] - 1s 7ms/step - loss: 8.5138e-04 - mae: 0.0247 - mse: 8.2003e-04 - val_loss: 8.9699e-04 - val_mae: 0.0256 - val_mse: 8.6546e-04 - lr: 3.1250e-04
Epoch 21/100
81/86 [===========================>..] - ETA: 0s - loss: 8.2489e-04 - mae: 0.0247 - mse: 8.1913e-04
86/86 [==============================] - 2s 28ms/step - loss: 8.2573e-04 - mae: 0.0247 - mse: 8.1979e-04 - val_loss: 8.7496e-04 - val_mae: 0.0257 - val_mse: 8.6630e-04 - lr: 1.5625e-04
Epoch 22/100
78/86 [==========================>...] - ETA: 0s - loss: 8.3957e-04 - mae: 0.0250 - mse: 8.3179e-04
86/86 [==============================] - 4s 43ms/step - loss: 8.2798e-04 - mae: 0.0247 - mse: 8.2020e-04 - val_loss: 8.7315e-04 - val_mae: 0.0257 - val_mse: 8.6531e-04 - lr: 1.5625e-04
Epoch 23/100
84/86 [============================>.] - ETA: 0s - loss: 8.2664e-04 - mae: 0.0247 - mse: 8.1888e-04
Epoch 23: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.
86/86 [==============================] - 1s 10ms/step - loss: 8.2769e-04 - mae: 0.0247 - mse: 8.1993e-04 - val_loss: 8.7328e-04 - val_mae: 0.0257 - val_mse: 8.6539e-04 - lr: 1.5625e-04
Epoch 24/100
79/86 [==========================>...] - ETA: 0s - loss: 8.2045e-04 - mae: 0.0248 - mse: 8.1901e-04
86/86 [==============================] - 3s 37ms/step - loss: 8.2099e-04 - mae: 0.0247 - mse: 8.1953e-04 - val_loss: 8.6772e-04 - val_mae: 0.0257 - val_mse: 8.6531e-04 - lr: 7.8125e-05
Epoch 25/100
78/86 [==========================>...] - ETA: 0s - loss: 8.2490e-04 - mae: 0.0247 - mse: 8.2297e-04
86/86 [==============================] - 3s 36ms/step - loss: 8.2150e-04 - mae: 0.0247 - mse: 8.1956e-04 - val_loss: 8.6731e-04 - val_mae: 0.0257 - val_mse: 8.6537e-04 - lr: 7.8125e-05
Epoch 26/100
81/86 [===========================>..] - ETA: 0s - loss: 8.2190e-04 - mae: 0.0247 - mse: 8.1995e-04
Epoch 26: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.
86/86 [==============================] - 1s 8ms/step - loss: 8.2142e-04 - mae: 0.0247 - mse: 8.1946e-04 - val_loss: 8.6739e-04 - val_mae: 0.0257 - val_mse: 8.6532e-04 - lr: 7.8125e-05
Epoch 27/100
81/86 [===========================>..] - ETA: 0s - loss: 8.2080e-04 - mae: 0.0247 - mse: 8.2036e-04
86/86 [==============================] - 3s 40ms/step - loss: 8.1981e-04 - mae: 0.0247 - mse: 8.1937e-04 - val_loss: 8.6588e-04 - val_mae: 0.0257 - val_mse: 8.6524e-04 - lr: 3.9062e-05
Epoch 28/100
83/86 [===========================>..] - ETA: 0s - loss: 8.1894e-04 - mae: 0.0247 - mse: 8.1845e-04
86/86 [==============================] - 3s 37ms/step - loss: 8.1994e-04 - mae: 0.0247 - mse: 8.1945e-04 - val_loss: 8.6571e-04 - val_mae: 0.0257 - val_mse: 8.6523e-04 - lr: 3.9062e-05
Epoch 29/100
79/86 [==========================>...] - ETA: 0s - loss: 8.2049e-04 - mae: 0.0248 - mse: 8.2001e-04
Epoch 29: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.
86/86 [==============================] - 1s 8ms/step - loss: 8.1976e-04 - mae: 0.0247 - mse: 8.1927e-04 - val_loss: 8.6572e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 3.9062e-05
Epoch 30/100
86/86 [==============================] - ETA: 0s - loss: 8.1934e-04 - mae: 0.0247 - mse: 8.1926e-04
86/86 [==============================] - 3s 40ms/step - loss: 8.1934e-04 - mae: 0.0247 - mse: 8.1926e-04 - val_loss: 8.6540e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.9531e-05
Epoch 31/100
84/86 [============================>.] - ETA: 0s - loss: 8.1717e-04 - mae: 0.0247 - mse: 8.1706e-04
86/86 [==============================] - 3s 41ms/step - loss: 8.1940e-04 - mae: 0.0247 - mse: 8.1928e-04 - val_loss: 8.6538e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.9531e-05
Epoch 32/100
78/86 [==========================>...] - ETA: 0s - loss: 8.2470e-04 - mae: 0.0248 - mse: 8.2458e-04
Epoch 32: ReduceLROnPlateau reducing learning rate to 1e-05.
86/86 [==============================] - 1s 10ms/step - loss: 8.1942e-04 - mae: 0.0247 - mse: 8.1931e-04 - val_loss: 8.6538e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.9531e-05
Epoch 33/100

81/86 [===========================>..] - ETA: 0s - loss: 8.2423e-04 - mae: 0.0248 - mse: 8.2421e-04
86/86 [==============================] - 3s 39ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1926e-04 - val_loss: 8.6530e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-05
Epoch 34/100
84/86 [============================>.] - ETA: 0s - loss: 8.1904e-04 - mae: 0.0247 - mse: 8.1901e-04
79/86 [==========================>...] - ETA: 0s - loss: 8.2554e-04 - mae: 0.0248 - mse: 8.2551e-044e-04 - val_loss: 8.6530e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-05
Epoch 35/100
79/86 [==========================>...] - ETA: 0s - loss: 8.2554e-04 - mae: 0.0248 - mse: 8.2551e-044e-04 - val_loss: 8.6530e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-05
86/86 [==============================] - 1s 8ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-055
Epoch 36/100
86/86 [==============================] - 1s 8ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-055
Epoch 37/100
84/86 [============================>.] - ETA: 0s - loss: 8.1757e-04 - mae: 0.0247 - mse: 8.1754e-04
86/86 [==============================] - 3s 36ms/step - loss: 8.1930e-04 - mae: 0.0247 - mse: 8.1927e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 38/100
80/86 [==========================>...] - ETA: 0s - loss: 8.2227e-04 - mae: 0.0248 - mse: 8.2224e-04
86/86 [==============================] - 3s 36ms/step - loss: 8.1930e-04 - mae: 0.0247 - mse: 8.1927e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
86/86 [==============================] - 1s 8ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-055
Epoch 40/100
80/86 [==========================>...] - ETA: 0s - loss: 8.1901e-04 - mae: 0.0247 - mse: 8.1898e-04
86/86 [==============================] - 1s 8ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-055
77/86 [=========================>....] - ETA: 0s - loss: 8.2056e-04 - mae: 0.0248 - mse: 8.2053e-045e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 42/100
86/86 [==============================] - 1s 10ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 43/100
77/86 [=========================>....] - ETA: 0s - loss: 8.2056e-04 - mae: 0.0248 - mse: 8.2053e-045e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
86/86 [==============================] - 1s 10ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6527e-04 - val_mae: 0.0257 - val_mse: 8.6524e-04 - lr: 1.0000e-05
Epoch 44/100
86/86 [==============================] - 1s 10ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
Epoch 45/100
86/86 [==============================] - 1s 10ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6527e-04 - val_mae: 0.0257 - val_mse: 8.6524e-04 - lr: 1.0000e-05
Epoch 46/100
86/86 [==============================] - 1s 9ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 47/100
86/86 [==============================] - 1s 9ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 48/100
86/86 [==============================] - 1s 9ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 49/100
86/86 [==============================] - 1s 9ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 50/100
86/86 [==============================] - 1s 9ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
Epoch 51/100
54/86 [=================>............] - ETA: 0s - loss: 8.1059e-04 - mae: 0.0246 - mse: 8.1056e-04
86/86 [==============================] - 1s 9ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6531e-04 - val_mae: 0.0257 - val_mse: 8.6528e-04 - lr: 1.0000e-05
Epoch 54/100
 9/86 [==>...........................] - ETA: 0s - loss: 9.7504e-04 - mae: 0.0280 - mse: 9.7502e-04
86/86 [==============================] - 1s 9ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6531e-04 - val_mae: 0.0257 - val_mse: 8.6528e-04 - lr: 1.0000e-05
86/86 [==============================] - 1s 10ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6527e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
86/86 [==============================] - 1s 10ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6527e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
84/86 [============================>.] - ETA: 0s - loss: 8.1886e-04 - mae: 0.0247 - mse: 8.1882e-043e-04 - val_loss: 8.6527e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-05
86/86 [==============================] - 1s 9ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-055
85/86 [============================>.] - ETA: 0s - loss: 8.1890e-04 - mae: 0.0247 - mse: 8.1887e-04e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 10ms/step - loss: 8.1924e-04 - mae: 0.0247 - mse: 8.1921e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-05
86/86 [==============================] - 1s 9ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 9ms/step - loss: 8.1928e-04 - mae: 0.0247 - mse: 8.1925e-04 - val_loss: 8.6528e-04 - val_mae: 0.0257 - val_mse: 8.6525e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 9ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6530e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 9ms/step - loss: 8.1926e-04 - mae: 0.0247 - mse: 8.1923e-04 - val_loss: 8.6530e-04 - val_mae: 0.0257 - val_mse: 8.6527e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 9ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 9ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 9ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 9ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 9ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6529e-04 - val_mae: 0.0257 - val_mse: 8.6526e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 8ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6527e-04 - val_mae: 0.0257 - val_mse: 8.6524e-04 - lr: 1.0000e-055
86/86 [==============================] - 1s 8ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6527e-04 - val_mae: 0.0257 - val_mse: 8.6524e-04 - lr: 1.0000e-055
wandb: Adding directory to artifact (C:\MyDocuments\Dizertatie\segments\wandb_visualization\wandb\run-20230406_165629-99qiq874\files\model-best)... Done. 0.0s
86/86 [==============================] - 1s 8ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6527e-04 - val_mae: 0.0257 - val_mse: 8.6524e-04 - lr: 1.0000e-055

wandb: Adding directory to artifact (C:\MyDocuments\Dizertatie\segments\wandb_visualization\wandb\run-20230406_165629-99qiq874\files\model-best)... Done. 0.0s
86/86 [==============================] - 1s 8ms/step - loss: 8.1927e-04 - mae: 0.0247 - mse: 8.1924e-04 - val_loss: 8.6527e-04 - val_mae: 0.0257 - val_mse: 8.6524e-04 - lr: 1.0000e-055