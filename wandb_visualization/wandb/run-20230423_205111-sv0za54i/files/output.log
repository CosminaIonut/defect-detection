Epoch 1/150
108/122 [=========================>....] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
122/122 [==============================] - 5s 39ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 2/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 3/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 4/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 5/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 6/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 7/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 8/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 9/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 10/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 11/150
111/122 [==========================>...] - ETA: 0s - loss: 0.8584 - mae: 0.9254 - mse: 0.8584
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.04519646242260933.
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0904
Epoch 12/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0452
Epoch 13/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0452
Epoch 14/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0452
Epoch 15/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0452
Epoch 16/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0452
Epoch 17/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0452
Epoch 18/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0452
Epoch 19/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0452
Epoch 20/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0452
Epoch 21/150
122/122 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.022598231211304665.
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0452
Epoch 22/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0226
Epoch 23/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0226
Epoch 24/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0226
Epoch 25/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0226
Epoch 26/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0226
Epoch 27/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0226
Epoch 28/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0226
Epoch 29/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0226
Epoch 30/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0226
Epoch 31/150
104/122 [========================>.....] - ETA: 0s - loss: 0.8568 - mae: 0.9246 - mse: 0.8568
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.011299115605652332.
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0226
Epoch 32/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0113
Epoch 33/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0113
Epoch 34/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0113
Epoch 35/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0113
Epoch 36/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0113
Epoch 37/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0113
Epoch 38/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0113
Epoch 39/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0113
Epoch 40/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0113
Epoch 41/150
 93/122 [=====================>........] - ETA: 0s - loss: 0.8568 - mae: 0.9246 - mse: 0.8568
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005649557802826166.
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0113
Epoch 42/150
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 43/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 44/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 45/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 46/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 47/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 48/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 49/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 50/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 51/150
117/122 [===========================>..] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002824778901413083.
122/122 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0056
Epoch 52/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 53/150
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 54/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 55/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 56/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 57/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 58/150
 41/122 [=========>....................] - ETA: 0s - loss: 0.8571 - mae: 0.9248 - mse: 0.8571
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 60/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 61/150
107/122 [=========================>....] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0014123894507065415.
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0028
Epoch 62/150
 91/122 [=====================>........] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 65/150
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 66/150
122/122 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 67/150
  1/122 [..............................] - ETA: 0s - loss: 0.8529 - mae: 0.9227 - mse: 0.8529
116/122 [===========================>..] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.85828579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
 68/122 [===============>..............] - ETA: 0s - loss: 0.8584 - mae: 0.9254 - mse: 0.85848579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.0619e-04
110/122 [==========================>...] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.0619e-04
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5310e-04
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5310e-04
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7655e-04
101/122 [=======================>......] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.85748579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7655e-04
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.8274e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.8274e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.4137e-05
117/122 [===========================>..] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.85818579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.4137e-05
106/122 [=========================>....] - ETA: 0s - loss: 0.8584 - mae: 0.9255 - mse: 0.85848579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.4137e-05
 24/122 [====>.........................] - ETA: 0s - loss: 0.8619 - mae: 0.9274 - mse: 0.86198579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2069e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.2069e-05
122/122 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1034e-05
121/122 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1034e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
158/162 [============================>.] - ETA: 0s - loss: 0.6429 - mae: 0.7997 - mse: 0.6429  79 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
158/162 [============================>.] - ETA: 0s - loss: 0.6429 - mae: 0.7997 - mse: 0.6429  79 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 2s 9ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0904e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0904e-05
143/162 [=========================>....] - ETA: 0s - loss: 0.6432 - mae: 0.7999 - mse: 0.64326431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0904e-05
162/162 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0452e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0452e-05
152/162 [===========================>..] - ETA: 0s - loss: 0.6429 - mae: 0.7997 - mse: 0.64296431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0452e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0226e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0226e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 40/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 42/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 47/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 50/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 54/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 57/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 61/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 65/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 68/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 71/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 75/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 78/150===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0003530973626766354. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 85/150duceLROnPlateau reducing learning rate to 0.0003530973626766354. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 89/150duceLROnPlateau reducing learning rate to 0.0003530973626766354. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0001765486813383177. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 96/150duceLROnPlateau reducing learning rate to 0.0001765486813383177. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 99/150duceLROnPlateau reducing learning rate to 0.0001765486813383177. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 101: ReduceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 106/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 109/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 112/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 117/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 121/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 124/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 128/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 131/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 136/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 138/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 141: ReduceLROnPlateau reducing learning rate to 1e-05.34066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
Epoch 147/150duceLROnPlateau reducing learning rate to 1e-05.34066915885e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0113e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 3/150rained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 9/150rained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 14/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 20/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 25/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 29/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 32/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 38/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 42/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 47/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002824778901413083.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 56/150duceLROnPlateau reducing learning rate to 0.002824778901413083.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 61/150duceLROnPlateau reducing learning rate to 0.002824778901413083.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 65/150duceLROnPlateau reducing learning rate to 0.002824778901413083.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 69/150duceLROnPlateau reducing learning rate to 0.002824778901413083.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 74/150duceLROnPlateau reducing learning rate to 0.002824778901413083.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 78/150duceLROnPlateau reducing learning rate to 0.002824778901413083.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 82/150duceLROnPlateau reducing learning rate to 0.002824778901413083.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 87/150duceLROnPlateau reducing learning rate to 0.002824778901413083.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0001765486813383177.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 97/150duceLROnPlateau reducing learning rate to 0.0001765486813383177.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 102/150uceLROnPlateau reducing learning rate to 0.0001765486813383177.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 108/150uceLROnPlateau reducing learning rate to 0.0001765486813383177.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 111: ReduceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 111: ReduceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 127/150duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 132/150duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 132/150duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 138/150duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 143/150duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 149/150duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 149/150duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 149/150duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/15050duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 9/15050duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 14/1500duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 19/1500duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 24/1500duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 28/1500duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 32/1500duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 38/1500duceLROnPlateau reducing learning rate to 2.206858516728971e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 47/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 50/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 55/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 60/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 64/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 69/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 72/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 78/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 82/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 86/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 90/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 95/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 99/150duceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 104/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 109/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 114/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 118/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 122/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 126/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 131/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 136/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 141/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 145/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 150/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 150/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0004s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0004s). Check your callbacks.
Epoch 150/150uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/15050uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 14/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 14/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 20/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 25/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 30/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 35/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 40/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 45/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 49/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 54/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 58/1500uceLROnPlateau reducing learning rate to 0.005649557802826166.5.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 67/150duceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 71/150duceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 76/150duceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 80/150duceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 84/150duceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 89/150duceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 94/150duceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 99/150duceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 102/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 108/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 112/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 116/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 121/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 126/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 130/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 135/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 139/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 144/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 148/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 148/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 148/150uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/15050uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 14/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 14/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 20/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 25/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 30/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 35/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 39/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 44/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 48/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 52/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 56/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 61/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 65/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
Epoch 70/1500uceLROnPlateau reducing learning rate to 0.0014123894507065415..285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_2.h5val_mse: 0.6442 - lr: 0.0113e-05
122/122 [==============================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 79/150===========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 84/150===========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 88/150===========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 92/150===========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 98/150===========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 101/150==========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 106/150==========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 111/150==========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 115/150==========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 120/150==========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 124/150==========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 129/150==========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 132/150==========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 138/150==========================] - 0s 4ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 141: ReduceLROnPlateau reducing learning rate to 1e-05.: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
Epoch 146/150duceLROnPlateau reducing learning rate to 1e-05.: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 7.0619e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/150rained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 9/150rained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 14/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 19/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 24/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 29/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.011299115605652332.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 37/150duceLROnPlateau reducing learning rate to 0.011299115605652332.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 41/150duceLROnPlateau reducing learning rate to 0.011299115605652332.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 46/150duceLROnPlateau reducing learning rate to 0.011299115605652332.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 51/150duceLROnPlateau reducing learning rate to 0.011299115605652332.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 55/150duceLROnPlateau reducing learning rate to 0.011299115605652332.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 61/150duceLROnPlateau reducing learning rate to 0.011299115605652332.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 77/150duceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 77/150duceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 82/150duceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 88/150duceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0001765486813383177.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 97/150duceLROnPlateau reducing learning rate to 0.0001765486813383177.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 101: ReduceLROnPlateau reducing learning rate to 8.827434066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 107/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 111/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 116/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 121/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 124/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 129/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 132/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 138/150duceLROnPlateau reducing learning rate to 8.827434066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 141: ReduceLROnPlateau reducing learning rate to 1e-05.34066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 147/150duceLROnPlateau reducing learning rate to 1e-05.34066915885e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_6.h5val_mse: 0.1423 - lr: 7.0619e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 3/150rained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 9/150rained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 14/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 18/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 22/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 27/150ained_models/models_segments_overlap-cnn_rmsprop_0.09039292771429285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.011299115605652332.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 37/150duceLROnPlateau reducing learning rate to 0.011299115605652332.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005649557802826166.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 46/150duceLROnPlateau reducing learning rate to 0.005649557802826166.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 51/150duceLROnPlateau reducing learning rate to 0.005649557802826166.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 54/150duceLROnPlateau reducing learning rate to 0.005649557802826166.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 59/150duceLROnPlateau reducing learning rate to 0.005649557802826166.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 64/150duceLROnPlateau reducing learning rate to 0.005649557802826166.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 68/150duceLROnPlateau reducing learning rate to 0.005649557802826166.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 72/150duceLROnPlateau reducing learning rate to 0.005649557802826166.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 77/150duceLROnPlateau reducing learning rate to 0.005649557802826166.29285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0003530973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 98/150duceLROnPlateau reducing learning rate to 0.0003530973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 103/150uceLROnPlateau reducing learning rate to 0.0003530973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 109/150uceLROnPlateau reducing learning rate to 0.0003530973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 114/150uceLROnPlateau reducing learning rate to 0.0003530973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 120/150uceLROnPlateau reducing learning rate to 0.0003530973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 125/150uceLROnPlateau reducing learning rate to 0.0003530973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 125/150uceLROnPlateau reducing learning rate to 0.0003530973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 131/150uceLROnPlateau reducing learning rate to 0.0003530973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 136/150uceLROnPlateau reducing learning rate to 0.0003530973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 141: ReduceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 147/150duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 147/150duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 147/150duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 147/150duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/15050duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 14/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 14/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 20/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 25/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 30/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 35/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 39/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 44/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 49/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 54/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 58/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 62/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 66/1500duceLROnPlateau reducing learning rate to 1e-05.30973626766354.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 76/150duceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 81/150duceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 85/150duceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 90/150duceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 94/150duceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 99/150duceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 104/150uceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 107/150uceLROnPlateau reducing learning rate to 0.0007061947253532708.9285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 111: ReduceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 116/150duceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 121/150duceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 125/150duceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 130/150duceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 135/150duceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 139/150duceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 142/150duceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 147/150duceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 147/150duceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04
Epoch 147/150duceLROnPlateau reducing learning rate to 4.413717033457942e-05.285LR_[20]CHN_64CNNI_32BS_10P_val_lossM_150epochs/model_7.h5val_mse: 0.1423 - lr: 7.0619e-04