(3883, 8)
(1665, 8)
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
wandb: WARNING The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.
Epoch 1/30

27/27 [==============================] - ETA: 0s - loss: 0.0433 - mae: 0.1245 - mse: 0.0433 - root_mean_squared_error: 0.2080
27/27 [==============================] - 18s 398ms/step - loss: 0.0433 - mae: 0.1245 - mse: 0.0433 - root_mean_squared_error: 0.2080 - val_loss: 0.0109 - val_mae: 0.0893 - val_mse: 0.0109 - val_root_mean_squared_error: 0.1046 - lr: 0.0494
Epoch 2/30
27/27 [==============================] - 2s 89ms/step - loss: 0.0105 - mae: 0.0836 - mse: 0.0105 - root_mean_squared_error: 0.1023 - val_loss: 0.0112 - val_mae: 0.0857 - val_mse: 0.0112 - val_root_mean_squared_error: 0.1061 - lr: 0.0494
Epoch 3/30
27/27 [==============================] - ETA: 0s - loss: 0.0075 - mae: 0.0720 - mse: 0.0075 - root_mean_squared_error: 0.0867
27/27 [==============================] - 9s 346ms/step - loss: 0.0075 - mae: 0.0720 - mse: 0.0075 - root_mean_squared_error: 0.0867 - val_loss: 0.0046 - val_mae: 0.0553 - val_mse: 0.0046 - val_root_mean_squared_error: 0.0675 - lr: 0.0494
Epoch 4/30
27/27 [==============================] - ETA: 0s - loss: 0.0038 - mae: 0.0498 - mse: 0.0038 - root_mean_squared_error: 0.0613
27/27 [==============================] - 9s 342ms/step - loss: 0.0038 - mae: 0.0498 - mse: 0.0038 - root_mean_squared_error: 0.0613 - val_loss: 0.0033 - val_mae: 0.0474 - val_mse: 0.0033 - val_root_mean_squared_error: 0.0578 - lr: 0.0494
Epoch 5/30
27/27 [==============================] - ETA: 0s - loss: 0.0025 - mae: 0.0419 - mse: 0.0025 - root_mean_squared_error: 0.0501
27/27 [==============================] - 10s 368ms/step - loss: 0.0025 - mae: 0.0419 - mse: 0.0025 - root_mean_squared_error: 0.0501 - val_loss: 0.0021 - val_mae: 0.0395 - val_mse: 0.0021 - val_root_mean_squared_error: 0.0462 - lr: 0.0494
Epoch 6/30

27/27 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0423 - mse: 0.0026 - root_mean_squared_error: 0.0510
27/27 [==============================] - 9s 329ms/step - loss: 0.0026 - mae: 0.0423 - mse: 0.0026 - root_mean_squared_error: 0.0510 - val_loss: 0.0021 - val_mae: 0.0391 - val_mse: 0.0021 - val_root_mean_squared_error: 0.0456 - lr: 0.0494
Epoch 7/30
27/27 [==============================] - 2s 77ms/step - loss: 0.0023 - mae: 0.0407 - mse: 0.0023 - root_mean_squared_error: 0.0483 - val_loss: 0.0035 - val_mae: 0.0483 - val_mse: 0.0035 - val_root_mean_squared_error: 0.0591 - lr: 0.0494
Epoch 8/30
27/27 [==============================] - 2s 72ms/step - loss: 0.0026 - mae: 0.0427 - mse: 0.0026 - root_mean_squared_error: 0.0512 - val_loss: 0.0028 - val_mae: 0.0437 - val_mse: 0.0028 - val_root_mean_squared_error: 0.0527 - lr: 0.0494
Epoch 9/30

27/27 [==============================] - ETA: 0s - loss: 0.0024 - mae: 0.0406 - mse: 0.0024 - root_mean_squared_error: 0.0485
27/27 [==============================] - 13s 507ms/step - loss: 0.0024 - mae: 0.0406 - mse: 0.0024 - root_mean_squared_error: 0.0485 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0444 - lr: 0.0494
Epoch 10/30
27/27 [==============================] - 3s 103ms/step - loss: 0.0026 - mae: 0.0422 - mse: 0.0026 - root_mean_squared_error: 0.0509 - val_loss: 0.0023 - val_mae: 0.0404 - val_mse: 0.0023 - val_root_mean_squared_error: 0.0475 - lr: 0.0494
Epoch 11/30
27/27 [==============================] - 3s 102ms/step - loss: 0.0024 - mae: 0.0412 - mse: 0.0024 - root_mean_squared_error: 0.0495 - val_loss: 0.0027 - val_mae: 0.0430 - val_mse: 0.0027 - val_root_mean_squared_error: 0.0516 - lr: 0.0494
Epoch 12/30
27/27 [==============================] - 3s 102ms/step - loss: 0.0025 - mae: 0.0418 - mse: 0.0025 - root_mean_squared_error: 0.0500 - val_loss: 0.0020 - val_mae: 0.0388 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0450 - lr: 0.0494
Epoch 13/30
27/27 [==============================] - 3s 104ms/step - loss: 0.0026 - mae: 0.0421 - mse: 0.0026 - root_mean_squared_error: 0.0505 - val_loss: 0.0020 - val_mae: 0.0385 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0444 - lr: 0.0494
Epoch 14/30
27/27 [==============================] - ETA: 0s - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0504
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.024719372391700745.
27/27 [==============================] - 3s 95ms/step - loss: 0.0025 - mae: 0.0420 - mse: 0.0025 - root_mean_squared_error: 0.0504 - val_loss: 0.0024 - val_mae: 0.0409 - val_mse: 0.0024 - val_root_mean_squared_error: 0.0486 - lr: 0.0494
Epoch 15/30
27/27 [==============================] - 2s 83ms/step - loss: 0.0020 - mae: 0.0389 - mse: 0.0020 - root_mean_squared_error: 0.0453 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0444 - lr: 0.0247
Epoch 16/30

27/27 [==============================] - 3s 98ms/step - loss: 0.0021 - mae: 0.0388 - mse: 0.0021 - root_mean_squared_error: 0.0453 - val_loss: 0.0020 - val_mae: 0.0387 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0448 - lr: 0.0247
Epoch 17/30
27/27 [==============================] - 3s 98ms/step - loss: 0.0021 - mae: 0.0392 - mse: 0.0021 - root_mean_squared_error: 0.0457 - val_loss: 0.0020 - val_mae: 0.0386 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0446 - lr: 0.0247
Epoch 18/30

27/27 [==============================] - ETA: 0s - loss: 0.0020 - mae: 0.0386 - mse: 0.0020 - root_mean_squared_error: 0.0450
27/27 [==============================] - 9s 360ms/step - loss: 0.0020 - mae: 0.0386 - mse: 0.0020 - root_mean_squared_error: 0.0450 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0442 - lr: 0.0247
Epoch 19/30
27/27 [==============================] - ETA: 0s - loss: 0.0021 - mae: 0.0389 - mse: 0.0021 - root_mean_squared_error: 0.0453
Epoch 19: ReduceLROnPlateau reducing learning rate to 0.012359686195850372.
27/27 [==============================] - 3s 95ms/step - loss: 0.0021 - mae: 0.0389 - mse: 0.0021 - root_mean_squared_error: 0.0453 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0442 - lr: 0.0247
Epoch 20/30
27/27 [==============================] - 3s 101ms/step - loss: 0.0020 - mae: 0.0381 - mse: 0.0020 - root_mean_squared_error: 0.0442 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0442 - lr: 0.0124
Epoch 21/30

27/27 [==============================] - 3s 100ms/step - loss: 0.0020 - mae: 0.0382 - mse: 0.0020 - root_mean_squared_error: 0.0443 - val_loss: 0.0021 - val_mae: 0.0392 - val_mse: 0.0021 - val_root_mean_squared_error: 0.0456 - lr: 0.0124
Epoch 22/30
27/27 [==============================] - 3s 100ms/step - loss: 0.0020 - mae: 0.0385 - mse: 0.0020 - root_mean_squared_error: 0.0447 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0442 - lr: 0.0124
Epoch 23/30
27/27 [==============================] - 3s 100ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - root_mean_squared_error: 0.0441 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0442 - lr: 0.0124
Epoch 24/30

27/27 [==============================] - ETA: 0s - loss: 0.0020 - mae: 0.0382 - mse: 0.0020 - root_mean_squared_error: 0.0443
Epoch 24: ReduceLROnPlateau reducing learning rate to 0.006179843097925186.
27/27 [==============================] - 3s 100ms/step - loss: 0.0020 - mae: 0.0382 - mse: 0.0020 - root_mean_squared_error: 0.0443 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0442 - lr: 0.0124
Epoch 25/30
27/27 [==============================] - 3s 101ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - root_mean_squared_error: 0.0440 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0443 - lr: 0.0062
Epoch 26/30
27/27 [==============================] - 3s 99ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - root_mean_squared_error: 0.0440 - val_loss: 0.0020 - val_mae: 0.0386 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0448 - lr: 0.0062
Epoch 27/30


27/27 [==============================] - ETA: 0s - loss: 0.0019 - mae: 0.0381 - mse: 0.0019 - root_mean_squared_error: 0.0440
27/27 [==============================] - 9s 347ms/step - loss: 0.0019 - mae: 0.0381 - mse: 0.0019 - root_mean_squared_error: 0.0440 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0442 - lr: 0.0062
Epoch 28/30
27/27 [==============================] - 3s 97ms/step - loss: 0.0019 - mae: 0.0381 - mse: 0.0019 - root_mean_squared_error: 0.0441 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0444 - lr: 0.0062
Epoch 29/30
27/27 [==============================] - ETA: 0s - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - root_mean_squared_error: 0.0439
Epoch 29: ReduceLROnPlateau reducing learning rate to 0.003089921548962593.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
27/27 [==============================] - 9s 347ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - root_mean_squared_error: 0.0439 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0442 - lr: 0.0062
Epoch 30/30
27/27 [==============================] - 3s 93ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - root_mean_squared_error: 0.0439 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0443 - lr: 0.0031
>Saved ../trained_models/RNN/models_segments_overlap-rnn_rmsprop_0.04943874456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_1.h5
dict_keys(['loss', 'mae', 'mse', 'root_mean_squared_error', 'val_loss', 'val_mae', 'val_mse', 'val_root_mean_squared_error', 'lr'])
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
(5161, 8)
(2212, 8)
Epoch 1/30

36/36 [==============================] - 8s 123ms/step - loss: 0.0429 - mae: 0.1318 - mse: 0.0429 - root_mean_squared_error: 0.2071 - val_loss: 0.0080 - val_mae: 0.0735 - val_mse: 0.0080 - val_root_mean_squared_error: 0.0896 - lr: 0.0494
Epoch 2/30

36/36 [==============================] - 3s 96ms/step - loss: 0.0099 - mae: 0.0815 - mse: 0.0099 - root_mean_squared_error: 0.0995 - val_loss: 0.0119 - val_mae: 0.0914 - val_mse: 0.0119 - val_root_mean_squared_error: 0.1090 - lr: 0.0494
Epoch 3/30
36/36 [==============================] - 3s 95ms/step - loss: 0.0068 - mae: 0.0671 - mse: 0.0068 - root_mean_squared_error: 0.0824 - val_loss: 0.0039 - val_mae: 0.0529 - val_mse: 0.0039 - val_root_mean_squared_error: 0.0622 - lr: 0.0494
Epoch 4/30

36/36 [==============================] - 3s 70ms/step - loss: 0.0046 - mae: 0.0564 - mse: 0.0046 - root_mean_squared_error: 0.0680 - val_loss: 0.0042 - val_mae: 0.0547 - val_mse: 0.0042 - val_root_mean_squared_error: 0.0651 - lr: 0.0494
Epoch 5/30
36/36 [==============================] - 2s 45ms/step - loss: 0.0042 - mae: 0.0544 - mse: 0.0042 - root_mean_squared_error: 0.0645 - val_loss: 0.0039 - val_mae: 0.0528 - val_mse: 0.0039 - val_root_mean_squared_error: 0.0621 - lr: 0.0494
Epoch 6/30
36/36 [==============================] - 1s 39ms/step - loss: 0.0040 - mae: 0.0535 - mse: 0.0040 - root_mean_squared_error: 0.0630 - val_loss: 0.0037 - val_mae: 0.0522 - val_mse: 0.0037 - val_root_mean_squared_error: 0.0611 - lr: 0.0494
Epoch 7/30
36/36 [==============================] - 1s 37ms/step - loss: 0.0040 - mae: 0.0538 - mse: 0.0040 - root_mean_squared_error: 0.0633 - val_loss: 0.0053 - val_mae: 0.0598 - val_mse: 0.0053 - val_root_mean_squared_error: 0.0726 - lr: 0.0494
Epoch 8/30
36/36 [==============================] - 1s 41ms/step - loss: 0.0040 - mae: 0.0532 - mse: 0.0040 - root_mean_squared_error: 0.0629 - val_loss: 0.0043 - val_mae: 0.0553 - val_mse: 0.0043 - val_root_mean_squared_error: 0.0658 - lr: 0.0494
Epoch 9/30
36/36 [==============================] - 1s 41ms/step - loss: 0.0038 - mae: 0.0526 - mse: 0.0038 - root_mean_squared_error: 0.0619 - val_loss: 0.0039 - val_mae: 0.0527 - val_mse: 0.0039 - val_root_mean_squared_error: 0.0625 - lr: 0.0494
Epoch 10/30
36/36 [==============================] - 2s 43ms/step - loss: 0.0041 - mae: 0.0542 - mse: 0.0041 - root_mean_squared_error: 0.0640 - val_loss: 0.0035 - val_mae: 0.0507 - val_mse: 0.0035 - val_root_mean_squared_error: 0.0588 - lr: 0.0494
Epoch 11/30
36/36 [==============================] - 1s 39ms/step - loss: 0.0040 - mae: 0.0536 - mse: 0.0040 - root_mean_squared_error: 0.0636 - val_loss: 0.0035 - val_mae: 0.0511 - val_mse: 0.0035 - val_root_mean_squared_error: 0.0595 - lr: 0.0494
Epoch 12/30
36/36 [==============================] - 1s 39ms/step - loss: 0.0040 - mae: 0.0533 - mse: 0.0040 - root_mean_squared_error: 0.0630 - val_loss: 0.0045 - val_mae: 0.0559 - val_mse: 0.0045 - val_root_mean_squared_error: 0.0668 - lr: 0.0494
Epoch 13/30
36/36 [==============================] - 1s 37ms/step - loss: 0.0040 - mae: 0.0535 - mse: 0.0040 - root_mean_squared_error: 0.0631 - val_loss: 0.0039 - val_mae: 0.0529 - val_mse: 0.0039 - val_root_mean_squared_error: 0.0627 - lr: 0.0494
Epoch 14/30
35/36 [============================>.] - ETA: 0s - loss: 0.0039 - mae: 0.0531 - mse: 0.0039 - root_mean_squared_error: 0.0627.0627 - val_loss: 0.0038 - val_mae: 0.0528 - val_mse: 0.0038 - val_root_mean_squared_error: 0.0620 - lr: 0.0494
Epoch 15/30
35/36 [============================>.] - ETA: 0s - loss: 0.0039 - mae: 0.0531 - mse: 0.0039 - root_mean_squared_error: 0.0627.0627 - val_loss: 0.0038 - val_mae: 0.0528 - val_mse: 0.0038 - val_root_mean_squared_error: 0.0620 - lr: 0.0494
Epoch 15: ReduceLROnPlateau reducing learning rate to 0.024719372391700745.
36/36 [==============================] - 1s 39ms/step - loss: 0.0039 - mae: 0.0531 - mse: 0.0039 - root_mean_squared_error: 0.0626 - val_loss: 0.0034 - val_mae: 0.0504 - val_mse: 0.0034 - val_root_mean_squared_error: 0.0586 - lr: 0.0494
Epoch 16/30
36/36 [==============================] - 1s 41ms/step - loss: 0.0035 - mae: 0.0511 - mse: 0.0035 - root_mean_squared_error: 0.0591 - val_loss: 0.0038 - val_mae: 0.0525 - val_mse: 0.0038 - val_root_mean_squared_error: 0.0616 - lr: 0.0247
Epoch 17/30
19/36 [==============>...............] - ETA: 0s - loss: 0.0037 - mae: 0.0520 - mse: 0.0037 - root_mean_squared_error: 0.0605
36/36 [==============================] - 1s 41ms/step - loss: 0.0035 - mae: 0.0511 - mse: 0.0035 - root_mean_squared_error: 0.0591 - val_loss: 0.0038 - val_mae: 0.0525 - val_mse: 0.0038 - val_root_mean_squared_error: 0.0616 - lr: 0.0247
36/36 [==============================] - 1s 41ms/step - loss: 0.0035 - mae: 0.0511 - mse: 0.0035 - root_mean_squared_error: 0.0591 - val_loss: 0.0038 - val_mae: 0.0525 - val_mse: 0.0038 - val_root_mean_squared_error: 0.0616 - lr: 0.0247
36/36 [==============================] - ETA: 0s - loss: 0.0035 - mae: 0.0510 - mse: 0.0035 - root_mean_squared_error: 0.0591.0591 - val_loss: 0.0038 - val_mae: 0.0525 - val_mse: 0.0038 - val_root_mean_squared_error: 0.0616 - lr: 0.0247
13/36 [=========>....................] - ETA: 0s - loss: 0.0035 - mae: 0.0511 - mse: 0.0035 - root_mean_squared_error: 0.0590.0591 - val_loss: 0.0044 - val_mae: 0.0557 - val_mse: 0.0044 - val_root_mean_squared_error: 0.0665 - lr: 0.0247
36/36 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0511 - mse: 0.0035 - root_mean_squared_error: 0.0589 - val_loss: 0.0035 - val_mae: 0.0506 - val_mse: 0.0035 - val_root_mean_squared_error: 0.0591 - lr: 0.0124
 1/36 [..............................] - ETA: 1s - loss: 0.0037 - mae: 0.0526 - mse: 0.0037 - root_mean_squared_error: 0.0608.0589 - val_loss: 0.0035 - val_mae: 0.0506 - val_mse: 0.0035 - val_root_mean_squared_error: 0.0591 - lr: 0.0124
36/36 [==============================] - 2s 43ms/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - root_mean_squared_error: 0.0585 - val_loss: 0.0036 - val_mae: 0.0511 - val_mse: 0.0036 - val_root_mean_squared_error: 0.0598 - lr: 0.0124
36/36 [==============================] - 1s 36ms/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - root_mean_squared_error: 0.0585 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - val_root_mean_squared_error: 0.0584 - lr: 0.0062
36/36 [==============================] - 1s 36ms/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - root_mean_squared_error: 0.0585 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - val_root_mean_squared_error: 0.0584 - lr: 0.0062
22/36 [=================>............] - ETA: 0s - loss: 0.0034 - mae: 0.0509 - mse: 0.0034 - root_mean_squared_error: 0.0585.0585 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - val_root_mean_squared_error: 0.0584 - lr: 0.0062
dict_keys(['loss', 'mae', 'mse', 'root_mean_squared_error', 'val_loss', 'val_mae', 'val_mse', 'val_root_mean_squared_error', 'lr'])- val_loss: 0.0034 - val_mae: 0.0505 - val_mse: 0.0034 - val_root_mean_squared_error: 0.0585 - lr: 0.0062
dict_keys(['loss', 'mae', 'mse', 'root_mean_squared_error', 'val_loss', 'val_mae', 'val_mse', 'val_root_mean_squared_error', 'lr'])- val_loss: 0.0034 - val_mae: 0.0505 - val_mse: 0.0034 - val_root_mean_squared_error: 0.0585 - lr: 0.0062
WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
dict_keys(['loss', 'mae', 'mse', 'root_mean_squared_error', 'val_loss', 'val_mae', 'val_mse', 'val_root_mean_squared_error', 'lr'])- val_loss: 0.0034 - val_mae: 0.0505 - val_mse: 0.0034 - val_root_mean_squared_error: 0.0585 - lr: 0.0062
27/27 [==============================] - 1s 41ms/step - loss: 0.0096 - mae: 0.0825 - mse: 0.0096 - root_mean_squared_error: 0.0982 - val_loss: 0.0076 - val_mae: 0.0709 - val_mse: 0.0076 - val_root_mean_squared_error: 0.0870 - lr: 0.0494
 7/27 [======>.......................] - ETA: 0s - loss: 0.0058 - mae: 0.0631 - mse: 0.0058 - root_mean_squared_error: 0.0764.0982 - val_loss: 0.0076 - val_mae: 0.0709 - val_mse: 0.0076 - val_root_mean_squared_error: 0.0870 - lr: 0.0494
27/27 [==============================] - 1s 52ms/step - loss: 0.0049 - mae: 0.0575 - mse: 0.0049 - root_mean_squared_error: 0.0702 - val_loss: 0.0041 - val_mae: 0.0522 - val_mse: 0.0041 - val_root_mean_squared_error: 0.0639 - lr: 0.0494
15/27 [===============>..............] - ETA: 0s - loss: 0.0034 - mae: 0.0476 - mse: 0.0034 - root_mean_squared_error: 0.0581.0702 - val_loss: 0.0041 - val_mae: 0.0522 - val_mse: 0.0041 - val_root_mean_squared_error: 0.0639 - lr: 0.0494
27/27 [==============================] - 1s 39ms/step - loss: 0.0034 - mae: 0.0477 - mse: 0.0034 - root_mean_squared_error: 0.0580 - val_loss: 0.0041 - val_mae: 0.0526 - val_mse: 0.0041 - val_root_mean_squared_error: 0.0642 - lr: 0.0494
27/27 [==============================] - 1s 37ms/step - loss: 0.0029 - mae: 0.0448 - mse: 0.0029 - root_mean_squared_error: 0.0543 - val_loss: 0.0049 - val_mae: 0.0577 - val_mse: 0.0049 - val_root_mean_squared_error: 0.0701 - lr: 0.0494
27/27 [==============================] - 1s 38ms/step - loss: 0.0020 - mae: 0.0385 - mse: 0.0020 - root_mean_squared_error: 0.0451 - val_loss: 0.0020 - val_mae: 0.0386 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0447 - lr: 0.0247
15/27 [===============>..............] - ETA: 0s - loss: 0.0020 - mae: 0.0377 - mse: 0.0020 - root_mean_squared_error: 0.0443.0451 - val_loss: 0.0020 - val_mae: 0.0386 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0447 - lr: 0.0247
27/27 [==============================] - ETA: 0s - loss: 0.0020 - mae: 0.0387 - mse: 0.0020 - root_mean_squared_error: 0.0451.0451 - val_loss: 0.0020 - val_mae: 0.0386 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0447 - lr: 0.0247
27/27 [==============================] - ETA: 0s - loss: 0.0020 - mae: 0.0387 - mse: 0.0020 - root_mean_squared_error: 0.0451.0451 - val_loss: 0.0020 - val_mae: 0.0386 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0447 - lr: 0.0247

27/27 [==============================] - 1s 39ms/step - loss: 0.0020 - mae: 0.0386 - mse: 0.0020 - root_mean_squared_error: 0.0451 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0444 - lr: 0.02477
27/27 [==============================] - 1s 39ms/step - loss: 0.0020 - mae: 0.0386 - mse: 0.0020 - root_mean_squared_error: 0.0451 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0444 - lr: 0.02477
27/27 [==============================] - 1s 41ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - root_mean_squared_error: 0.0441 - val_loss: 0.0020 - val_mae: 0.0382 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0443 - lr: 0.01247
27/27 [==============================] - 1s 41ms/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - root_mean_squared_error: 0.0441 - val_loss: 0.0020 - val_mae: 0.0382 - val_mse: 0.0020 - val_root_mean_squared_error: 0.0443 - lr: 0.01247
27/27 [==============================] - 1s 39ms/step - loss: 0.0019 - mae: 0.0375 - mse: 0.0019 - root_mean_squared_error: 0.0436 - val_loss: 0.0021 - val_mae: 0.0389 - val_mse: 0.0021 - val_root_mean_squared_error: 0.0459 - lr: 0.00624
27/27 [==============================] - 1s 37ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - root_mean_squared_error: 0.0435 - val_loss: 0.0019 - val_mae: 0.0378 - val_mse: 0.0019 - val_root_mean_squared_error: 0.0439 - lr: 0.00624
27/27 [==============================] - 1s 37ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - root_mean_squared_error: 0.0435 - val_loss: 0.0019 - val_mae: 0.0378 - val_mse: 0.0019 - val_root_mean_squared_error: 0.0439 - lr: 0.00624
27/27 [==============================] - 1s 37ms/step - loss: 0.0019 - mae: 0.0372 - mse: 0.0019 - root_mean_squared_error: 0.0435 - val_loss: 0.0019 - val_mae: 0.0378 - val_mse: 0.0019 - val_root_mean_squared_error: 0.0439 - lr: 0.00624
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
 7/27 [======>.......................] - ETA: 0s - loss: 0.0083 - mae: 0.0782 - mse: 0.0083 - root_mean_squared_error: 0.0909'lr'])- val_loss: 0.0019 - val_mae: 0.0378 - val_mse: 0.0019 - val_root_mean_squared_error: 0.0439 - lr: 0.00624
 7/27 [======>.......................] - ETA: 0s - loss: 0.0083 - mae: 0.0782 - mse: 0.0083 - root_mean_squared_error: 0.0909'lr'])- val_loss: 0.0019 - val_mae: 0.0378 - val_mse: 0.0019 - val_root_mean_squared_error: 0.0439 - lr: 0.00624
27/27 [==============================] - 1s 37ms/step - loss: 0.0089 - mae: 0.0801 - mse: 0.0089 - root_mean_squared_error: 0.0941 - val_loss: 0.0102 - val_mae: 0.0907 - val_mse: 0.0102 - val_root_mean_squared_error: 0.1009 - lr: 0.04944
27/27 [==============================] - 1s 37ms/step - loss: 0.0045 - mae: 0.0546 - mse: 0.0045 - root_mean_squared_error: 0.0671 - val_loss: 0.0027 - val_mae: 0.0433 - val_mse: 0.0027 - val_root_mean_squared_error: 0.0520 - lr: 0.04944
11/27 [===========>..................] - ETA: 0s - loss: 0.0036 - mae: 0.0487 - mse: 0.0036 - root_mean_squared_error: 0.0599.0671 - val_loss: 0.0027 - val_mae: 0.0433 - val_mse: 0.0027 - val_root_mean_squared_error: 0.0520 - lr: 0.04944
27/27 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0487 - mse: 0.0035 - root_mean_squared_error: 0.0595 - val_loss: 0.0038 - val_mae: 0.0500 - val_mse: 0.0038 - val_root_mean_squared_error: 0.0615 - lr: 0.04944
27/27 [==============================] - ETA: 0s - loss: 0.0027 - mae: 0.0429 - mse: 0.0027 - root_mean_squared_error: 0.0524.0595 - val_loss: 0.0038 - val_mae: 0.0500 - val_mse: 0.0038 - val_root_mean_squared_error: 0.0615 - lr: 0.04944
27/27 [==============================] - 1s 37ms/step - loss: 0.0022 - mae: 0.0393 - mse: 0.0022 - root_mean_squared_error: 0.0464 - val_loss: 0.0028 - val_mae: 0.0429 - val_mse: 0.0028 - val_root_mean_squared_error: 0.0530 - lr: 0.02474
27/27 [==============================] - 1s 37ms/step - loss: 0.0022 - mae: 0.0393 - mse: 0.0022 - root_mean_squared_error: 0.0464 - val_loss: 0.0028 - val_mae: 0.0429 - val_mse: 0.0028 - val_root_mean_squared_error: 0.0530 - lr: 0.02474
27/27 [==============================] - 1s 37ms/step - loss: 0.0022 - mae: 0.0393 - mse: 0.0022 - root_mean_squared_error: 0.0464 - val_loss: 0.0028 - val_mae: 0.0429 - val_mse: 0.0028 - val_root_mean_squared_error: 0.0530 - lr: 0.02474
27/27 [==============================] - 1s 37ms/step - loss: 0.0022 - mae: 0.0393 - mse: 0.0022 - root_mean_squared_error: 0.0464 - val_loss: 0.0028 - val_mae: 0.0429 - val_mse: 0.0028 - val_root_mean_squared_error: 0.0530 - lr: 0.02474
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
27/27 [==============================] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0020 - root_mean_squared_error: 0.0444.0464 - val_loss: 0.0028 - val_mae: 0.0429 - val_mse: 0.0028 - val_root_mean_squared_error: 0.0530 - lr: 0.02474
27/27 [==============================] - 4s 141ms/step - loss: 0.0020 - mae: 0.0376 - mse: 0.0020 - root_mean_squared_error: 0.0444 - val_loss: 0.0018 - val_mae: 0.0360 - val_mse: 0.0018 - val_root_mean_squared_error: 0.0419 - lr: 0.0247
27/27 [==============================] - 4s 141ms/step - loss: 0.0020 - mae: 0.0376 - mse: 0.0020 - root_mean_squared_error: 0.0444 - val_loss: 0.0018 - val_mae: 0.0360 - val_mse: 0.0018 - val_root_mean_squared_error: 0.0419 - lr: 0.0247
27/27 [==============================] - 1s 38ms/step - loss: 0.0016 - mae: 0.0343 - mse: 0.0016 - root_mean_squared_error: 0.0402 - val_loss: 0.0019 - val_mae: 0.0384 - val_mse: 0.0019 - val_root_mean_squared_error: 0.0440 - lr: 0.01247
27/27 [==============================] - 1s 38ms/step - loss: 0.0016 - mae: 0.0343 - mse: 0.0016 - root_mean_squared_error: 0.0402 - val_loss: 0.0019 - val_mae: 0.0384 - val_mse: 0.0019 - val_root_mean_squared_error: 0.0440 - lr: 0.01247
27/27 [==============================] - 1s 42ms/step - loss: 0.0016 - mae: 0.0342 - mse: 0.0016 - root_mean_squared_error: 0.0402 - val_loss: 0.0016 - val_mae: 0.0335 - val_mse: 0.0016 - val_root_mean_squared_error: 0.0400 - lr: 0.01247
27/27 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0338 - mse: 0.0016 - root_mean_squared_error: 0.0398.0402 - val_loss: 0.0016 - val_mae: 0.0335 - val_mse: 0.0016 - val_root_mean_squared_error: 0.0400 - lr: 0.01247
27/27 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0338 - mse: 0.0016 - root_mean_squared_error: 0.0398.0402 - val_loss: 0.0016 - val_mae: 0.0335 - val_mse: 0.0016 - val_root_mean_squared_error: 0.0400 - lr: 0.01247
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
27/27 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0338 - mse: 0.0016 - root_mean_squared_error: 0.0398.0402 - val_loss: 0.0016 - val_mae: 0.0335 - val_mse: 0.0016 - val_root_mean_squared_error: 0.0400 - lr: 0.01247
27/27 [==============================] - ETA: 0s - loss: 0.0015 - mae: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387.0402 - val_loss: 0.0016 - val_mae: 0.0335 - val_mse: 0.0016 - val_root_mean_squared_error: 0.0400 - lr: 0.01247
27/27 [==============================] - ETA: 0s - loss: 0.0015 - mae: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387.0402 - val_loss: 0.0016 - val_mae: 0.0335 - val_mse: 0.0016 - val_root_mean_squared_error: 0.0400 - lr: 0.01247
27/27 [==============================] - 4s 146ms/step - loss: 0.0015 - mae: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
27/27 [==============================] - 4s 146ms/step - loss: 0.0015 - mae: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 3/30===========================] - 4s 146ms/step - loss: 0.0015 - mae: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 3/30===========================] - 4s 146ms/step - loss: 0.0015 - mae: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 6/30===========================] - 4s 146ms/step - loss: 0.0015 - mae: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 6/30===========================] - 4s 146ms/step - loss: 0.0015 - mae: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 10/30==========================] - 4s 146ms/step - loss: 0.0015 - mae: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
Epoch 14/30educeLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062

Epoch 16/30educeLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 16/30educeLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 20/30educeLROnPlateau reducing learning rate to 0.012359686195850372.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 20/30educeLROnPlateau reducing learning rate to 0.012359686195850372.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 20/30educeLROnPlateau reducing learning rate to 0.012359686195850372.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 25/30educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 25/30educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 27: ReduceLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 27: ReduceLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 27: ReduceLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
Epoch 27: ReduceLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
Epoch 3/30ReduceLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 3/30ReduceLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 6/30ReduceLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 9/30ReduceLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 11/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 13/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 14/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 14/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 14/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
Epoch 16/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 18/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 18/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 21/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 21/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 23/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 23/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 25/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 25/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 28/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 28/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230426_193242-xyglomri\files\model-best)... Done. 0.0s
Epoch 28/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
(3883, 8)30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 3/300educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 5/300educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 7/300educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 9/300educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 11/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 13/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 15/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 19/30educeLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 21/30educeLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 22/30educeLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 24/30educeLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 24/30educeLROnPlateau reducing learning rate to 0.024719372391700745.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 27: ReduceLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 30/30educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 30/30educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
(3883, 8)30educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
(3883, 8)30educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 4/300educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 6/300educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 7/300educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 9/300educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 11/30educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.012359686195850372.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 15/30educeLROnPlateau reducing learning rate to 0.012359686195850372.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 17: ReduceLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 20/30educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 22/30educeLROnPlateau reducing learning rate to 0.006179843097925186.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 23: ReduceLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 23: ReduceLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 27/30educeLROnPlateau reducing learning rate to 0.003089921548962593.: 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0015449607744812965. 0.0331 - mse: 0.0015 - root_mean_squared_error: 0.0387 - val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
>Saved ../trained_models/RNN/models_segments_overlap-rnn_rmsprop_0.04943874456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
>Saved ../trained_models/RNN/models_segments_overlap-rnn_rmsprop_0.04943874456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
>Saved ../trained_models/RNN/models_segments_overlap-rnn_rmsprop_0.04943874456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 4/30trained_models/RNN/models_segments_overlap-rnn_rmsprop_0.04943874456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 6/30trained_models/RNN/models_segments_overlap-rnn_rmsprop_0.04943874456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 7/30trained_models/RNN/models_segments_overlap-rnn_rmsprop_0.04943874456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 10/30rained_models/RNN/models_segments_overlap-rnn_rmsprop_0.04943874456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.012359686195850372.456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 14/30educeLROnPlateau reducing learning rate to 0.012359686195850372.456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 16/30educeLROnPlateau reducing learning rate to 0.012359686195850372.456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 18/30educeLROnPlateau reducing learning rate to 0.012359686195850372.456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 20/30educeLROnPlateau reducing learning rate to 0.012359686195850372.456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 20/30educeLROnPlateau reducing learning rate to 0.012359686195850372.456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 23/30educeLROnPlateau reducing learning rate to 0.012359686195850372.456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 25/30educeLROnPlateau reducing learning rate to 0.012359686195850372.456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 26/30educeLROnPlateau reducing learning rate to 0.012359686195850372.456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
Epoch 29/30educeLROnPlateau reducing learning rate to 0.012359686195850372.456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_8.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
>Saved ../trained_models/RNN/models_segments_overlap-rnn_rmsprop_0.04943874456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_9.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062
>Saved ../trained_models/RNN/models_segments_overlap-rnn_rmsprop_0.04943874456393668LR_[20]HL4DU_144BS_5P_val_mseM_30epochs/model_9.h5val_loss: 0.0015 - val_mae: 0.0334 - val_mse: 0.0015 - val_root_mean_squared_error: 0.0391 - lr: 0.0062