Epoch 1/200
 1/18 [>.............................] - ETA: 11s - loss: 0.6509 - mae: 0.4485 - mse: 0.2032
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.
18/18 [==============================] - 2s 76ms/step - loss: 0.1276 - mae: 0.0840 - mse: 0.0205 - val_loss: 0.0399 - val_mae: 0.0363 - val_mse: 0.0020 - lr: 0.0970
Epoch 2/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0399 - mae: 0.0367 - mse: 0.0021
18/18 [==============================] - 1s 62ms/step - loss: 0.0158 - mae: 0.0383 - mse: 0.0020 - val_loss: 0.0074 - val_mae: 0.0386 - val_mse: 0.0020 - lr: 0.0970
Epoch 3/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0047 - mae: 0.0393 - mse: 0.0022 - val_loss: 0.0083 - val_mae: 0.0465 - val_mse: 0.0041 - lr: 0.0970
Epoch 4/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0166 - mae: 0.0500 - mse: 0.0046 - val_loss: 0.0106 - val_mae: 0.0472 - val_mse: 0.0042 - lr: 0.0970
Epoch 5/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0105 - mae: 0.0471 - mse: 0.0041
18/18 [==============================] - 1s 65ms/step - loss: 0.0059 - mae: 0.0421 - mse: 0.0030 - val_loss: 0.0030 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0970
Epoch 6/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0029 - mae: 0.0376 - mse: 0.0019
18/18 [==============================] - 1s 66ms/step - loss: 0.0026 - mae: 0.0376 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0365 - val_mse: 0.0020 - lr: 0.0970
Epoch 7/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0050 - mae: 0.0430 - mse: 0.0032 - val_loss: 0.0028 - val_mae: 0.0374 - val_mse: 0.0020 - lr: 0.0970
Epoch 8/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0049 - mae: 0.0426 - mse: 0.0031 - val_loss: 0.0047 - val_mae: 0.0376 - val_mse: 0.0026 - lr: 0.0970
Epoch 9/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0053 - mae: 0.0436 - mse: 0.0034 - val_loss: 0.0064 - val_mae: 0.0454 - val_mse: 0.0040 - lr: 0.0970
Epoch 10/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0038 - mae: 0.0400 - mse: 0.0026 - val_loss: 0.0033 - val_mae: 0.0369 - val_mse: 0.0024 - lr: 0.0970
Epoch 11/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0044 - mae: 0.0419 - mse: 0.0031 - val_loss: 0.0026 - val_mae: 0.0391 - val_mse: 0.0020 - lr: 0.0970
Epoch 12/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0025 - mae: 0.0385 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
18/18 [==============================] - 1s 68ms/step - loss: 0.0029 - mae: 0.0380 - mse: 0.0021 - val_loss: 0.0024 - val_mae: 0.0376 - val_mse: 0.0019 - lr: 0.0970
Epoch 13/200
18/18 [==============================] - 1s 60ms/step - loss: 0.0022 - mae: 0.0371 - mse: 0.0019 - val_loss: 0.0022 - val_mae: 0.0364 - val_mse: 0.0020 - lr: 0.0970
Epoch 14/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0021 - mae: 0.0372 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0360 - val_mse: 0.0022 - lr: 0.0970
Epoch 15/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0039 - mae: 0.0408 - mse: 0.0027 - val_loss: 0.0118 - val_mae: 0.0472 - val_mse: 0.0042 - lr: 0.0970
Epoch 16/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0075 - mae: 0.0459 - mse: 0.0038 - val_loss: 0.0054 - val_mae: 0.0432 - val_mse: 0.0037 - lr: 0.0970
Epoch 17/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0035 - mae: 0.0380 - mse: 0.0022 - val_loss: 0.0027 - val_mae: 0.0381 - val_mse: 0.0019 - lr: 0.0970
Epoch 18/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0023 - mae: 0.0372 - mse: 0.0019 - val_loss: 0.0022 - val_mae: 0.0359 - val_mse: 0.0020 - lr: 0.0970
Epoch 19/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0021 - mae: 0.0346 - mse: 0.0018
18/18 [==============================] - 1s 64ms/step - loss: 0.0022 - mae: 0.0369 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0370 - val_mse: 0.0019 - lr: 0.0970
Epoch 20/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0038 - mae: 0.0411 - mse: 0.0028 - val_loss: 0.0058 - val_mae: 0.0469 - val_mse: 0.0042 - lr: 0.0970
Epoch 21/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0060 - mae: 0.0459 - mse: 0.0038 - val_loss: 0.0039 - val_mae: 0.0403 - val_mse: 0.0033 - lr: 0.0970
Epoch 22/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0080 - mae: 0.0458 - mse: 0.0038 - val_loss: 0.0040 - val_mae: 0.0374 - val_mse: 0.0019 - lr: 0.0970
Epoch 23/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0039 - mae: 0.0368 - mse: 0.0019
Epoch 23: ReduceLROnPlateau reducing learning rate to 0.048476092517375946.
18/18 [==============================] - 0s 5ms/step - loss: 0.0033 - mae: 0.0365 - mse: 0.0019 - val_loss: 0.0025 - val_mae: 0.0376 - val_mse: 0.0019 - lr: 0.0970
Epoch 24/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0024 - mae: 0.0371 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
18/18 [==============================] - 1s 71ms/step - loss: 0.0021 - mae: 0.0365 - mse: 0.0018 - val_loss: 0.0021 - val_mae: 0.0359 - val_mse: 0.0019 - lr: 0.0485
Epoch 25/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0019 - mae: 0.0343 - mse: 0.0018
18/18 [==============================] - 2s 89ms/step - loss: 0.0020 - mae: 0.0364 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0365 - val_mse: 0.0019 - lr: 0.0485
Epoch 26/200
18/18 [==============================] - 1s 61ms/step - loss: 0.0019 - mae: 0.0362 - mse: 0.0018 - val_loss: 0.0020 - val_mae: 0.0374 - val_mse: 0.0019 - lr: 0.0485
Epoch 27/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0019 - mae: 0.0369 - mse: 0.0018
18/18 [==============================] - 1s 62ms/step - loss: 0.0019 - mae: 0.0363 - mse: 0.0018 - val_loss: 0.0020 - val_mae: 0.0373 - val_mse: 0.0019 - lr: 0.0485
Epoch 28/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0362 - mse: 0.0018 - val_loss: 0.0020 - val_mae: 0.0375 - val_mse: 0.0019 - lr: 0.0485
Epoch 29/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0363 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0359 - val_mse: 0.0019 - lr: 0.0485
Epoch 30/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0020 - mae: 0.0362 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
18/18 [==============================] - 1s 66ms/step - loss: 0.0019 - mae: 0.0361 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0361 - val_mse: 0.0019 - lr: 0.0485
Epoch 31/200
18/18 [==============================] - 1s 63ms/step - loss: 0.0019 - mae: 0.0361 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0373 - val_mse: 0.0019 - lr: 0.0485
Epoch 32/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0360 - mse: 0.0018 - val_loss: 0.0020 - val_mae: 0.0385 - val_mse: 0.0019 - lr: 0.0485
Epoch 33/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0360 - mse: 0.0018 - val_loss: 0.0023 - val_mae: 0.0427 - val_mse: 0.0023 - lr: 0.0485
Epoch 34/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0026 - mae: 0.0381 - mse: 0.0024 - val_loss: 0.0026 - val_mae: 0.0353 - val_mse: 0.0022 - lr: 0.0485
Epoch 35/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0026 - mae: 0.0345 - mse: 0.0022
Epoch 35: ReduceLROnPlateau reducing learning rate to 0.024238046258687973.
18/18 [==============================] - 0s 5ms/step - loss: 0.0021 - mae: 0.0361 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0386 - val_mse: 0.0019 - lr: 0.0485
Epoch 36/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0019 - mae: 0.0360 - mse: 0.0018 - val_loss: 0.0020 - val_mae: 0.0354 - val_mse: 0.0019 - lr: 0.0242
Epoch 37/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0018 - mae: 0.0337 - mse: 0.0017
18/18 [==============================] - 1s 73ms/step - loss: 0.0019 - mae: 0.0359 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0369 - val_mse: 0.0019 - lr: 0.0242
Epoch 38/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019
18/18 [==============================] - 1s 66ms/step - loss: 0.0019 - mae: 0.0357 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0368 - val_mse: 0.0018 - lr: 0.0242
Epoch 39/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0358 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0376 - val_mse: 0.0019 - lr: 0.0242
Epoch 40/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0358 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0356 - val_mse: 0.0018 - lr: 0.0242
Epoch 41/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0019 - mae: 0.0351 - mse: 0.0018
18/18 [==============================] - 1s 78ms/step - loss: 0.0019 - mae: 0.0358 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0362 - val_mse: 0.0018 - lr: 0.0242
Epoch 42/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0357 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0359 - val_mse: 0.0018 - lr: 0.0242
Epoch 43/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0019 - mae: 0.0359 - mse: 0.0018
18/18 [==============================] - 1s 63ms/step - loss: 0.0018 - mae: 0.0356 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0359 - val_mse: 0.0018 - lr: 0.0242
Epoch 44/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0356 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0376 - val_mse: 0.0019 - lr: 0.0242
Epoch 45/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0018 - mae: 0.0362 - mse: 0.0018
Epoch 45: ReduceLROnPlateau reducing learning rate to 0.012119023129343987.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
18/18 [==============================] - 1s 73ms/step - loss: 0.0018 - mae: 0.0357 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0368 - val_mse: 0.0018 - lr: 0.0242
Epoch 46/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0355 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0375 - val_mse: 0.0019 - lr: 0.0121
Epoch 47/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0358 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0357 - val_mse: 0.0018 - lr: 0.0121
Epoch 48/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0018 - mae: 0.0356 - mse: 0.0018
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0357 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0366 - val_mse: 0.0018 - lr: 0.01211
Epoch 49/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0357 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0366 - val_mse: 0.0018 - lr: 0.01211
Epoch 50/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0355 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0367 - val_mse: 0.0018 - lr: 0.0121
Epoch 51/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0356 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0367 - val_mse: 0.0018 - lr: 0.0121
Epoch 52/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0017 - mae: 0.0344 - mse: 0.0016
18/18 [==============================] - 1s 68ms/step - loss: 0.0018 - mae: 0.0356 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0358 - val_mse: 0.0018 - lr: 0.0121
Epoch 53/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0018 - mae: 0.0343 - mse: 0.0017
18/18 [==============================] - 1s 68ms/step - loss: 0.0018 - mae: 0.0356 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0358 - val_mse: 0.0018 - lr: 0.0121
18/18 [==============================] - 1s 68ms/step - loss: 0.0018 - mae: 0.0356 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0358 - val_mse: 0.0018 - lr: 0.0121
18/18 [==============================] - 1s 72ms/step - loss: 0.0018 - mae: 0.0355 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 0.0121
18/18 [==============================] - 1s 72ms/step - loss: 0.0018 - mae: 0.0355 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 0.0121
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
18/18 [==============================] - 1s 72ms/step - loss: 0.0018 - mae: 0.0355 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 0.0121
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0370 - val_mse: 0.0018 - lr: 0.00611
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0370 - val_mse: 0.0018 - lr: 0.00611
 1/18 [>.............................] - ETA: 0s - loss: 0.0017 - mae: 0.0326 - mse: 0.00160017 - val_loss: 0.0019 - val_mae: 0.0356 - val_mse: 0.0018 - lr: 0.00611
 1/18 [>.............................] - ETA: 0s - loss: 0.0017 - mae: 0.0326 - mse: 0.00160017 - val_loss: 0.0019 - val_mae: 0.0356 - val_mse: 0.0018 - lr: 0.00611
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
 1/18 [>.............................] - ETA: 0s - loss: 0.0017 - mae: 0.0333 - mse: 0.0016.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 0.0030
18/18 [==============================] - 1s 68ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 0.0030
18/18 [==============================] - 1s 68ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 0.0030
 1/18 [>.............................] - ETA: 0s - loss: 0.0019 - mae: 0.0353 - mse: 0.0018.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 0.0030
 1/18 [>.............................] - ETA: 0s - loss: 0.0019 - mae: 0.0353 - mse: 0.0018.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 0.0030
18/18 [==============================] - 1s 68ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0363 - val_mse: 0.0018 - lr: 0.0030
18/18 [==============================] - 1s 68ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0363 - val_mse: 0.0018 - lr: 0.0030
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0366 - val_mse: 0.0018 - lr: 0.00150
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0366 - val_mse: 0.0018 - lr: 0.00150
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0366 - val_mse: 0.0018 - lr: 0.00150
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0359 - val_mse: 0.0018 - lr: 0.00150
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0359 - val_mse: 0.0018 - lr: 0.00150
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0359 - val_mse: 0.0018 - lr: 0.00150
 1/18 [>.............................] - ETA: 0s - loss: 0.0018 - mae: 0.0350 - mse: 0.00170017 - val_loss: 0.0019 - val_mae: 0.0359 - val_mse: 0.0018 - lr: 0.00150
 1/18 [>.............................] - ETA: 0s - loss: 0.0018 - mae: 0.0350 - mse: 0.00170017 - val_loss: 0.0019 - val_mae: 0.0359 - val_mse: 0.0018 - lr: 0.00150
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0362 - val_mse: 0.0018 - lr: 7.5744e-044
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0362 - val_mse: 0.0018 - lr: 7.5744e-044
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0362 - val_mse: 0.0018 - lr: 7.5744e-044
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
15/18 [========================>.....] - ETA: 0s - loss: 0.0018 - mae: 0.0354 - mse: 0.00170017 - val_loss: 0.0019 - val_mae: 0.0362 - val_mse: 0.0018 - lr: 7.5744e-044
15/18 [========================>.....] - ETA: 0s - loss: 0.0018 - mae: 0.0354 - mse: 0.00170017 - val_loss: 0.0019 - val_mae: 0.0362 - val_mse: 0.0018 - lr: 7.5744e-044
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0359 - val_mse: 0.0018 - lr: 3.7872e-044
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0359 - val_mse: 0.0018 - lr: 3.7872e-044
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 3.7872e-044
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 3.7872e-044
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 3.7872e-044
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
18/18 [==============================] - 1s 72ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0361 - val_mse: 0.0018 - lr: 3.7872e-04
18/18 [==============================] - 1s 72ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0361 - val_mse: 0.0018 - lr: 3.7872e-04
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 1.8936e-044
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 1.8936e-044
18/18 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 1.8936e-044
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 1.8936e-044
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0354 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 1.8936e-044
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-054
18/18 [==============================] - 0s 5ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-054
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
 1/18 [>.............................] - ETA: 0s - loss: 0.0018 - mae: 0.0348 - mse: 0.00170017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-054
 1/18 [>.............................] - ETA: 0s - loss: 0.0018 - mae: 0.0348 - mse: 0.00170017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-054
18/18 [==============================] - 1s 68ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-05
18/18 [==============================] - 1s 68ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-05
 1/18 [>.............................] - ETA: 0s - loss: 0.0017 - mae: 0.0345 - mse: 0.0017.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-05
 1/18 [>.............................] - ETA: 0s - loss: 0.0017 - mae: 0.0345 - mse: 0.0017.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-05
18/18 [==============================] - 1s 69ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-05
18/18 [==============================] - 1s 69ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-05
 1/18 [>.............................] - ETA: 0s - loss: 0.0018 - mae: 0.0353 - mse: 0.0017.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-05
 1/18 [>.............................] - ETA: 0s - loss: 0.0018 - mae: 0.0353 - mse: 0.0017.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 9.4680e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
 1/18 [>.............................] - ETA: 0s - loss: 0.0017 - mae: 0.0343 - mse: 0.0016.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
 1/18 [>.............................] - ETA: 0s - loss: 0.0017 - mae: 0.0343 - mse: 0.0016.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
18/18 [==============================] - 1s 84ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 135/200========================] - 1s 84ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 135/200========================] - 1s 84ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 138/200========================] - 1s 84ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 138/200========================] - 1s 84ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 142/200========================] - 1s 84ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 142/200========================] - 1s 84ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 142/200========================] - 1s 84ms/step - loss: 0.0018 - mae: 0.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 146: ReduceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 146: ReduceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 150/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 150/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 150/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 150/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 150/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 150/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 156/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 158/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 158/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 162/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 162/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 162/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 166/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 166/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 173/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 177/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 177/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 181/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 181/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 181/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 181/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 186/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 186/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 188/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 193/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_121643-yifcg2ip\files\model-best)... Done. 0.0s
Epoch 195/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 195/200duceLROnPlateau reducing learning rate to 1.1834983524749987e-05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
Epoch 49/200duceLROnPlateau reducing learning rate to 0.048476092517375946.05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
24/24 [==============================] - 0s 5ms/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0485
Epoch 46/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0105 - val_mae: 0.0904 - val_mse: 0.0103 - lr: 0.0485
Epoch 47/200
24/24 [==============================] - 0s 6ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0098 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0485
Epoch 48/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0485
Epoch 49/200duceLROnPlateau reducing learning rate to 0.048476092517375946.05.0353 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0360 - val_mse: 0.0018 - lr: 4.7340e-05
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0485
Epoch 50/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0097 - val_mae: 0.0902 - val_mse: 0.0096 - lr: 0.0485
Epoch 51/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0101 - val_mae: 0.0912 - val_mse: 0.0100 - lr: 0.0485
Epoch 52/200
24/24 [==============================] - ETA: 0s - loss: 0.0098 - mae: 0.0904 - mse: 0.0097
Epoch 52: ReduceLROnPlateau reducing learning rate to 0.024238046258687973.
24/24 [==============================] - 0s 5ms/step - loss: 0.0098 - mae: 0.0904 - mse: 0.0097 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0485
Epoch 53/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0242
Epoch 54/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0242
Epoch 55/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0242
Epoch 56/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0242
Epoch 57/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0242
Epoch 58/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0242
Epoch 59/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0242
Epoch 60/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0096 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0242
Epoch 61/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0242
Epoch 62/200
 1/24 [>.............................] - ETA: 0s - loss: 0.0096 - mae: 0.0902 - mse: 0.0095
Epoch 62: ReduceLROnPlateau reducing learning rate to 0.012119023129343987.
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0903 - val_mse: 0.0096 - lr: 0.0242
Epoch 63/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0121
Epoch 64/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0121
Epoch 65/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0121
Epoch 66/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0121
Epoch 67/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0121
Epoch 68/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0121
Epoch 69/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0121
Epoch 70/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0121
Epoch 71/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0121
Epoch 72/200
23/24 [===========================>..] - ETA: 0s - loss: 0.0096 - mae: 0.0902 - mse: 0.0095
Epoch 72: ReduceLROnPlateau reducing learning rate to 0.006059511564671993.
24/24 [==============================] - 0s 6ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0121
Epoch 73/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0061
Epoch 74/200
24/24 [==============================] - 0s 6ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0903 - val_mse: 0.0096 - lr: 0.0061
Epoch 75/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0061
Epoch 76/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0061
Epoch 77/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0061
Epoch 78/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0061
Epoch 79/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0061
Epoch 80/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0061
Epoch 81/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0061
Epoch 82/200
 1/24 [>.............................] - ETA: 0s - loss: 0.0091 - mae: 0.0876 - mse: 0.0090
Epoch 82: ReduceLROnPlateau reducing learning rate to 0.0030297557823359966.
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0061
Epoch 83/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0030
Epoch 84/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0030
Epoch 85/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0030
Epoch 86/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0030
Epoch 87/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0030
Epoch 88/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0030
Epoch 89/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0030
Epoch 90/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0030
Epoch 91/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0030
Epoch 92/200
 1/24 [>.............................] - ETA: 0s - loss: 0.0098 - mae: 0.0912 - mse: 0.0097
Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0015148778911679983.
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0030
Epoch 93/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0096 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0015
Epoch 94/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0015
Epoch 95/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0015
Epoch 96/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0015
Epoch 97/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0015
Epoch 98/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0015
Epoch 99/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0015
Epoch 100/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0015
Epoch 101/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0015
Epoch 102/200
24/24 [==============================] - ETA: 0s - loss: 0.0096 - mae: 0.0903 - mse: 0.0095
Epoch 102: ReduceLROnPlateau reducing learning rate to 0.0007574389455839992.
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 0.0015
Epoch 103/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 7.5744e-04
Epoch 104/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 7.5744e-04
Epoch 105/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 7.5744e-04
Epoch 106/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 7.5744e-04
Epoch 107/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 7.5744e-04
Epoch 108/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 7.5744e-04
Epoch 109/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 7.5744e-04
Epoch 110/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 7.5744e-04
Epoch 111/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 7.5744e-04
Epoch 112/200
 1/24 [>.............................] - ETA: 0s - loss: 0.0095 - mae: 0.0905 - mse: 0.0094
Epoch 112: ReduceLROnPlateau reducing learning rate to 0.0003787194727919996.
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 7.5744e-04
Epoch 113/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 3.7872e-04
Epoch 114/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 3.7872e-04
Epoch 115/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 3.7872e-04
Epoch 116/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 3.7872e-04
Epoch 117/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 3.7872e-04
Epoch 118/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 3.7872e-04
Epoch 119/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 3.7872e-04
Epoch 120/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 3.7872e-04
Epoch 121/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 3.7872e-04
Epoch 122/200
 1/24 [>.............................] - ETA: 0s - loss: 0.0094 - mae: 0.0904 - mse: 0.0094
Epoch 122: ReduceLROnPlateau reducing learning rate to 0.0001893597363959998.
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 3.7872e-04
Epoch 123/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.8936e-04
Epoch 124/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.8936e-04
Epoch 125/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.8936e-04
Epoch 126/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.8936e-04
Epoch 127/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.8936e-04
Epoch 128/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.8936e-04
Epoch 129/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.8936e-04
Epoch 130/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.8936e-04
Epoch 131/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.8936e-04
Epoch 132/200
 1/24 [>.............................] - ETA: 0s - loss: 0.0091 - mae: 0.0876 - mse: 0.0090
Epoch 132: ReduceLROnPlateau reducing learning rate to 9.46798681979999e-05.
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.8936e-04
Epoch 133/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 9.4680e-05
Epoch 134/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 9.4680e-05
Epoch 135/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 9.4680e-05
Epoch 136/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 9.4680e-05
Epoch 137/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 9.4680e-05
Epoch 138/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 9.4680e-05
Epoch 139/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 9.4680e-05
Epoch 140/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 9.4680e-05
Epoch 141/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 9.4680e-05
Epoch 142/200
 1/24 [>.............................] - ETA: 0s - loss: 0.0092 - mae: 0.0880 - mse: 0.0091
Epoch 142: ReduceLROnPlateau reducing learning rate to 4.733993409899995e-05.
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 9.4680e-05
Epoch 143/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 4.7340e-05
Epoch 144/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 4.7340e-05
Epoch 145/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 4.7340e-05
Epoch 146/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 4.7340e-05
Epoch 147/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 4.7340e-05
Epoch 148/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 4.7340e-05
Epoch 149/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 4.7340e-05
Epoch 150/200
24/24 [==============================] - 0s 6ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 4.7340e-05
Epoch 151/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 4.7340e-05
Epoch 152/200
24/24 [==============================] - ETA: 0s - loss: 0.0096 - mae: 0.0903 - mse: 0.0095
Epoch 152: ReduceLROnPlateau reducing learning rate to 2.3669967049499974e-05.
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 4.7340e-05
Epoch 153/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 2.3670e-05
Epoch 154/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 2.3670e-05
Epoch 155/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 2.3670e-05
Epoch 156/200
24/24 [==============================] - 0s 6ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 2.3670e-05
Epoch 157/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 2.3670e-05
Epoch 158/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 2.3670e-05
Epoch 159/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 2.3670e-05
Epoch 160/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 2.3670e-05
Epoch 161/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 2.3670e-05
Epoch 162/200
 1/24 [>.............................] - ETA: 0s - loss: 0.0094 - mae: 0.0899 - mse: 0.0094
Epoch 162: ReduceLROnPlateau reducing learning rate to 1.1834983524749987e-05.
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 2.3670e-05
Epoch 163/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.1835e-05
Epoch 164/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.1835e-05
Epoch 165/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.1835e-05
Epoch 166/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.1835e-05
Epoch 167/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.1835e-05
Epoch 168/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.1835e-05
Epoch 169/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.1835e-05
Epoch 170/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.1835e-05
Epoch 171/200
24/24 [==============================] - 0s 6ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.1835e-05
Epoch 172/200
 1/24 [>.............................] - ETA: 0s - loss: 0.0098 - mae: 0.0909 - mse: 0.0097
Epoch 172: ReduceLROnPlateau reducing learning rate to 1e-05.
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.1835e-05
Epoch 173/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 174/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 175/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 176/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 177/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 178/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 179/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 180/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 181/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 182/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 183/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 184/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 185/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 186/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 187/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 188/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 189/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 190/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 191/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 192/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 193/200
24/24 [==============================] - 0s 6ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 194/200
24/24 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 195/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 196/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 197/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 198/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 199/200
24/24 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
Epoch 200/200
24/24 [==============================] - 0s 5ms/step - loss: 0.0096 - mae: 0.0903 - mse: 0.0095 - val_loss: 0.0095 - val_mae: 0.0902 - val_mse: 0.0095 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.09695218206194554LR_[23]HN_224BS_10P_val_lossM_200epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
18/18 [==============================] - 1s 14ms/step - loss: 0.1388 - mae: 0.1645 - mse: 0.0343 - val_loss: 0.0560 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0970
Epoch 2/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0363 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0293 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0970
Epoch 3/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0268 - mae: 0.1525 - mse: 0.0248 - val_loss: 0.0264 - val_mae: 0.1532 - val_mse: 0.0258 - lr: 0.0970
Epoch 4/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0251 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 5/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0249 - mae: 0.1525 - mse: 0.0247 - val_loss: 0.0251 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0970
Epoch 6/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0248 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 7/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0970
Epoch 8/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0970
Epoch 9/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 10/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0970
Epoch 11/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 12/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0258 - val_mae: 0.1532 - val_mse: 0.0256 - lr: 0.0970
Epoch 13/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0248 - mae: 0.1525 - mse: 0.0247 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 14/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 15/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 16/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 17/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 18/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0970
Epoch 19/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0970
Epoch 20/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0970
Epoch 21/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 22/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0970
Epoch 23/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0970
Epoch 24/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0970
Epoch 25/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0970
Epoch 26/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0970
Epoch 27/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0243 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0970
Epoch 28/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0970
Epoch 29/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0235 - mae: 0.1500 - mse: 0.0234
Epoch 29: ReduceLROnPlateau reducing learning rate to 0.048476092517375946.
18/18 [==============================] - 0s 6ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0970
Epoch 30/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 31/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 32/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 33/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 34/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 35/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 36/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 37/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 38/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0485
Epoch 39/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 40/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 41/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 42/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 43/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 44/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 45/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0485
Epoch 46/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 47/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0245 - lr: 0.0485
Epoch 48/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0485
Epoch 49/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0237 - mae: 0.1508 - mse: 0.0236
Epoch 49: ReduceLROnPlateau reducing learning rate to 0.024238046258687973.
18/18 [==============================] - 0s 7ms/step - loss: 0.0243 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0485
Epoch 50/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0242 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0242
Epoch 51/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0242
Epoch 52/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0242
Epoch 53/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0242
Epoch 54/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0242
Epoch 55/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0242
Epoch 56/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0245 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0242
Epoch 57/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0242
Epoch 58/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0242
Epoch 59/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0245 - mae: 0.1537 - mse: 0.0245
Epoch 59: ReduceLROnPlateau reducing learning rate to 0.012119023129343987.
18/18 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0242
Epoch 60/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0121
Epoch 61/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0121
Epoch 62/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0121
Epoch 63/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0121
Epoch 64/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0121
Epoch 65/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0121
Epoch 66/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0121
Epoch 67/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0121
Epoch 68/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0121
Epoch 69/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0235 - mae: 0.1510 - mse: 0.0235
Epoch 69: ReduceLROnPlateau reducing learning rate to 0.006059511564671993.
18/18 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0121
Epoch 70/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0061
Epoch 71/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0061
Epoch 72/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0061
Epoch 73/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0061
Epoch 74/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0061
Epoch 75/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0061
Epoch 76/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0061
Epoch 77/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0061
Epoch 78/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0061
Epoch 79/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0238 - mae: 0.1513 - mse: 0.0238
Epoch 79: ReduceLROnPlateau reducing learning rate to 0.0030297557823359966.
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0061
Epoch 80/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0030
Epoch 81/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0030
Epoch 82/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0244 - lr: 0.0030
Epoch 83/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0030
Epoch 84/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0030
Epoch 85/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0030
Epoch 86/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0030
Epoch 87/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0030
Epoch 88/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0030
Epoch 89/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0246 - mae: 0.1544 - mse: 0.0246
Epoch 89: ReduceLROnPlateau reducing learning rate to 0.0015148778911679983.
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0030
Epoch 90/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0015
Epoch 91/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0015
Epoch 92/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0015
Epoch 93/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0015
Epoch 94/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0015
Epoch 95/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0015
Epoch 96/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0015
Epoch 97/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0015
Epoch 98/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0015
Epoch 99/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0248 - mae: 0.1549 - mse: 0.0247
Epoch 99: ReduceLROnPlateau reducing learning rate to 0.0007574389455839992.
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 0.0015
Epoch 100/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 7.5744e-04
Epoch 101/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 7.5744e-04
Epoch 102/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 7.5744e-04
Epoch 103/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 7.5744e-04
Epoch 104/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 7.5744e-04
Epoch 105/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 7.5744e-04
Epoch 106/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 7.5744e-04
Epoch 107/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 7.5744e-04
Epoch 108/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 7.5744e-04
Epoch 109/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0245 - mae: 0.1537 - mse: 0.0245
Epoch 109: ReduceLROnPlateau reducing learning rate to 0.0003787194727919996.
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 7.5744e-04
Epoch 110/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 3.7872e-04
Epoch 111/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 3.7872e-04
Epoch 112/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 3.7872e-04
Epoch 113/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 3.7872e-04
Epoch 114/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 3.7872e-04
Epoch 115/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 3.7872e-04
Epoch 116/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 3.7872e-04
Epoch 117/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 3.7872e-04
Epoch 118/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 3.7872e-04
Epoch 119/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0241 - mae: 0.1524 - mse: 0.0240
Epoch 119: ReduceLROnPlateau reducing learning rate to 0.0001893597363959998.
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 3.7872e-04
Epoch 120/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.8936e-04
Epoch 121/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.8936e-04
Epoch 122/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.8936e-04
Epoch 123/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.8936e-04
Epoch 124/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.8936e-04
Epoch 125/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.8936e-04
Epoch 126/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.8936e-04
Epoch 127/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.8936e-04
Epoch 128/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.8936e-04
Epoch 129/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0245 - mae: 0.1537 - mse: 0.0245
Epoch 129: ReduceLROnPlateau reducing learning rate to 9.46798681979999e-05.
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.8936e-04
Epoch 130/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 9.4680e-05
Epoch 131/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 9.4680e-05
Epoch 132/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 9.4680e-05
Epoch 133/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 9.4680e-05
Epoch 134/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 9.4680e-05
Epoch 135/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 9.4680e-05
Epoch 136/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 9.4680e-05
Epoch 137/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 9.4680e-05
Epoch 138/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 9.4680e-05
Epoch 139/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0240 - mae: 0.1522 - mse: 0.0240
Epoch 139: ReduceLROnPlateau reducing learning rate to 4.733993409899995e-05.
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 9.4680e-05
Epoch 140/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 4.7340e-05
Epoch 141/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 4.7340e-05
Epoch 142/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 4.7340e-05
Epoch 143/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 4.7340e-05
Epoch 144/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 4.7340e-05
Epoch 145/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 4.7340e-05
Epoch 146/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 4.7340e-05
Epoch 147/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 4.7340e-05
Epoch 148/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 4.7340e-05
Epoch 149/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0244 - mae: 0.1529 - mse: 0.0243
Epoch 149: ReduceLROnPlateau reducing learning rate to 2.3669967049499974e-05.
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 4.7340e-05
Epoch 150/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 2.3670e-05
Epoch 151/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 2.3670e-05
Epoch 152/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 2.3670e-05
Epoch 153/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 2.3670e-05
Epoch 154/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 2.3670e-05
Epoch 155/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 2.3670e-05
Epoch 156/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 2.3670e-05
Epoch 157/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 2.3670e-05
Epoch 158/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 2.3670e-05
Epoch 159/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0246 - mae: 0.1536 - mse: 0.0245
Epoch 159: ReduceLROnPlateau reducing learning rate to 1.1834983524749987e-05.
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 2.3670e-05
Epoch 160/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.1835e-05
Epoch 161/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.1835e-05
Epoch 162/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.1835e-05
Epoch 163/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.1835e-05
Epoch 164/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.1835e-05
Epoch 165/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.1835e-05
Epoch 166/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.1835e-05
Epoch 167/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.1835e-05
Epoch 168/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.1835e-05
Epoch 169/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0237 - mae: 0.1511 - mse: 0.0237
Epoch 169: ReduceLROnPlateau reducing learning rate to 1e-05.
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.1835e-05
Epoch 170/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 171/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 172/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 173/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 174/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 175/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 176/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 177/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 178/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 179/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 180/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 181/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 182/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 183/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 184/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 185/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 186/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 187/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 188/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 189/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 190/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 191/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 192/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 193/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 194/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 195/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 196/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 197/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 198/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 199/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
Epoch 200/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1525 - mse: 0.0241 - val_loss: 0.0244 - val_mae: 0.1532 - val_mse: 0.0243 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.09695218206194554LR_[23]HN_224BS_10P_val_lossM_200epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
18/18 [==============================] - 1s 16ms/step - loss: 0.1576 - mae: 0.2069 - mse: 0.0491 - val_loss: 0.0770 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0970
Epoch 2/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0542 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0472 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0970
Epoch 3/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0441 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0432 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0970
Epoch 4/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0427 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0970
Epoch 5/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0970
Epoch 6/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0970
Epoch 7/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0970
Epoch 8/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0427 - lr: 0.0970
Epoch 9/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0970
Epoch 10/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
Epoch 11/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0419 - mae: 0.2019 - mse: 0.0419
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.
Epoch 178/200========================] - 0s 6ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
Epoch 12/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0970
Epoch 13/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
Epoch 14/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
Epoch 15/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0421 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
Epoch 16/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0421 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
Epoch 17/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
Epoch 18/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0421 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
Epoch 19/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0421 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
Epoch 20/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0411 - mae: 0.1999 - mse: 0.0411
Epoch 20: ReduceLROnPlateau reducing learning rate to 0.048476092517375946.
18/18 [==============================] - 0s 5ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0421 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
Epoch 21/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0421 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0485
Epoch 22/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0421 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0485
Epoch 23/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0485
Epoch 24/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0485
Epoch 25/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0485
Epoch 26/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0421 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0485
Epoch 27/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0485
Epoch 28/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0485
Epoch 29/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0485
Epoch 30/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0485
Epoch 31/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0485
Epoch 32/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0485
Epoch 33/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0485
Epoch 34/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0485
Epoch 35/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0421 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0485
Epoch 36/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0410 - mae: 0.1996 - mse: 0.0409
Epoch 36: ReduceLROnPlateau reducing learning rate to 0.024238046258687973.
18/18 [==============================] - 0s 5ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0420 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0485
Epoch 37/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 38/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0424 - val_mae: 0.2032 - val_mse: 0.0423 - lr: 0.0242
Epoch 39/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 40/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 41/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 42/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 43/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 44/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0420 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 45/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 46/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 47/200
18/18 [==============================] - 0s 9ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 48/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 49/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0420 - mae: 0.2029 - mse: 0.0419
Epoch 49: ReduceLROnPlateau reducing learning rate to 0.012119023129343987.
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0242
Epoch 50/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0121
Epoch 51/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0121
Epoch 52/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0121
Epoch 53/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0121
Epoch 54/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0121
Epoch 55/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0121
Epoch 56/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0121
Epoch 57/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0121
Epoch 58/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0121
Epoch 59/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0416 - mae: 0.2017 - mse: 0.0415
Epoch 59: ReduceLROnPlateau reducing learning rate to 0.006059511564671993.
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0423 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0121
Epoch 60/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0061
Epoch 61/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0061
Epoch 62/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0061
Epoch 63/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0061
Epoch 64/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0061
Epoch 65/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0061
Epoch 66/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0061
Epoch 67/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0061
Epoch 68/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0061
Epoch 69/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0418 - mae: 0.2023 - mse: 0.0418
Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0030297557823359966.
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0419 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0061
Epoch 70/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0030
Epoch 71/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0030
Epoch 72/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0030
Epoch 73/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0030
Epoch 74/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0030
Epoch 75/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0030
Epoch 76/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0422 - lr: 0.0030
Epoch 77/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0030
Epoch 78/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0030
Epoch 79/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0420 - mae: 0.2027 - mse: 0.0419
Epoch 79: ReduceLROnPlateau reducing learning rate to 0.0015148778911679983.
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0030
Epoch 80/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0015
Epoch 81/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0015
Epoch 82/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0015
Epoch 83/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0015
Epoch 84/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0015
Epoch 85/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0015
Epoch 86/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0015
Epoch 87/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0015
Epoch 88/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0015
Epoch 89/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0413 - mae: 0.2014 - mse: 0.0413
Epoch 89: ReduceLROnPlateau reducing learning rate to 0.0007574389455839992.
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 0.0015
Epoch 90/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 7.5744e-04
Epoch 91/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 7.5744e-04
Epoch 92/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 7.5744e-04
Epoch 93/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 7.5744e-04
Epoch 94/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 7.5744e-04
Epoch 95/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 7.5744e-04
Epoch 96/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 7.5744e-04
Epoch 97/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 7.5744e-04
Epoch 98/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 7.5744e-04
Epoch 99/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0421 - mae: 0.2031 - mse: 0.0421
Epoch 99: ReduceLROnPlateau reducing learning rate to 0.0003787194727919996.
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 7.5744e-04
Epoch 100/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 3.7872e-04
Epoch 101/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 3.7872e-04
Epoch 102/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 3.7872e-04
Epoch 103/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 3.7872e-04
Epoch 104/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 3.7872e-04
Epoch 105/200
18/18 [==============================] - 0s 2ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 3.7872e-04
Epoch 106/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 3.7872e-04
Epoch 107/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 3.7872e-04
Epoch 108/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 3.7872e-04
Epoch 109/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0415 - mae: 0.2016 - mse: 0.0415
Epoch 109: ReduceLROnPlateau reducing learning rate to 0.0001893597363959998.
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 3.7872e-04
Epoch 110/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.8936e-04
Epoch 111/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.8936e-04
Epoch 112/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.8936e-04
Epoch 113/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.8936e-04
Epoch 114/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.8936e-04
Epoch 115/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.8936e-04
Epoch 116/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.8936e-04
Epoch 117/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.8936e-04
Epoch 118/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.8936e-04
Epoch 119/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0417 - mae: 0.2023 - mse: 0.0417
Epoch 119: ReduceLROnPlateau reducing learning rate to 9.46798681979999e-05.
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.8936e-04
Epoch 120/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 9.4680e-05
Epoch 121/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 9.4680e-05
Epoch 122/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 9.4680e-05
Epoch 123/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 9.4680e-05
Epoch 124/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 9.4680e-05
Epoch 125/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 9.4680e-05
Epoch 126/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 9.4680e-05
Epoch 127/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 9.4680e-05
Epoch 128/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 9.4680e-05
Epoch 129/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0414 - mae: 0.2014 - mse: 0.0414
Epoch 129: ReduceLROnPlateau reducing learning rate to 4.733993409899995e-05.
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 9.4680e-05
Epoch 130/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 4.7340e-05
Epoch 131/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 4.7340e-05
Epoch 132/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 4.7340e-05
Epoch 133/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 4.7340e-05
Epoch 134/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 4.7340e-05
Epoch 135/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 4.7340e-05
Epoch 136/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 4.7340e-05
Epoch 137/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 4.7340e-05
Epoch 138/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 4.7340e-05
Epoch 139/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0423 - mae: 0.2038 - mse: 0.0422
Epoch 139: ReduceLROnPlateau reducing learning rate to 2.3669967049499974e-05.
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 4.7340e-05
Epoch 140/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 2.3670e-05
Epoch 141/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 2.3670e-05
Epoch 142/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 2.3670e-05
Epoch 143/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 2.3670e-05
Epoch 144/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 2.3670e-05
Epoch 145/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 2.3670e-05
Epoch 146/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 2.3670e-05
Epoch 147/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 2.3670e-05
Epoch 148/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 2.3670e-05
Epoch 149/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0424 - mae: 0.2036 - mse: 0.0423
Epoch 149: ReduceLROnPlateau reducing learning rate to 1.1834983524749987e-05.
18/18 [==============================] - 0s 7ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 2.3670e-05
Epoch 150/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.1835e-05
Epoch 151/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.1835e-05
Epoch 152/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.1835e-05
Epoch 153/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.1835e-05
Epoch 154/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.1835e-05
Epoch 155/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.1835e-05
Epoch 156/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.1835e-05
Epoch 157/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.1835e-05
Epoch 158/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.1835e-05
Epoch 159/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0421 - mae: 0.2030 - mse: 0.0421
Epoch 159: ReduceLROnPlateau reducing learning rate to 1e-05.
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.1835e-05
Epoch 160/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 161/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 162/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 163/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 164/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 165/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 166/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 167/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 168/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 169/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 170/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 171/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 172/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 173/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 174/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 175/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 176/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 177/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 178/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 179/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 180/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 181/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 182/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 183/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 184/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 185/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 186/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 187/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 188/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 189/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 190/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 191/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 192/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 193/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 194/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 195/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 196/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 197/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 198/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 199/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
Epoch 200/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0419 - mae: 0.2025 - mse: 0.0418 - val_loss: 0.0422 - val_mae: 0.2032 - val_mse: 0.0421 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.09695218206194554LR_[23]HN_224BS_10P_val_lossM_200epochs/model_4.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
18/18 [==============================] - 1s 13ms/step - loss: 0.1653 - mae: 0.2529 - mse: 0.0692 - val_loss: 0.0921 - val_mae: 0.2532 - val_mse: 0.0656 - lr: 0.0970
Epoch 2/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0749 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0693 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 3/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0666 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0658 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 4/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0652 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 5/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 6/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 7/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 8/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 9/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 10/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 11/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 12/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 13/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 14/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0653 - mae: 0.2531 - mse: 0.0653
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.048476092517375946.
18/18 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0970
Epoch 15/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 16/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 17/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 18/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 19/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 20/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 21/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 22/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 23/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 24/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 25/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 26/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 27/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 28/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 29/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 30/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 31/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 32/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0637 - mae: 0.2499 - mse: 0.0637
Epoch 32: ReduceLROnPlateau reducing learning rate to 0.024238046258687973.
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0485
Epoch 33/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0242
Epoch 34/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0242
Epoch 35/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0242
Epoch 36/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0242
Epoch 37/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0242
Epoch 38/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0242
Epoch 39/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0242
Epoch 40/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0242
Epoch 41/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0242
Epoch 42/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0640 - mae: 0.2507 - mse: 0.0640
Epoch 42: ReduceLROnPlateau reducing learning rate to 0.012119023129343987.
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0242
Epoch 43/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0121
Epoch 44/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0121
Epoch 45/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0121
Epoch 46/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0121
Epoch 47/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0121
Epoch 48/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 49/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0121
Epoch 50/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 51/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 52/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 53/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0652 - lr: 0.0121
Epoch 54/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 55/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 56/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 57/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 58/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 59/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0121
Epoch 60/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0648 - val_loss: 0.0652 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 61/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0639 - mae: 0.2507 - mse: 0.0639
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.006059511564671993.
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0121
Epoch 62/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0061
Epoch 63/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0061
Epoch 64/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0061
Epoch 65/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0061
Epoch 66/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0061
Epoch 67/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0061
Epoch 68/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0061
Epoch 69/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0061
Epoch 70/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0061
Epoch 71/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0643 - mae: 0.2515 - mse: 0.0643
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0030297557823359966.
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0061
Epoch 72/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0030
Epoch 73/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0030
Epoch 74/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0030
Epoch 75/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0030
Epoch 76/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0030
Epoch 77/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0030
Epoch 78/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0030
Epoch 79/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0030
Epoch 80/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0030
Epoch 81/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0657 - mae: 0.2545 - mse: 0.0656
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0015148778911679983.
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0030
Epoch 82/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0015
Epoch 83/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0015
Epoch 84/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0015
Epoch 85/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0015
Epoch 86/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0015
Epoch 87/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0015
Epoch 88/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0015
Epoch 89/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0015
Epoch 90/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0015
Epoch 91/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0651 - mae: 0.2532 - mse: 0.0650
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0007574389455839992.
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 0.0015
Epoch 92/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 7.5744e-04
Epoch 93/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 7.5744e-04
Epoch 94/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 7.5744e-04
Epoch 95/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 7.5744e-04
Epoch 96/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 7.5744e-04
Epoch 97/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 7.5744e-04
Epoch 98/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 7.5744e-04
Epoch 99/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 7.5744e-04
Epoch 100/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 7.5744e-04
Epoch 101/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0645 - mae: 0.2522 - mse: 0.0645
Epoch 101: ReduceLROnPlateau reducing learning rate to 0.0003787194727919996.
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 7.5744e-04
Epoch 102/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 3.7872e-04
Epoch 103/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 3.7872e-04
Epoch 104/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 3.7872e-04
Epoch 105/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 3.7872e-04
Epoch 106/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 3.7872e-04
Epoch 107/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 3.7872e-04
Epoch 108/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 3.7872e-04
Epoch 109/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 3.7872e-04
Epoch 110/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 3.7872e-04
Epoch 111/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0647 - mae: 0.2526 - mse: 0.0647
Epoch 111: ReduceLROnPlateau reducing learning rate to 0.0001893597363959998.
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 3.7872e-04
Epoch 112/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.8936e-04
Epoch 113/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.8936e-04
Epoch 114/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.8936e-04
Epoch 115/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.8936e-04
Epoch 116/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.8936e-04
Epoch 117/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.8936e-04
Epoch 118/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.8936e-04
Epoch 119/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.8936e-04
Epoch 120/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.8936e-04
Epoch 121/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0647 - mae: 0.2526 - mse: 0.0646
Epoch 121: ReduceLROnPlateau reducing learning rate to 9.46798681979999e-05.
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.8936e-04
Epoch 122/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 9.4680e-05
Epoch 123/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 9.4680e-05
Epoch 124/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 9.4680e-05
Epoch 125/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 9.4680e-05
Epoch 126/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 9.4680e-05
Epoch 127/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 9.4680e-05
Epoch 128/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 9.4680e-05
Epoch 129/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 9.4680e-05
Epoch 130/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 9.4680e-05
Epoch 131/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0651 - mae: 0.2531 - mse: 0.0651
Epoch 131: ReduceLROnPlateau reducing learning rate to 4.733993409899995e-05.
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 9.4680e-05
Epoch 132/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 4.7340e-05
Epoch 133/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 4.7340e-05
Epoch 134/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 4.7340e-05
Epoch 135/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 4.7340e-05
Epoch 136/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 4.7340e-05
Epoch 137/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 4.7340e-05
Epoch 138/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 4.7340e-05
Epoch 139/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 4.7340e-05
Epoch 140/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 4.7340e-05
Epoch 141/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0650 - mae: 0.2530 - mse: 0.0649
Epoch 141: ReduceLROnPlateau reducing learning rate to 2.3669967049499974e-05.
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 4.7340e-05
Epoch 142/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 2.3670e-05
Epoch 143/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 2.3670e-05
Epoch 144/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 2.3670e-05
Epoch 145/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 2.3670e-05
Epoch 146/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 2.3670e-05
Epoch 147/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 2.3670e-05
Epoch 148/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 2.3670e-05
Epoch 149/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 2.3670e-05
Epoch 150/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 2.3670e-05
Epoch 151/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0640 - mae: 0.2511 - mse: 0.0639
Epoch 151: ReduceLROnPlateau reducing learning rate to 1.1834983524749987e-05.
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 2.3670e-05
Epoch 152/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.1835e-05
Epoch 153/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.1835e-05
Epoch 154/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.1835e-05
Epoch 155/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.1835e-05
Epoch 156/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.1835e-05
Epoch 157/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.1835e-05
Epoch 158/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.1835e-05
Epoch 159/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.1835e-05
Epoch 160/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.1835e-05
Epoch 161/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0643 - mae: 0.2517 - mse: 0.0643
Epoch 161: ReduceLROnPlateau reducing learning rate to 1e-05.
18/18 [==============================] - 0s 3ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.1835e-05
Epoch 162/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 163/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 164/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 165/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 166/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 167/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 168/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 169/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 170/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 171/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 172/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 173/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 174/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 175/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 176/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 177/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 178/200========================] - 0s 6ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0424 - lr: 0.0970
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 179/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 180/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 181/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 182/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 183/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 184/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 185/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 186/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 187/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 188/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 189/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 190/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 191/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 192/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 193/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 194/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 195/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 196/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 197/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 198/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 199/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
Epoch 200/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.2525 - mse: 0.0647 - val_loss: 0.0651 - val_mae: 0.2532 - val_mse: 0.0651 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.09695218206194554LR_[23]HN_224BS_10P_val_lossM_200epochs/model_5.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
18/18 [==============================] - 1s 10ms/step - loss: 0.1972 - mae: 0.3025 - mse: 0.0969 - val_loss: 0.1236 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 2/200
18/18 [==============================] - 0s 6ms/step - loss: 0.1034 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0976 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 3/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0944 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0937 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 4/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0930 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0932 - lr: 0.0970
Epoch 5/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 6/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 7/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 8/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 9/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 10/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 11/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 12/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 13/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 14/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 15/200
17/18 [===========================>..] - ETA: 0s - loss: 0.0927 - mae: 0.3026 - mse: 0.0927
Epoch 15: ReduceLROnPlateau reducing learning rate to 0.048476092517375946.
18/18 [==============================] - 0s 7ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0970
Epoch 16/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0485
Epoch 17/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0485
Epoch 18/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0485
Epoch 19/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0485
Epoch 20/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0485
Epoch 21/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0485
Epoch 22/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0485
Epoch 23/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0485
Epoch 24/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0485
Epoch 25/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0910 - mae: 0.2997 - mse: 0.0910
Epoch 25: ReduceLROnPlateau reducing learning rate to 0.024238046258687973.
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0485
Epoch 26/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0242
Epoch 27/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0242
Epoch 28/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0242
Epoch 29/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0242
Epoch 30/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0242
Epoch 31/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0242
Epoch 32/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0242
Epoch 33/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0242
Epoch 34/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0242
Epoch 35/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0926 - mae: 0.3026 - mse: 0.0926
Epoch 35: ReduceLROnPlateau reducing learning rate to 0.012119023129343987.
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0242
Epoch 36/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0121
Epoch 37/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0121
Epoch 38/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0121
Epoch 39/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0121
Epoch 40/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0121
Epoch 41/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0121
Epoch 42/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0121
Epoch 43/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0121
Epoch 44/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0121
Epoch 45/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0909 - mae: 0.2997 - mse: 0.0909
Epoch 45: ReduceLROnPlateau reducing learning rate to 0.006059511564671993.
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0121
Epoch 46/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0061
Epoch 47/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0061
Epoch 48/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0061
Epoch 49/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0061
Epoch 50/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0061
Epoch 51/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0926 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0061
Epoch 52/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0926 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0061
Epoch 53/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0926 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0061
Epoch 54/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0926 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0061
Epoch 55/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0919 - mae: 0.3013 - mse: 0.0919
Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0030297557823359966.
18/18 [==============================] - 0s 7ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0926 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 0.0061
Epoch 56/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0926 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 0.0030
Epoch 57/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0926 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 0.0030
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.8936e-04
Epoch 105/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.8936e-04
Epoch 106/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.8936e-04
Epoch 107/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.8936e-04
Epoch 108/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.8936e-04
Epoch 109/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0914 - mae: 0.3004 - mse: 0.0913
Epoch 109: ReduceLROnPlateau reducing learning rate to 9.46798681979999e-05.
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.8936e-04
Epoch 110/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 9.4680e-05
Epoch 111/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 9.4680e-05
Epoch 112/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 9.4680e-05
Epoch 113/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 9.4680e-05
Epoch 114/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 9.4680e-05
Epoch 115/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 9.4680e-05
Epoch 116/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 9.4680e-05
Epoch 117/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 9.4680e-05
Epoch 118/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 9.4680e-05
Epoch 119/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0932 - mae: 0.3037 - mse: 0.0932
Epoch 119: ReduceLROnPlateau reducing learning rate to 4.733993409899995e-05.
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 9.4680e-05
Epoch 120/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 4.7340e-05
Epoch 121/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 4.7340e-05
Epoch 122/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 4.7340e-05
Epoch 123/200
18/18 [==============================] - 0s 8ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 4.7340e-05
Epoch 124/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 4.7340e-05
Epoch 125/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 4.7340e-05
Epoch 126/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 4.7340e-05
Epoch 127/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 4.7340e-05
Epoch 128/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 4.7340e-05
Epoch 129/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0934 - mae: 0.3039 - mse: 0.0933
Epoch 129: ReduceLROnPlateau reducing learning rate to 2.3669967049499974e-05.
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 4.7340e-05
Epoch 130/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 2.3670e-05
Epoch 131/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 2.3670e-05
Epoch 132/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 2.3670e-05
Epoch 133/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 2.3670e-05
Epoch 134/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 2.3670e-05
Epoch 135/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 2.3670e-05
Epoch 136/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 2.3670e-05
Epoch 137/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 2.3670e-05
Epoch 138/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 2.3670e-05
Epoch 139/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0931 - mae: 0.3036 - mse: 0.0931
Epoch 139: ReduceLROnPlateau reducing learning rate to 1.1834983524749987e-05.
18/18 [==============================] - 0s 7ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 2.3670e-05
Epoch 140/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.1835e-05
Epoch 141/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.1835e-05
Epoch 142/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.1835e-05
Epoch 143/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.1835e-05
Epoch 144/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.1835e-05
Epoch 145/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.1835e-05
Epoch 146/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.1835e-05
Epoch 147/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.1835e-05
Epoch 148/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.1835e-05
Epoch 149/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0921 - mae: 0.3016 - mse: 0.0920
Epoch 149: ReduceLROnPlateau reducing learning rate to 1e-05.
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.1835e-05
Epoch 150/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 151/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 152/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 153/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 154/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 155/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 156/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 157/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 158/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 159/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 160/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 161/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 162/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.0000e-05
Epoch 163/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0939 - mae: 0.3049 - mse: 0.0939
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 3.7872e-04
Epoch 93/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 3.7872e-04
Epoch 94/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 3.7872e-04
Epoch 95/200
18/18 [==============================] - 0s 3ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 3.7872e-04
Epoch 96/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 3.7872e-04
Epoch 97/200
18/18 [==============================] - 0s 7ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 3.7872e-04
Epoch 98/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 3.7872e-04
Epoch 99/200
 1/18 [>.............................] - ETA: 0s - loss: 0.0926 - mae: 0.3027 - mse: 0.0926
Epoch 99: ReduceLROnPlateau reducing learning rate to 0.0001893597363959998.
18/18 [==============================] - 0s 3ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 3.7872e-04
Epoch 100/200
18/18 [==============================] - 0s 4ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.8936e-04
Epoch 101/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.8936e-04
Epoch 102/200
18/18 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.8936e-04
Epoch 103/200
18/18 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.3025 - mse: 0.0925 - val_loss: 0.0930 - val_mae: 0.3032 - val_mse: 0.0930 - lr: 1.8936e-04
Epoch 104/200
Epoch 104/200
Epoch 104/200
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0020s). Check your callbacks.
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200
Epoch 104/200