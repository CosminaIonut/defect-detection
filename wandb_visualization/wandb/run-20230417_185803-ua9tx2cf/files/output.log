Epoch 1/20
242/243 [============================>.] - ETA: 0s - loss: 0.0128 - mae: 0.0454 - mse: 0.0036
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230417_185803-ua9tx2cf\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0127 - mae: 0.0454 - mse: 0.0036 - val_loss: 0.0053 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 0.0759
Epoch 2/20
243/243 [==============================] - 0s 960us/step - loss: 0.0059 - mae: 0.0377 - mse: 0.0019 - val_loss: 0.0059 - val_mae: 0.0367 - val_mse: 0.0018 - lr: 0.0759
Epoch 3/20
243/243 [==============================] - 0s 924us/step - loss: 0.0059 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0058 - val_mae: 0.0408 - val_mse: 0.0022 - lr: 0.0759
Epoch 4/20
167/243 [===================>..........] - ETA: 0s - loss: 0.0059 - mae: 0.0373 - mse: 0.0019
243/243 [==============================] - 1s 3ms/step - loss: 0.0059 - mae: 0.0364 - mse: 0.0019 - val_loss: 0.0051 - val_mae: 0.0334 - val_mse: 0.0016 - lr: 0.0759
Epoch 5/20
243/243 [==============================] - 1s 4ms/step - loss: 0.0057 - mae: 0.0337 - mse: 0.0017 - val_loss: 0.0051 - val_mae: 0.0286 - val_mse: 0.0012 - lr: 0.0759
Epoch 6/20
237/243 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0317 - mse: 0.0015
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230417_185803-ua9tx2cf\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0056 - mae: 0.0317 - mse: 0.0015 - val_loss: 0.0050 - val_mae: 0.0267 - val_mse: 0.0011 - lr: 0.0759
Epoch 7/20
243/243 [==============================] - 0s 948us/step - loss: 0.0057 - mae: 0.0322 - mse: 0.0016 - val_loss: 0.0051 - val_mae: 0.0318 - val_mse: 0.0015 - lr: 0.0759
Epoch 8/20
243/243 [==============================] - 1s 3ms/step - loss: 0.0056 - mae: 0.0308 - mse: 0.0015 - val_loss: 0.0049 - val_mae: 0.0263 - val_mse: 0.0011 - lr: 0.0759
Epoch 9/20
241/243 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0312 - mse: 0.0015
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.03795401751995087.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230417_185803-ua9tx2cf\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0056 - mae: 0.0311 - mse: 0.0015 - val_loss: 0.0047 - val_mae: 0.0273 - val_mse: 0.0011 - lr: 0.0759
Epoch 10/20
239/243 [============================>.] - ETA: 0s - loss: 0.0015 - mae: 0.0257 - mse: 0.0011
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230417_185803-ua9tx2cf\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0258 - mse: 0.0011 - val_loss: 0.0019 - val_mae: 0.0284 - val_mse: 0.0012 - lr: 0.0380
Epoch 11/20
239/243 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0255 - mse: 0.0010
243/243 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0255 - mse: 0.0010 - val_loss: 0.0016 - val_mae: 0.0292 - val_mse: 0.0014 - lr: 0.0380
Epoch 12/20
220/243 [==========================>...] - ETA: 0s - loss: 0.0014 - mae: 0.0247 - mse: 9.9460e-04
243/243 [==============================] - 1s 3ms/step - loss: 0.0014 - mae: 0.0248 - mse: 0.0010 - val_loss: 0.0012 - val_mae: 0.0240 - val_mse: 9.2703e-04 - lr: 0.0380
Epoch 13/20
243/243 [==============================] - 0s 951us/step - loss: 0.0014 - mae: 0.0248 - mse: 0.0010 - val_loss: 0.0017 - val_mae: 0.0223 - val_mse: 7.7088e-04 - lr: 0.0380
Epoch 14/20
243/243 [==============================] - 0s 1ms/step - loss: 0.0014 - mae: 0.0252 - mse: 0.0010 - val_loss: 0.0013 - val_mae: 0.0264 - val_mse: 0.0011 - lr: 0.0380
Epoch 15/20
243/243 [==============================] - 0s 971us/step - loss: 0.0014 - mae: 0.0245 - mse: 9.8882e-04 - val_loss: 0.0013 - val_mae: 0.0238 - val_mse: 9.1668e-04 - lr: 0.0380
Epoch 16/20
235/243 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0247 - mse: 9.8540e-04
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.018977008759975433.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230417_185803-ua9tx2cf\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0247 - mse: 9.8702e-04 - val_loss: 0.0011 - val_mae: 0.0232 - val_mse: 8.2762e-04 - lr: 0.0380
Epoch 17/20
240/243 [============================>.] - ETA: 0s - loss: 0.0010 - mae: 0.0226 - mse: 8.3648e-04
243/243 [==============================] - 1s 3ms/step - loss: 0.0010 - mae: 0.0227 - mse: 8.3921e-04 - val_loss: 9.6822e-04 - val_mae: 0.0219 - val_mse: 7.5695e-04 - lr: 0.0190
Epoch 18/20
243/243 [==============================] - 0s 953us/step - loss: 0.0010 - mae: 0.0227 - mse: 8.3537e-04 - val_loss: 0.0011 - val_mae: 0.0232 - val_mse: 8.7791e-04 - lr: 0.0190
Epoch 19/20
160/243 [==================>...........] - ETA: 0s - loss: 9.8887e-04 - mae: 0.0226 - mse: 8.2884e-04
Epoch 19: ReduceLROnPlateau reducing learning rate to 0.009488504379987717.
243/243 [==============================] - 1s 3ms/step - loss: 9.9527e-04 - mae: 0.0227 - mse: 8.3584e-04 - val_loss: 9.0608e-04 - val_mae: 0.0218 - val_mse: 7.7076e-04 - lr: 0.0190
Epoch 20/20
236/243 [============================>.] - ETA: 0s - loss: 8.7801e-04 - mae: 0.0218 - mse: 7.7606e-04
243/243 [==============================] - 1s 4ms/step - loss: 8.7684e-04 - mae: 0.0218 - mse: 7.7471e-04 - val_loss: 8.6680e-04 - val_mae: 0.0216 - val_mse: 7.5308e-04 - lr: 0.0095
>Saved ../trained_models/models_segments_overlap-new-network-sigmoid-adam-8IN-20HN_rmsprop_0.07590803843709379_30_16_20epochs/model_1.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
323/323 [==============================] - 1s 1ms/step - loss: 0.0114 - mae: 0.0549 - mse: 0.0044 - val_loss: 0.0090 - val_mae: 0.0519 - val_mse: 0.0037 - lr: 0.0759
Epoch 2/20
323/323 [==============================] - 0s 1ms/step - loss: 0.0075 - mae: 0.0517 - mse: 0.0036 - val_loss: 0.0076 - val_mae: 0.0518 - val_mse: 0.0037 - lr: 0.0759
Epoch 3/20
323/323 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0519 - mse: 0.0036 - val_loss: 0.0073 - val_mae: 0.0506 - val_mse: 0.0035 - lr: 0.0759
Epoch 4/20
323/323 [==============================] - 0s 1ms/step - loss: 0.0075 - mae: 0.0516 - mse: 0.0036 - val_loss: 0.0101 - val_mae: 0.0606 - val_mse: 0.0054 - lr: 0.0759
Epoch 5/20
323/323 [==============================] - 0s 933us/step - loss: 0.0076 - mae: 0.0519 - mse: 0.0037 - val_loss: 0.0072 - val_mae: 0.0507 - val_mse: 0.0035 - lr: 0.0759
Epoch 6/20
317/323 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0518 - mse: 0.0036
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.03795401751995087.
323/323 [==============================] - 0s 936us/step - loss: 0.0075 - mae: 0.0518 - mse: 0.0036 - val_loss: 0.0070 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0759
Epoch 7/20
323/323 [==============================] - 0s 906us/step - loss: 0.0036 - mae: 0.0508 - mse: 0.0035 - val_loss: 0.0037 - val_mae: 0.0513 - val_mse: 0.0036 - lr: 0.0380
Epoch 8/20
323/323 [==============================] - 0s 918us/step - loss: 0.0036 - mae: 0.0509 - mse: 0.0035 - val_loss: 0.0039 - val_mae: 0.0525 - val_mse: 0.0038 - lr: 0.0380
Epoch 9/20
248/323 [======================>.......] - ETA: 0s - loss: 0.0035 - mae: 0.0504 - mse: 0.0034
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.018977008759975433.
323/323 [==============================] - 0s 899us/step - loss: 0.0036 - mae: 0.0510 - mse: 0.0035 - val_loss: 0.0036 - val_mae: 0.0504 - val_mse: 0.0034 - lr: 0.0380
Epoch 10/20
323/323 [==============================] - 0s 904us/step - loss: 0.0035 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0035 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0190
Epoch 11/20
323/323 [==============================] - 0s 903us/step - loss: 0.0035 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0035 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0190
Epoch 12/20
245/323 [=====================>........] - ETA: 0s - loss: 0.0034 - mae: 0.0506 - mse: 0.0034
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.009488504379987717.
323/323 [==============================] - 0s 898us/step - loss: 0.0035 - mae: 0.0508 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0190
Epoch 13/20
323/323 [==============================] - 0s 898us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0095
Epoch 14/20
323/323 [==============================] - 0s 924us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0095
Epoch 15/20
320/323 [============================>.] - ETA: 0s - loss: 0.0034 - mae: 0.0506 - mse: 0.0034
Epoch 15: ReduceLROnPlateau reducing learning rate to 0.004744252189993858.
323/323 [==============================] - 0s 914us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0095
Epoch 16/20
323/323 [==============================] - 0s 904us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0047
Epoch 17/20
323/323 [==============================] - 0s 898us/step - loss: 0.0034 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0504 - val_mse: 0.0034 - lr: 0.0047
Epoch 18/20
248/323 [======================>.......] - ETA: 0s - loss: 0.0034 - mae: 0.0508 - mse: 0.0034
Epoch 18: ReduceLROnPlateau reducing learning rate to 0.002372126094996929.
323/323 [==============================] - 0s 898us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0047
Epoch 19/20
323/323 [==============================] - 0s 889us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0024
Epoch 20/20
323/323 [==============================] - 0s 895us/step - loss: 0.0034 - mae: 0.0506 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0503 - val_mse: 0.0034 - lr: 0.0024
>Saved ../trained_models/models_segments_overlap-new-network-sigmoid-adam-8IN-20HN_rmsprop_0.07590803843709379_30_16_20epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0008s vs `on_train_batch_end` time: 0.0009s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0019s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_train_batch_end` time: 0.0019s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_train_batch_end` time: 0.0009s). Check your callbacks.
Epoch 1/20
243/243 [==============================] - 1s 1ms/step - loss: 0.0105 - mae: 0.0426 - mse: 0.0028 - val_loss: 0.0063 - val_mae: 0.0406 - val_mse: 0.0023 - lr: 0.0759
Epoch 2/20
243/243 [==============================] - 0s 916us/step - loss: 0.0062 - mae: 0.0403 - mse: 0.0023 - val_loss: 0.0062 - val_mae: 0.0421 - val_mse: 0.0025 - lr: 0.0759
Epoch 3/20
243/243 [==============================] - 0s 907us/step - loss: 0.0062 - mae: 0.0400 - mse: 0.0022 - val_loss: 0.0059 - val_mae: 0.0402 - val_mse: 0.0022 - lr: 0.0759
Epoch 4/20
164/243 [===================>..........] - ETA: 0s - loss: 0.0063 - mae: 0.0410 - mse: 0.0024
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.03795401751995087.
243/243 [==============================] - 0s 891us/step - loss: 0.0063 - mae: 0.0406 - mse: 0.0023 - val_loss: 0.0058 - val_mae: 0.0398 - val_mse: 0.0022 - lr: 0.0759
Epoch 5/20
243/243 [==============================] - 0s 878us/step - loss: 0.0021 - mae: 0.0384 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0380
Epoch 6/20
243/243 [==============================] - 0s 916us/step - loss: 0.0021 - mae: 0.0384 - mse: 0.0020 - val_loss: 0.0023 - val_mae: 0.0399 - val_mse: 0.0022 - lr: 0.0380
Epoch 7/20
243/243 [==============================] - 0s 914us/step - loss: 0.0021 - mae: 0.0385 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0390 - val_mse: 0.0021 - lr: 0.0380
Epoch 8/20
240/243 [============================>.] - ETA: 0s - loss: 0.0022 - mae: 0.0387 - mse: 0.0020
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.018977008759975433.
243/243 [==============================] - 0s 918us/step - loss: 0.0022 - mae: 0.0387 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0380
Epoch 9/20
243/243 [==============================] - 0s 1ms/step - loss: 0.0020 - mae: 0.0382 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0190
Epoch 10/20
243/243 [==============================] - 0s 1ms/step - loss: 0.0020 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0190
Epoch 11/20
229/243 [===========================>..] - ETA: 0s - loss: 0.0020 - mae: 0.0381 - mse: 0.0019
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.009488504379987717.
243/243 [==============================] - 0s 980us/step - loss: 0.0020 - mae: 0.0382 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0388 - val_mse: 0.0020 - lr: 0.0190
Epoch 12/20
243/243 [==============================] - 0s 971us/step - loss: 0.0019 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
Epoch 13/20
243/243 [==============================] - 0s 929us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0095
Epoch 14/20
167/243 [===================>..........] - ETA: 0s - loss: 0.0019 - mae: 0.0380 - mse: 0.0019
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.004744252189993858.
243/243 [==============================] - 0s 910us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
Epoch 15/20
243/243 [==============================] - 0s 990us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0047
Epoch 16/20
243/243 [==============================] - 0s 905us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0047
Epoch 17/20
241/243 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0380 - mse: 0.0019
Epoch 17: ReduceLROnPlateau reducing learning rate to 0.002372126094996929.
243/243 [==============================] - 0s 917us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0047
Epoch 18/20
243/243 [==============================] - 0s 900us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0024
Epoch 19/20
243/243 [==============================] - 0s 914us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0024
Epoch 20/20
163/243 [===================>..........] - ETA: 0s - loss: 0.0019 - mae: 0.0382 - mse: 0.0019
Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0011860630474984646.
243/243 [==============================] - 0s 913us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0024
>Saved ../trained_models/models_segments_overlap-new-network-sigmoid-adam-8IN-20HN_rmsprop_0.07590803843709379_30_16_20epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
243/243 [==============================] - 1s 1ms/step - loss: 0.0108 - mae: 0.0429 - mse: 0.0027 - val_loss: 0.0055 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0759
Epoch 2/20
243/243 [==============================] - 0s 973us/step - loss: 0.0064 - mae: 0.0411 - mse: 0.0025 - val_loss: 0.0057 - val_mae: 0.0391 - val_mse: 0.0021 - lr: 0.0759
Epoch 3/20
243/243 [==============================] - 0s 890us/step - loss: 0.0063 - mae: 0.0410 - mse: 0.0024 - val_loss: 0.0066 - val_mae: 0.0414 - val_mse: 0.0024 - lr: 0.0759
Epoch 4/20
166/243 [===================>..........] - ETA: 0s - loss: 0.0064 - mae: 0.0406 - mse: 0.0024
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.03795401751995087.
243/243 [==============================] - 0s 896us/step - loss: 0.0063 - mae: 0.0403 - mse: 0.0023 - val_loss: 0.0100 - val_mae: 0.0572 - val_mse: 0.0049 - lr: 0.0759
Epoch 5/20
243/243 [==============================] - 0s 877us/step - loss: 0.0022 - mae: 0.0386 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0380
Epoch 6/20
243/243 [==============================] - 0s 894us/step - loss: 0.0021 - mae: 0.0386 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0408 - val_mse: 0.0023 - lr: 0.0380
Epoch 7/20
164/243 [===================>..........] - ETA: 0s - loss: 0.0021 - mae: 0.0383 - mse: 0.0020
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.018977008759975433.
243/243 [==============================] - 0s 889us/step - loss: 0.0022 - mae: 0.0386 - mse: 0.0020 - val_loss: 0.0023 - val_mae: 0.0393 - val_mse: 0.0021 - lr: 0.0380
Epoch 8/20
243/243 [==============================] - 0s 887us/step - loss: 0.0020 - mae: 0.0381 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0190
Epoch 9/20
243/243 [==============================] - 0s 895us/step - loss: 0.0020 - mae: 0.0382 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0190
Epoch 10/20
165/243 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0384 - mse: 0.0020
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.009488504379987717.
243/243 [==============================] - 0s 897us/step - loss: 0.0020 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0190
Epoch 11/20
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0095
Epoch 12/20
243/243 [==============================] - 0s 882us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
Epoch 13/20
235/243 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0380 - mse: 0.0019
Epoch 13: ReduceLROnPlateau reducing learning rate to 0.004744252189993858.
243/243 [==============================] - 0s 937us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0095
Epoch 14/20
243/243 [==============================] - 0s 920us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0047
Epoch 15/20
243/243 [==============================] - 0s 891us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0047
Epoch 16/20
167/243 [===================>..........] - ETA: 0s - loss: 0.0019 - mae: 0.0381 - mse: 0.0019
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.002372126094996929.
243/243 [==============================] - 0s 885us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0047
Epoch 17/20
243/243 [==============================] - 0s 963us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0024
Epoch 18/20
243/243 [==============================] - 0s 897us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0024
Epoch 19/20
170/243 [===================>..........] - ETA: 0s - loss: 0.0019 - mae: 0.0381 - mse: 0.0019
Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0011860630474984646.
243/243 [==============================] - 0s 889us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0024
Epoch 20/20
243/243 [==============================] - 0s 896us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0019 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0012
>Saved ../trained_models/models_segments_overlap-new-network-sigmoid-adam-8IN-20HN_rmsprop_0.07590803843709379_30_16_20epochs/model_4.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
243/243 [==============================] - 1s 1ms/step - loss: 0.0108 - mae: 0.0425 - mse: 0.0029 - val_loss: 0.0059 - val_mae: 0.0398 - val_mse: 0.0022 - lr: 0.0759
Epoch 2/20
243/243 [==============================] - 0s 877us/step - loss: 0.0063 - mae: 0.0408 - mse: 0.0023 - val_loss: 0.0057 - val_mae: 0.0395 - val_mse: 0.0021 - lr: 0.0759
Epoch 3/20
243/243 [==============================] - 0s 900us/step - loss: 0.0067 - mae: 0.0425 - mse: 0.0027 - val_loss: 0.0055 - val_mae: 0.0387 - val_mse: 0.0020 - lr: 0.0759
Epoch 4/20
243/243 [==============================] - 0s 907us/step - loss: 0.0065 - mae: 0.0417 - mse: 0.0025 - val_loss: 0.0075 - val_mae: 0.0442 - val_mse: 0.0028 - lr: 0.0759
Epoch 5/20
243/243 [==============================] - 0s 890us/step - loss: 0.0065 - mae: 0.0420 - mse: 0.0026 - val_loss: 0.0054 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0759
Epoch 6/20
164/243 [===================>..........] - ETA: 0s - loss: 0.0067 - mae: 0.0420 - mse: 0.0027
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.03795401751995087.
243/243 [==============================] - 0s 891us/step - loss: 0.0066 - mae: 0.0418 - mse: 0.0026 - val_loss: 0.0055 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0759
Epoch 7/20
243/243 [==============================] - 0s 887us/step - loss: 0.0022 - mae: 0.0387 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0408 - val_mse: 0.0023 - lr: 0.0380
Epoch 8/20
243/243 [==============================] - 0s 900us/step - loss: 0.0021 - mae: 0.0386 - mse: 0.0020 - val_loss: 0.0023 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0380
Epoch 9/20
163/243 [===================>..........] - ETA: 0s - loss: 0.0021 - mae: 0.0380 - mse: 0.0020
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.018977008759975433.
243/243 [==============================] - 0s 889us/step - loss: 0.0021 - mae: 0.0386 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0380
Epoch 10/20
243/243 [==============================] - 0s 895us/step - loss: 0.0020 - mae: 0.0382 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0190
Epoch 11/20
243/243 [==============================] - 0s 882us/step - loss: 0.0020 - mae: 0.0382 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0190
Epoch 12/20
162/243 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0377 - mse: 0.0019
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.009488504379987717.
243/243 [==============================] - 0s 903us/step - loss: 0.0020 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0190
Epoch 13/20
243/243 [==============================] - 0s 907us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0095
Epoch 14/20
243/243 [==============================] - 0s 930us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0095
Epoch 15/20
163/243 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0384 - mse: 0.0020
Epoch 15: ReduceLROnPlateau reducing learning rate to 0.004744252189993858.
243/243 [==============================] - 0s 917us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0095
Epoch 16/20
243/243 [==============================] - 0s 913us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0047
Epoch 17/20
243/243 [==============================] - 0s 905us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0047
Epoch 18/20
241/243 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0379 - mse: 0.0019
Epoch 18: ReduceLROnPlateau reducing learning rate to 0.002372126094996929.
243/243 [==============================] - 0s 944us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0047
Epoch 19/20
243/243 [==============================] - 0s 919us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0024
Epoch 20/20
243/243 [==============================] - 0s 899us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0024
>Saved ../trained_models/models_segments_overlap-new-network-sigmoid-adam-8IN-20HN_rmsprop_0.07590803843709379_30_16_20epochs/model_5.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
243/243 [==============================] - 1s 2ms/step - loss: 0.0101 - mae: 0.0421 - mse: 0.0027 - val_loss: 0.0060 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0759
Epoch 2/20
243/243 [==============================] - 0s 1ms/step - loss: 0.0065 - mae: 0.0415 - mse: 0.0025 - val_loss: 0.0055 - val_mae: 0.0391 - val_mse: 0.0021 - lr: 0.0759
Epoch 3/20
243/243 [==============================] - 0s 1ms/step - loss: 0.0064 - mae: 0.0411 - mse: 0.0024 - val_loss: 0.0064 - val_mae: 0.0426 - val_mse: 0.0026 - lr: 0.0759
Epoch 4/20
215/243 [=========================>....] - ETA: 0s - loss: 0.0064 - mae: 0.0409 - mse: 0.0024
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.03795401751995087.
243/243 [==============================] - 0s 1ms/step - loss: 0.0063 - mae: 0.0409 - mse: 0.0024 - val_loss: 0.0055 - val_mae: 0.0389 - val_mse: 0.0021 - lr: 0.0759
Epoch 5/20
243/243 [==============================] - 0s 926us/step - loss: 0.0021 - mae: 0.0382 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0380
Epoch 6/20
243/243 [==============================] - 0s 889us/step - loss: 0.0022 - mae: 0.0386 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0394 - val_mse: 0.0021 - lr: 0.0380
Epoch 7/20
162/243 [===================>..........] - ETA: 0s - loss: 0.0021 - mae: 0.0384 - mse: 0.0020
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.018977008759975433.
243/243 [==============================] - 0s 910us/step - loss: 0.0021 - mae: 0.0386 - mse: 0.0020 - val_loss: 0.0020 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0380
Epoch 8/20
243/243 [==============================] - 0s 912us/step - loss: 0.0020 - mae: 0.0382 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0190
Epoch 9/20
243/243 [==============================] - 0s 904us/step - loss: 0.0020 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0388 - val_mse: 0.0020 - lr: 0.0190
Epoch 10/20
165/243 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0381 - mse: 0.0020
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.009488504379987717.
243/243 [==============================] - 0s 892us/step - loss: 0.0020 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0190
Epoch 11/20
243/243 [==============================] - 0s 910us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
Epoch 12/20
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
Epoch 13/20
165/243 [===================>..........] - ETA: 0s - loss: 0.0019 - mae: 0.0379 - mse: 0.0019
Epoch 13: ReduceLROnPlateau reducing learning rate to 0.004744252189993858.
243/243 [==============================] - 0s 892us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
Epoch 14/20
243/243 [==============================] - 0s 935us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0047
Epoch 15/20
243/243 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0047
Epoch 16/20
183/243 [=====================>........] - ETA: 0s - loss: 0.0019 - mae: 0.0382 - mse: 0.0019
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.002372126094996929.
243/243 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0047
Epoch 17/20
243/243 [==============================] - 0s 948us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0019 - lr: 0.0024
Epoch 18/20
243/243 [==============================] - 0s 904us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0024
Epoch 19/20
168/243 [===================>..........] - ETA: 0s - loss: 0.0019 - mae: 0.0380 - mse: 0.0019
Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0011860630474984646.
243/243 [==============================] - 0s 883us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0024
Epoch 20/20
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0012
>Saved ../trained_models/models_segments_overlap-new-network-sigmoid-adam-8IN-20HN_rmsprop_0.07590803843709379_30_16_20epochs/model_6.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0008s vs `on_train_batch_end` time: 0.0011s). Check your callbacks.
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095
243/243 [==============================] - 0s 906us/step - loss: 0.0019 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0095