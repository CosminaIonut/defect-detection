Epoch 1/200
236/243 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
243/243 [==============================] - 2s 8ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0135
Epoch 2/200
241/243 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.8580
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.006754588335752487.
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0135
Epoch 3/200
242/243 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0033772941678762436.
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0068
Epoch 4/200
232/243 [===========================>..] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0016886470839381218.
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0034
Epoch 5/200
235/243 [============================>.] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0008443235419690609.
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0017
Epoch 6/200
241/243 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00042216177098453045.
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.4432e-04
Epoch 7/200
233/243 [===========================>..] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.8578
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00021108088549226522.
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2216e-04
Epoch 8/200
242/243 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010554044274613261.
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1108e-04
Epoch 9/200
237/243 [============================>.] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 9: ReduceLROnPlateau reducing learning rate to 5.2770221373066306e-05.
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0554e-04
Epoch 10/200
217/243 [=========================>....] - ETA: 0s - loss: 0.8573 - mae: 0.9249 - mse: 0.8573
Epoch 10: ReduceLROnPlateau reducing learning rate to 2.6385110686533153e-05.
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.2770e-05
Epoch 11/200
226/243 [==========================>...] - ETA: 0s - loss: 0.8584 - mae: 0.9255 - mse: 0.8584
Epoch 11: ReduceLROnPlateau reducing learning rate to 1.3192555343266577e-05.
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.6385e-05
Epoch 12/200
234/243 [===========================>..] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
Epoch 12: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.3193e-05
Epoch 13/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 14/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 15/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 17/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 18/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 20/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 22/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 24/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 28/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 30/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 32/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 33/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 35/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 37/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 38/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 39/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 41/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 42/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 43/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 44/200
229/243 [===========================>..] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/200
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/200
149/243 [=================>............] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 49/200
 88/243 [=========>....................] - ETA: 0s - loss: 0.8591 - mae: 0.9258 - mse: 0.8591
215/243 [=========================>....] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 16/243 [>.............................] - ETA: 0s - loss: 0.8559 - mae: 0.9240 - mse: 0.85598579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 16/243 [>.............................] - ETA: 0s - loss: 0.8659 - mae: 0.9296 - mse: 0.86598579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
234/243 [===========================>..] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 65/243 [=======>......................] - ETA: 0s - loss: 0.8512 - mae: 0.9216 - mse: 0.85128579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 18/243 [=>............................] - ETA: 0s - loss: 0.8460 - mae: 0.9187 - mse: 0.84608579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
237/243 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
145/243 [================>.............] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 85/243 [=========>....................] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 95/243 [==========>...................] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
233/243 [===========================>..] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
146/243 [=================>............] - ETA: 0s - loss: 0.8591 - mae: 0.9258 - mse: 0.85918579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
192/243 [======================>.......] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
  1/243 [..............................] - ETA: 0s - loss: 0.8824 - mae: 0.9385 - mse: 0.88248579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 34/243 [===>..........................] - ETA: 0s - loss: 0.8564 - mae: 0.9244 - mse: 0.85648579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
229/243 [===========================>..] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.85748579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
183/243 [=====================>........] - ETA: 0s - loss: 0.8588 - mae: 0.9257 - mse: 0.85888579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 59/243 [======>.......................] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_213316-fn0klk8c\files\model-best)...

323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0017e-05
310/323 [===========================>..] - ETA: 0s - loss: 0.6429 - mae: 0.7997 - mse: 0.64296431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0017e-05
323/323 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.1108e-04
309/323 [===========================>..] - ETA: 0s - loss: 0.6433 - mae: 0.7999 - mse: 0.64336431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.1108e-04
193/323 [================>.............] - ETA: 0s - loss: 0.6439 - mae: 0.8003 - mse: 0.64396431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.6385e-05
323/323 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
314/323 [============================>.] - ETA: 0s - loss: 0.6434 - mae: 0.8000 - mse: 0.64346431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
 57/323 [====>.........................] - ETA: 0s - loss: 0.6453 - mae: 0.8013 - mse: 0.64536431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
 48/323 [===>..........................] - ETA: 0s - loss: 0.6470 - mae: 0.8022 - mse: 0.64706431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
 38/323 [==>...........................] - ETA: 0s - loss: 0.6490 - mae: 0.8035 - mse: 0.64906431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
304/323 [===========================>..] - ETA: 0s - loss: 0.6435 - mae: 0.8000 - mse: 0.64356431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
148/323 [============>.................] - ETA: 0s - loss: 0.6417 - mae: 0.7989 - mse: 0.64176431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
 16/323 [>.............................] - ETA: 1s - loss: 0.6514 - mae: 0.8052 - mse: 0.65146431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
222/323 [===================>..........] - ETA: 0s - loss: 0.6441 - mae: 0.8004 - mse: 0.64416431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 59/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 61/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 63/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 65/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 67/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 69/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 71/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 73/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 73/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 76/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 78/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 82/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 84/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 88/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 88/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 91/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 91/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 95/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/200===========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 104/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 106/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 108/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 108/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 111/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 113/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 115/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 117/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 119/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 121/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 121/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 124/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 126/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 128/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 130/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 132/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 134/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 136/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 138/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 140/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 142/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 144/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 151/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 153/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 155/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 157/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 159/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 161/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 163/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 165/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 167/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 169/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 171/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 173/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 175/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 177/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 179/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 182/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 184/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 186/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 188/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 188/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 194/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 196/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 198/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 1/20000==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 1/20000==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 1/20000==========================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0008443235419690609.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 7/200duceLROnPlateau reducing learning rate to 0.0008443235419690609.: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 10/200uceLROnPlateau reducing learning rate to 0.00010554044274613261. 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 13/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 13/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 16/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 22/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 25/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 28/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 28/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 31/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 43/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 49/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 52/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 55/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 55/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 76/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 78/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 83/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 109/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 132/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 137/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 140/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 145/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 151/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 154/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 156/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 158/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 161/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 163/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 166/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 168/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 172/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 177/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 180/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 182/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 187/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 189/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 193/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 196/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.01350917701035953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.01350917701035953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.01350917701035953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0008443235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 7/200duceLROnPlateau reducing learning rate to 0.0008443235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 10/200uceLROnPlateau reducing learning rate to 0.00010554044274613261.5953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 13/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 16/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 16/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 22/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 25/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 28/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 28/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 31/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 40/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 43/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 49/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 52/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 55/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 76/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 91/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 94/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 106/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 109/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 119/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 131/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 137/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 155/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 161/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 164/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 176/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 182/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 191/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 197/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 197/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
Epoch 200/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 5/200duceLROnPlateau reducing learning rate to 0.0033772941678762436.5.953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 8/200duceLROnPlateau reducing learning rate to 0.00042216177098453045..953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 11/200uceLROnPlateau reducing learning rate to 5.2770221373066306e-05..953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 12: ReduceLROnPlateau reducing learning rate to 1e-05.21373066306e-05..953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/200duceLROnPlateau reducing learning rate to 1e-05.21373066306e-05..953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 18/200duceLROnPlateau reducing learning rate to 1e-05.21373066306e-05..953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 21/200duceLROnPlateau reducing learning rate to 1e-05.21373066306e-05..953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/200duceLROnPlateau reducing learning rate to 1e-05.21373066306e-05..953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 28/200
181/243 [=====================>........] - ETA: 0s - loss: 0.2281 - mae: 0.4756 - mse: 0.2281
Epoch 33/200duceLROnPlateau reducing learning rate to 1e-05.21373066306e-05..953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 31/200
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 32/200
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 33/200duceLROnPlateau reducing learning rate to 1e-05.21373066306e-05..953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_3.h5val_mse: 0.6442 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 34/200
Epoch 39/200===========================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 35/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 36/200
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 37/200
243/243 [==============================] - 1s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 38/200
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 39/200===========================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 40/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 41/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 42/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 43/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 44/200
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 45/200
241/243 [============================>.] - ETA: 0s - loss: 0.2277 - mae: 0.4752 - mse: 0.22772277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 46/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 47/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 48/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 49/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 50/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 51/200
  1/243 [..............................] - ETA: 0s - loss: 0.2137 - mae: 0.4609 - mse: 0.21372277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 52/200
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 53/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 54/200
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 55/200
243/243 [==============================] - 1s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 56/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 57/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 58/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 59/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 60/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 61/200
120/243 [=============>................] - ETA: 0s - loss: 0.2272 - mae: 0.4747 - mse: 0.2272
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 64/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 65/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 66/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 67/200
 28/243 [==>...........................] - ETA: 0s - loss: 0.2269 - mae: 0.4743 - mse: 0.2269
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 70/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 71/200
243/243 [==============================] - 1s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 72/200
143/243 [================>.............] - ETA: 0s - loss: 0.2277 - mae: 0.4751 - mse: 0.2277
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 76/200
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 77/200
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 78/200
 53/243 [=====>........................] - ETA: 0s - loss: 0.2285 - mae: 0.4761 - mse: 0.2285
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 82/200
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 83/200
  1/243 [..............................] - ETA: 1s - loss: 0.2308 - mae: 0.4787 - mse: 0.2308
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
Epoch 88/200
 44/243 [====>.........................] - ETA: 0s - loss: 0.2292 - mae: 0.4767 - mse: 0.2292
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
111/243 [============>.................] - ETA: 0s - loss: 0.2257 - mae: 0.4731 - mse: 0.22572277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
180/243 [=====================>........] - ETA: 0s - loss: 0.2273 - mae: 0.4747 - mse: 0.22732277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
221/243 [==========================>...] - ETA: 0s - loss: 0.2276 - mae: 0.4750 - mse: 0.22762277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
175/243 [====================>.........] - ETA: 0s - loss: 0.2286 - mae: 0.4761 - mse: 0.22862277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.2277 - mae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.4752 - mse: 0.2277 - val_loss: 0.2272 - val_mae: 0.4746 - val_mse: 0.2272 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 10: ReduceLROnPlateau reducing learning rate to 2.6385110686533153e-05.
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 5.2770e-05
Epoch 11/200
230/243 [===========================>..] - ETA: 0s - loss: 0.1427 - mae: 0.3752 - mse: 0.1427
Epoch 11: ReduceLROnPlateau reducing learning rate to 1.3192555343266577e-05.
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 2.6385e-05
Epoch 12/200
210/243 [========================>.....] - ETA: 0s - loss: 0.1422 - mae: 0.3746 - mse: 0.1422
Epoch 12: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.3193e-05
Epoch 13/200
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 14/200
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 15/200
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 16/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 17/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 18/200
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 19/200
 65/243 [=======>......................] - ETA: 0s - loss: 0.1436 - mae: 0.3764 - mse: 0.1436
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 22/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 23/200
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 24/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 29/200
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 30/200
110/243 [============>.................] - ETA: 0s - loss: 0.1428 - mae: 0.3753 - mse: 0.1428
 80/243 [========>.....................] - ETA: 0s - loss: 0.1430 - mae: 0.3756 - mse: 0.14301427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
128/243 [==============>...............] - ETA: 0s - loss: 0.1421 - mae: 0.3745 - mse: 0.14211427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
 43/243 [====>.........................] - ETA: 0s - loss: 0.1435 - mae: 0.3763 - mse: 0.14351427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
158/243 [==================>...........] - ETA: 0s - loss: 0.1422 - mae: 0.3746 - mse: 0.14221427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 159/200==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 165/200==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 170/200==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 176/200==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 180/200==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 183/200==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 189/200==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
Epoch 195/200==========================] - 0s 2ms/step - loss: 0.1427 - mae: 0.3752 - mse: 0.1427 - val_loss: 0.1423 - val_mae: 0.3746 - val_mse: 0.1423 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.01350917701035953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.01350917701035953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9/200duceLROnPlateau reducing learning rate to 0.0008443235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 12: ReduceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 12: ReduceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 30/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 34/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 42/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 55/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 59/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 63/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 68/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 76/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 84/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 109/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 127/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 134/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 146/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 151/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 155/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 160/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 165/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 169/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 174/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 178/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 182/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 189/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 193/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 197/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
223/243 [==========================>...] - ETA: 0s - loss: 0.0326 - mae: 0.1751 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
223/243 [==========================>...] - ETA: 0s - loss: 0.0326 - mae: 0.1751 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0008443235419690609.1751 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 9/200duceLROnPlateau reducing learning rate to 0.0008443235419690609.1751 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 9/200duceLROnPlateau reducing learning rate to 0.0008443235419690609.1751 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 11: ReduceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 14/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 17/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 20/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 21/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 24/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 29/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 31/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 33/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 36/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 38/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 41/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 43/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 48/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 53/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 55/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 64/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 67/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 72/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 75/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 78/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 83/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 86/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 91/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 99/200duceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 116/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 123/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 126/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 128/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 131/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 138/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 140/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 144/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 147/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 152/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 154/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 157/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 159/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 161/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 164/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 166/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 168/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 171/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 173/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 178/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 180/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 182/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 185/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 187/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 190/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 192/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 194/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 196/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 1.3192555343266577e-05.51 - mse: 0.0326  NNI_16BS_1P_val_lossM_200epochs/model_6.h5val_mse: 0.1423 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.01350917701035953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.01350917701035953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_rmsprop_0.01350917701035953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 19/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 19/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 26/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 33/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 35/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 37/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 39/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 41/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 44/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 46/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 48/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 51/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 53/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 58/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 60/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 63/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 68/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 71/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 76/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 78/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 80/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 82/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 87/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 90/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 95/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 97/200duceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 100/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 103/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 107/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 109/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 111/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 117/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 119/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 121/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 124/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 126/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 129/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 131/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 134/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 139/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 144/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 150/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 156/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 162/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 166/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 171/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 177/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 183/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 188/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 193/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05
Epoch 199/200uceLROnPlateau reducing learning rate to 1e-05.43235419690609.35953LR_[516]CHN_50CNNI_16BS_1P_val_lossM_200epochs/model_8.h5val_mse: 0.1423 - lr: 1.0000e-05