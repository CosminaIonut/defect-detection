Epoch 1/150
167/243 [===================>..........] - ETA: 0s - loss: 0.8563 - mae: 0.9244 - mse: 0.8563
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
243/243 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0301
Epoch 2/150
184/243 [=====================>........] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
Epoch 2: ReduceLROnPlateau reducing learning rate to 0.015042516402900219.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0301
Epoch 3/150
210/243 [========================>.....] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0075212582014501095.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0150
Epoch 4/150
210/243 [========================>.....] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0037606291007250547.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0075
Epoch 5/150
228/243 [===========================>..] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0018803145503625274.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0038
Epoch 6/150
188/243 [======================>.......] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0009401572751812637.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0019
Epoch 7/150
232/243 [===========================>..] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00047007863759063184.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 9.4016e-04
Epoch 8/150
199/243 [=======================>......] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.8578
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00023503931879531592.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.7008e-04
Epoch 9/150
200/243 [=======================>......] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00011751965939765796.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.3504e-04
Epoch 10/150
213/243 [=========================>....] - ETA: 0s - loss: 0.8569 - mae: 0.9247 - mse: 0.8569
Epoch 10: ReduceLROnPlateau reducing learning rate to 5.875982969882898e-05.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.1752e-04
Epoch 11/150
194/243 [======================>.......] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
Epoch 11: ReduceLROnPlateau reducing learning rate to 2.937991484941449e-05.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.8760e-05
Epoch 12/150
201/243 [=======================>......] - ETA: 0s - loss: 0.8589 - mae: 0.9258 - mse: 0.8589
Epoch 12: ReduceLROnPlateau reducing learning rate to 1.4689957424707245e-05.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.9380e-05
Epoch 13/150
236/243 [============================>.] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.4690e-05
Epoch 14/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 15/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 16/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 17/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 18/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 19/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 20/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 21/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 22/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 23/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 24/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 25/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 26/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 27/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 28/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 29/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 30/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 31/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 32/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 33/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 34/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 35/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 36/150
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 37/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 38/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 39/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 40/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 41/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 42/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 43/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 44/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 45/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 46/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 47/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 48/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 49/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 50/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 51/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 52/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 53/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 54/150
198/243 [=======================>......] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 56/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 57/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 58/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 59/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 60/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 61/150
  1/243 [..............................] - ETA: 0s - loss: 0.8622 - mae: 0.9275 - mse: 0.8622
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 63/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 64/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 65/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 66/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 67/150
207/243 [========================>.....] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 70/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 71/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 72/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 73/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 74/150
135/243 [===============>..............] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.8574
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 77/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 78/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 79/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 80/150
153/243 [=================>............] - ETA: 0s - loss: 0.8571 - mae: 0.9248 - mse: 0.8571
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 84/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 85/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 86/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 87/150
  1/243 [..............................] - ETA: 0s - loss: 0.8053 - mae: 0.8964 - mse: 0.8053
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 91/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 92/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 93/150
153/243 [=================>............] - ETA: 0s - loss: 0.8592 - mae: 0.9259 - mse: 0.8592
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 98/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 99/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 100/150
 40/243 [===>..........................] - ETA: 0s - loss: 0.8589 - mae: 0.9257 - mse: 0.8589
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 105/150
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 106/150
136/243 [===============>..............] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.8574
146/243 [=================>............] - ETA: 0s - loss: 0.8584 - mae: 0.9254 - mse: 0.85848579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
 48/243 [====>.........................] - ETA: 0s - loss: 0.8594 - mae: 0.9260 - mse: 0.85948579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 0s 1ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
318/323 [============================>.] - ETA: 0s - loss: 0.6430 - mae: 0.7997 - mse: 0.64306431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0038e-05
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00011751965939765796.
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.3504e-04
Epoch 10/150
246/323 [=====================>........] - ETA: 0s - loss: 0.6432 - mae: 0.7999 - mse: 0.6432
Epoch 10: ReduceLROnPlateau reducing learning rate to 5.875982969882898e-05.
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.1752e-04
Epoch 11/150
305/323 [===========================>..] - ETA: 0s - loss: 0.6431 - mae: 0.7998 - mse: 0.6431
Epoch 11: ReduceLROnPlateau reducing learning rate to 2.937991484941449e-05.
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 5.8760e-05
Epoch 12/150
209/323 [==================>...........] - ETA: 0s - loss: 0.6414 - mae: 0.7988 - mse: 0.6414
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.9380e-05
Epoch 13/150
291/323 [==========================>...] - ETA: 0s - loss: 0.6431 - mae: 0.7998 - mse: 0.6431
Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.4690e-05
Epoch 14/150
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/150
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 16/150
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 17/150
308/323 [===========================>..] - ETA: 0s - loss: 0.6431 - mae: 0.7998 - mse: 0.6431
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 19/150
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/150
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 21/150
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 22/150
299/323 [==========================>...] - ETA: 0s - loss: 0.6431 - mae: 0.7998 - mse: 0.6431
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 26/150
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/150
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 28/150
 67/323 [=====>........................] - ETA: 0s - loss: 0.6386 - mae: 0.7971 - mse: 0.6386
 69/323 [=====>........................] - ETA: 0s - loss: 0.6390 - mae: 0.7973 - mse: 0.63906431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/150
138/323 [===========>..................] - ETA: 0s - loss: 0.6444 - mae: 0.8007 - mse: 0.6444
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
300/323 [==========================>...] - ETA: 0s - loss: 0.6428 - mae: 0.7996 - mse: 0.64286431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
148/323 [============>.................] - ETA: 0s - loss: 0.6434 - mae: 0.8000 - mse: 0.64346431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
323/323 [==============================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/150==========================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/150==========================] - 0s 1ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 12: ReduceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 18/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 18/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 25/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 32/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 39/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 46/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 53/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 60/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 67/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 74/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 81/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 88/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 95/150duceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 102/150uceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 102/150uceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 109/150uceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 116/150uceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 123/150uceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 130/150uceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 137/150uceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 144/150uceLROnPlateau reducing learning rate to 1.4689957424707245e-05.0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.030085032066700212LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.030085032066700212LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11: ReduceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 16/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 23/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 30/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 37/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 44/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 51/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 58/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 65/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 72/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 79/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 86/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 93/150duceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 100/150uceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 107/150uceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 114/150uceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 121/150uceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 128/150uceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 135/150uceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 142/150uceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 142/150uceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/150uceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 149/150uceLROnPlateau reducing learning rate to 2.937991484941449e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 10: ReduceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 14/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 21/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 28/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 35/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 42/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 49/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 56/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 63/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 70/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 77/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 77/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 84/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 91/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 98/150duceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 105/150uceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 112/150uceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 119/150uceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 126/150uceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 133/150uceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 140/150uceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 147/150uceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 147/150uceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 147/150uceLROnPlateau reducing learning rate to 5.875982969882898e-05.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 19/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 19/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 26/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 33/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 40/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 47/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 54/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 61/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 68/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 75/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 82/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 89/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 96/150uceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 103/150ceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 103/150ceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 110/150ceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 117/150ceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 124/150ceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 131/150ceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 138/150ceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 145/150ceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 1/15050ceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 1/15050ceLROnPlateau reducing learning rate to 0.00011751965939765796.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 12/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 17/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 24/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 24/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 31/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 38/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 45/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 52/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 59/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 66/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 73/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 80/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 87/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 94/150uceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 101/150ceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 108/150ceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 108/150ceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 115/150ceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 122/150ceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 129/150ceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 136/150ceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 143/150ceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/150ceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 150/150ceLROnPlateau reducing learning rate to 0.00023503931879531592.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 11/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 15/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 22/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 29/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 36/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 43/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 43/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 50/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 57/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 64/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 71/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 78/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 85/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 92/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 99/150uceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 106/150ceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 113/150ceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 113/150ceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 120/150ceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 127/150ceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 134/150ceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 141/150ceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 148/150ceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 148/150ceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 148/150ceLROnPlateau reducing learning rate to 0.00047007863759063184.12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 20/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 27/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 34/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 41/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 41/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 48/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 55/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 62/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 69/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 76/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 76/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 83/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 90/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 97/150duceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 104/150uceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 111/150uceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 118/150uceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 125/150uceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 132/150uceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 139/150uceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/150uceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/150uceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05
Epoch 146/150uceLROnPlateau reducing learning rate to 1e-05.01572751812637..12LR_[24]CHN_50CNNI_16BS_1P_val_lossM_150epochs/model_3.h5 - val_mse: 0.6442 - lr: 1.0000e-05