Epoch 1/150
90/98 [==========================>...] - ETA: 0s - loss: 0.4244 - mae: 0.0837 - mse: 0.0167
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 2s 14ms/step - loss: 0.4045 - mae: 0.0811 - mse: 0.0157 - val_loss: 0.1383 - val_mae: 0.0491 - val_mse: 0.0030 - lr: 0.0906
Epoch 2/150
87/98 [=========================>....] - ETA: 0s - loss: 0.0715 - mae: 0.0479 - mse: 0.0029
98/98 [==============================] - 1s 13ms/step - loss: 0.0671 - mae: 0.0479 - mse: 0.0029 - val_loss: 0.0265 - val_mae: 0.0473 - val_mse: 0.0028 - lr: 0.0906
Epoch 3/150
67/98 [===================>..........] - ETA: 0s - loss: 0.0173 - mae: 0.0459 - mse: 0.0026
98/98 [==============================] - 1s 13ms/step - loss: 0.0149 - mae: 0.0457 - mse: 0.0026 - val_loss: 0.0082 - val_mae: 0.0453 - val_mse: 0.0026 - lr: 0.0906
Epoch 4/150
98/98 [==============================] - 1s 13ms/step - loss: 0.0060 - mae: 0.0439 - mse: 0.0024 - val_loss: 0.0047 - val_mae: 0.0437 - val_mse: 0.0024 - lr: 0.0906
Epoch 5/150
98/98 [==============================] - ETA: 0s - loss: 0.0042 - mae: 0.0427 - mse: 0.0023
98/98 [==============================] - 1s 13ms/step - loss: 0.0042 - mae: 0.0427 - mse: 0.0023 - val_loss: 0.0038 - val_mae: 0.0427 - val_mse: 0.0023 - lr: 0.0906
Epoch 6/150
86/98 [=========================>....] - ETA: 0s - loss: 0.0036 - mae: 0.0420 - mse: 0.0022
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 2s 17ms/step - loss: 0.0035 - mae: 0.0418 - mse: 0.0022 - val_loss: 0.0034 - val_mae: 0.0419 - val_mse: 0.0022 - lr: 0.0906
Epoch 7/150
83/98 [========================>.....] - ETA: 0s - loss: 0.0032 - mae: 0.0414 - mse: 0.0022
98/98 [==============================] - 1s 13ms/step - loss: 0.0032 - mae: 0.0412 - mse: 0.0022 - val_loss: 0.0031 - val_mae: 0.0414 - val_mse: 0.0022 - lr: 0.0906
Epoch 8/150
58/98 [================>.............] - ETA: 0s - loss: 0.0031 - mae: 0.0411 - mse: 0.0021
98/98 [==============================] - 1s 11ms/step - loss: 0.0030 - mae: 0.0407 - mse: 0.0021 - val_loss: 0.0030 - val_mae: 0.0412 - val_mse: 0.0022 - lr: 0.0906
Epoch 9/150
98/98 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0403 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0407 - val_mse: 0.0021 - lr: 0.0906
Epoch 10/150
85/98 [=========================>....] - ETA: 0s - loss: 0.0027 - mae: 0.0401 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 15ms/step - loss: 0.0027 - mae: 0.0400 - mse: 0.0021 - val_loss: 0.0027 - val_mae: 0.0404 - val_mse: 0.0021 - lr: 0.0906
Epoch 11/150
82/98 [========================>.....] - ETA: 0s - loss: 0.0026 - mae: 0.0399 - mse: 0.0020
98/98 [==============================] - 1s 13ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0906
Epoch 12/150
96/98 [============================>.] - ETA: 0s - loss: 0.0025 - mae: 0.0395 - mse: 0.0020
98/98 [==============================] - 1s 14ms/step - loss: 0.0025 - mae: 0.0395 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0399 - val_mse: 0.0021 - lr: 0.0906
Epoch 13/150
98/98 [==============================] - 1s 12ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 0.0906
Epoch 14/150
88/98 [=========================>....] - ETA: 0s - loss: 0.0024 - mae: 0.0390 - mse: 0.0020
98/98 [==============================] - 1s 15ms/step - loss: 0.0024 - mae: 0.0391 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 0.0906
Epoch 15/150
89/98 [==========================>...] - ETA: 0s - loss: 0.0023 - mae: 0.0390 - mse: 0.0020
98/98 [==============================] - 2s 17ms/step - loss: 0.0023 - mae: 0.0390 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0394 - val_mse: 0.0020 - lr: 0.0906
Epoch 16/150
92/98 [===========================>..] - ETA: 0s - loss: 0.0023 - mae: 0.0389 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 12ms/step - loss: 0.0023 - mae: 0.0389 - mse: 0.0020 - val_loss: 0.0023 - val_mae: 0.0392 - val_mse: 0.0020 - lr: 0.0906
Epoch 17/150
81/98 [=======================>......] - ETA: 0s - loss: 0.0022 - mae: 0.0385 - mse: 0.0020
98/98 [==============================] - 1s 12ms/step - loss: 0.0022 - mae: 0.0387 - mse: 0.0020 - val_loss: 0.0023 - val_mae: 0.0393 - val_mse: 0.0020 - lr: 0.0906
Epoch 18/150
73/98 [=====================>........] - ETA: 0s - loss: 0.0022 - mae: 0.0388 - mse: 0.0020
98/98 [==============================] - 1s 13ms/step - loss: 0.0022 - mae: 0.0386 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0391 - val_mse: 0.0020 - lr: 0.0906
Epoch 19/150
85/98 [=========================>....] - ETA: 0s - loss: 0.0022 - mae: 0.0386 - mse: 0.0020
98/98 [==============================] - 1s 11ms/step - loss: 0.0022 - mae: 0.0385 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0389 - val_mse: 0.0020 - lr: 0.0906
Epoch 20/150
98/98 [==============================] - 1s 11ms/step - loss: 0.0022 - mae: 0.0384 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0386 - val_mse: 0.0020 - lr: 0.0906
Epoch 21/150
76/98 [======================>.......] - ETA: 0s - loss: 0.0021 - mae: 0.0382 - mse: 0.0020
98/98 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0383 - mse: 0.0020 - val_loss: 0.0022 - val_mae: 0.0387 - val_mse: 0.0020 - lr: 0.0906
Epoch 22/150
68/98 [===================>..........] - ETA: 0s - loss: 0.0021 - mae: 0.0386 - mse: 0.0020
98/98 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0382 - mse: 0.0019 - val_loss: 0.0022 - val_mae: 0.0386 - val_mse: 0.0020 - lr: 0.0906
Epoch 23/150
58/98 [================>.............] - ETA: 0s - loss: 0.0021 - mae: 0.0379 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0906
Epoch 24/150
98/98 [==============================] - ETA: 0s - loss: 0.0021 - mae: 0.0380 - mse: 0.0019
98/98 [==============================] - 2s 16ms/step - loss: 0.0021 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0906
Epoch 25/150
98/98 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0379 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0906
Epoch 26/150
81/98 [=======================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0378 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 11ms/step - loss: 0.0021 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0906
Epoch 27/150
82/98 [========================>.....] - ETA: 0s - loss: 0.0021 - mae: 0.0379 - mse: 0.0019
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0378 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0906
Epoch 28/150
98/98 [==============================] - ETA: 0s - loss: 0.0020 - mae: 0.0378 - mse: 0.0019
Epoch 28: ReduceLROnPlateau reducing learning rate to 0.04532407969236374.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0378 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0383 - val_mse: 0.0020 - lr: 0.0906
Epoch 29/150
88/98 [=========================>....] - ETA: 0s - loss: 0.0020 - mae: 0.0379 - mse: 0.0019
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0378 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0382 - val_mse: 0.0020 - lr: 0.0453
Epoch 30/150
83/98 [========================>.....] - ETA: 0s - loss: 0.0020 - mae: 0.0379 - mse: 0.0019
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0378 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0381 - val_mse: 0.0020 - lr: 0.0453
Epoch 31/150
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0377 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0381 - val_mse: 0.0020 - lr: 0.0453
Epoch 32/150
81/98 [=======================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0377 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0377 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0381 - val_mse: 0.0020 - lr: 0.0453
Epoch 33/150
77/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0378 - mse: 0.0019
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0377 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0381 - val_mse: 0.0020 - lr: 0.0453
Epoch 34/150
78/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019
98/98 [==============================] - 2s 15ms/step - loss: 0.0020 - mae: 0.0377 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0381 - val_mse: 0.0020 - lr: 0.0453
Epoch 35/150
76/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0378 - mse: 0.0019
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0377 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0453
Epoch 36/150
84/98 [========================>.....] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0381 - val_mse: 0.0020 - lr: 0.0453
Epoch 37/150
98/98 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0381 - val_mse: 0.0020 - lr: 0.0453
Epoch 38/150
97/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019
Epoch 38: ReduceLROnPlateau reducing learning rate to 0.02266203984618187.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0381 - val_mse: 0.0020 - lr: 0.0453
Epoch 39/150
78/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0227
Epoch 40/150
72/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0227
Epoch 41/150
68/98 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0227
Epoch 42/150
89/98 [==========================>...] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0227
Epoch 43/150
98/98 [==============================] - 2s 16ms/step - loss: 0.0020 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0227
Epoch 44/150
78/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0376 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0227
Epoch 45/150
66/98 [===================>..........] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0227
Epoch 46/150
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0380 - val_mse: 0.0020 - lr: 0.0227
Epoch 47/150
97/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0227
Epoch 48/150
77/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0377 - mse: 0.0019
Epoch 48: ReduceLROnPlateau reducing learning rate to 0.011331019923090935.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0227
Epoch 49/150
98/98 [==============================] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
Epoch 50/150
71/98 [====================>.........] - ETA: 0s - loss: 0.0020 - mae: 0.0373 - mse: 0.0019
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
Epoch 51/150=========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
75/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019
Epoch 51/150=========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
92/98 [===========================>..] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
92/98 [===========================>..] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
98/98 [==============================] - 1s 15ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
98/98 [==============================] - 1s 15ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
Epoch 54/150=========================] - 1s 15ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
74/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
74/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
Epoch 57/150=========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
Epoch 57/150=========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
85/98 [=========================>....] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
85/98 [=========================>....] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
Epoch 59/150====================>....] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
Epoch 59/150====================>....] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
97/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0113
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
Epoch 62/150=========================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
Epoch 62/150=========================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
81/98 [=======================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
81/98 [=======================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
Epoch 65/150=========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
Epoch 65/150=========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
96/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
96/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
Epoch 68/150=========================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
Epoch 70/150=========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
Epoch 70/150=========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
78/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
78/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0057
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 2s 16ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
98/98 [==============================] - 2s 16ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
Epoch 73/150=========================] - 2s 16ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
Epoch 73/150=========================] - 2s 16ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
74/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0373 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
Epoch 76/150=========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
Epoch 76/150=========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
76/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
76/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
Epoch 78: ReduceLROnPlateau reducing learning rate to 0.0014163774903863668.0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
73/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
73/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0028
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
Epoch 81/150=========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
Epoch 81/150=========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
95/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
Epoch 84/150=========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
Epoch 84/150=========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
77/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
77/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0376 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
Epoch 87/150=========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
Epoch 87/150=========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
76/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
Epoch 89/150=================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
Epoch 89/150=================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
74/98 [=====================>........] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
74/98 [=====================>........] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 0.0014
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
Epoch 92/150=========================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
Epoch 92/150=========================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
95/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
95/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
Epoch 95/150=========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
Epoch 95/150=========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
98/98 [==============================] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
98/98 [==============================] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
Epoch 98/150=========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
Epoch 98/150=========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
Epoch 100/150========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
76/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
76/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 7.0819e-04
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
Epoch 103/150========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
Epoch 103/150========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
76/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0373 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
76/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0373 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
Epoch 106/150========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
56/98 [================>.............] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
56/98 [================>.............] - ETA: 0s - loss: 0.0019 - mae: 0.0373 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
Epoch 108: ReduceLROnPlateau reducing learning rate to 0.00017704718629829586.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
Epoch 108: ReduceLROnPlateau reducing learning rate to 0.00017704718629829586.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
97/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
97/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 3.5409e-04
98/98 [==============================] - 1s 15ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
98/98 [==============================] - 1s 15ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
Epoch 111/150========================] - 1s 15ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
Epoch 111/150========================] - 1s 15ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
75/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
75/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
98/98 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
Epoch 114/150========================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
Epoch 114/150========================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
76/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
76/98 [======================>.......] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
98/98 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
Epoch 117/150========================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
Epoch 117/150========================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
79/98 [=======================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0373 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
79/98 [=======================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0373 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
Epoch 119/150=================>......] - ETA: 0s - loss: 0.0020 - mae: 0.0373 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
96/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
96/98 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 1.7705e-04
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
98/98 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
Epoch 122/150========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
Epoch 122/150========================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
94/98 [===========================>..] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
94/98 [===========================>..] - ETA: 0s - loss: 0.0020 - mae: 0.0375 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
Epoch 125/150========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
Epoch 125/150========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
74/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
74/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.0019.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
98/98 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
Epoch 128/150========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
Epoch 128/150========================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_123500-t8ovwmsm\files\model-best)... Done. 0.0s
Epoch 130/150========================] - 1s 15ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
Epoch 130/150========================] - 1s 15ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 8.8524e-05
98/98 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 4.4262e-055
Epoch 133/150========================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 4.4262e-055
Epoch 133/150========================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0374 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 4.4262e-055
74/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.00190019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 4.4262e-055
74/98 [=====================>........] - ETA: 0s - loss: 0.0020 - mae: 0.0374 - mse: 0.00190019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 4.4262e-055
Epoch 147/150========================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 2.2131e-055
Epoch 147/150========================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0379 - val_mse: 0.0020 - lr: 2.2131e-055
Epoch 15/150=====================>.....] - ETA: 0s - loss: 0.0101 - mae: 0.0907 - mse: 0.01010100 - val_loss: 0.0100 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0906055
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.04532407969236374.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0906
Epoch 15/150=====================>.....] - ETA: 0s - loss: 0.0101 - mae: 0.0907 - mse: 0.01010100 - val_loss: 0.0100 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 0.0906055
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0453
Epoch 16/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0453
Epoch 17/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0453
Epoch 18/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0453
Epoch 19/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0453
Epoch 20/150
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0453
Epoch 21/150
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0453
Epoch 22/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0453
Epoch 23/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0453
Epoch 24/150
128/130 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0905 - mse: 0.0100
Epoch 24: ReduceLROnPlateau reducing learning rate to 0.02266203984618187.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0453
Epoch 25/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0227
Epoch 26/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0227
Epoch 27/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0227
Epoch 28/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0227
Epoch 29/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0227
Epoch 30/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0227
Epoch 31/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0227
Epoch 32/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0227
Epoch 33/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0227
Epoch 34/150
101/130 [======================>.......] - ETA: 0s - loss: 0.0100 - mae: 0.0903 - mse: 0.0100
Epoch 34: ReduceLROnPlateau reducing learning rate to 0.011331019923090935.
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0227
Epoch 35/150
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0113
Epoch 36/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0113
Epoch 37/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0113
Epoch 38/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0113
Epoch 39/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0113
Epoch 40/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0113
Epoch 41/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0113
Epoch 42/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0113
Epoch 43/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0113
Epoch 44/150
123/130 [===========================>..] - ETA: 0s - loss: 0.0100 - mae: 0.0906 - mse: 0.0100
Epoch 44: ReduceLROnPlateau reducing learning rate to 0.005665509961545467.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0113
Epoch 45/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0057
Epoch 46/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0057
Epoch 47/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0057
Epoch 48/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0057
Epoch 49/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0057
Epoch 50/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0057
Epoch 51/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0057
Epoch 52/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0057
Epoch 53/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0057
Epoch 54/150
107/130 [=======================>......] - ETA: 0s - loss: 0.0101 - mae: 0.0906 - mse: 0.0101
Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0028327549807727337.
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0057
Epoch 55/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0028
Epoch 56/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0028
Epoch 57/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0028
Epoch 58/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0028
Epoch 59/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0028
Epoch 60/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0028
Epoch 61/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0028
Epoch 62/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0028
Epoch 63/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0028
Epoch 64/150
107/130 [=======================>......] - ETA: 0s - loss: 0.0100 - mae: 0.0905 - mse: 0.0100
Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0014163774903863668.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0028
Epoch 65/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0014
Epoch 66/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0906 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0014
Epoch 67/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0014
Epoch 68/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0014
Epoch 69/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0014
Epoch 70/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0014
Epoch 71/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0014
Epoch 72/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0014
Epoch 73/150
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0014
Epoch 74/150
117/130 [==========================>...] - ETA: 0s - loss: 0.0100 - mae: 0.0906 - mse: 0.0100
Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0007081887451931834.
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 0.0014
Epoch 75/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 7.0819e-04
Epoch 76/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 7.0819e-04
Epoch 77/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 7.0819e-04
Epoch 78/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 7.0819e-04
Epoch 79/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 7.0819e-04
Epoch 80/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 7.0819e-04
Epoch 81/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 7.0819e-04
Epoch 82/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 7.0819e-04
Epoch 83/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 7.0819e-04
Epoch 84/150
129/130 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0905 - mse: 0.0100
Epoch 84: ReduceLROnPlateau reducing learning rate to 0.0003540943725965917.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 7.0819e-04
Epoch 85/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 3.5409e-04
Epoch 86/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 3.5409e-04
Epoch 87/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 3.5409e-04
Epoch 88/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 3.5409e-04
Epoch 89/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 3.5409e-04
Epoch 90/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 3.5409e-04
Epoch 91/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 3.5409e-04
Epoch 92/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 3.5409e-04
Epoch 93/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 3.5409e-04
Epoch 94/150
121/130 [==========================>...] - ETA: 0s - loss: 0.0100 - mae: 0.0905 - mse: 0.0100
Epoch 94: ReduceLROnPlateau reducing learning rate to 0.00017704718629829586.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 3.5409e-04
Epoch 95/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.7705e-04
Epoch 96/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.7705e-04
Epoch 97/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.7705e-04
Epoch 98/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.7705e-04
Epoch 99/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.7705e-04
Epoch 100/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.7705e-04
Epoch 101/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.7705e-04
Epoch 102/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.7705e-04
Epoch 103/150
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.7705e-04
Epoch 104/150
127/130 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0905 - mse: 0.0100
Epoch 104: ReduceLROnPlateau reducing learning rate to 8.852359314914793e-05.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.7705e-04
Epoch 105/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 8.8524e-05
Epoch 106/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 8.8524e-05
Epoch 107/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 8.8524e-05
Epoch 108/150
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 8.8524e-05
Epoch 109/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 8.8524e-05
Epoch 110/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 8.8524e-05
Epoch 111/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 8.8524e-05
Epoch 112/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 8.8524e-05
Epoch 113/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 8.8524e-05
Epoch 114/150
101/130 [======================>.......] - ETA: 0s - loss: 0.0100 - mae: 0.0904 - mse: 0.0100
Epoch 114: ReduceLROnPlateau reducing learning rate to 4.4261796574573964e-05.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 8.8524e-05
Epoch 115/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 4.4262e-05
Epoch 116/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 4.4262e-05
Epoch 117/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 4.4262e-05
Epoch 118/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 4.4262e-05
Epoch 119/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 4.4262e-05
Epoch 120/150
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 4.4262e-05
Epoch 121/150
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 4.4262e-05
Epoch 122/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 4.4262e-05
Epoch 123/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 4.4262e-05
Epoch 124/150
129/130 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0905 - mse: 0.0100
Epoch 124: ReduceLROnPlateau reducing learning rate to 2.2130898287286982e-05.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 4.4262e-05
Epoch 125/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 2.2131e-05
Epoch 126/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 2.2131e-05
Epoch 127/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 2.2131e-05
Epoch 128/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 2.2131e-05
Epoch 129/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 2.2131e-05
Epoch 130/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 2.2131e-05
Epoch 131/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 2.2131e-05
Epoch 132/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 2.2131e-05
Epoch 133/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 2.2131e-05
Epoch 134/150
112/130 [========================>.....] - ETA: 0s - loss: 0.0100 - mae: 0.0902 - mse: 0.0100
Epoch 134: ReduceLROnPlateau reducing learning rate to 1.1065449143643491e-05.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 2.2131e-05
Epoch 135/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.1065e-05
Epoch 136/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.1065e-05
Epoch 137/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.1065e-05
Epoch 138/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.1065e-05
Epoch 139/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.1065e-05
Epoch 140/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.1065e-05
Epoch 141/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.1065e-05
Epoch 142/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.1065e-05
Epoch 143/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.1065e-05
Epoch 144/150
104/130 [=======================>......] - ETA: 0s - loss: 0.0100 - mae: 0.0904 - mse: 0.0100
Epoch 144: ReduceLROnPlateau reducing learning rate to 1e-05.
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.1065e-05
Epoch 145/150
130/130 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 146/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 147/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 148/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 149/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 150/150
130/130 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0905 - mse: 0.0100 - val_loss: 0.0100 - val_mae: 0.0905 - val_mse: 0.0100 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.09064815996608284LR_[36]HN_40BS_10P_val_mseM_150epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/150
98/98 [==============================] - 1s 4ms/step - loss: 0.4033 - mae: 0.1628 - mse: 0.0317 - val_loss: 0.1530 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0906
Epoch 2/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0845 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0458 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0906
Epoch 3/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0347 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0286 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0906
Epoch 4/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0265 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0257 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0906
Epoch 5/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0251 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0250 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0906
Epoch 6/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0247 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0906
Epoch 7/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0906
Epoch 8/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0906
Epoch 9/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0906
Epoch 10/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0906
Epoch 11/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0906
Epoch 12/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0906
Epoch 13/150
83/98 [========================>.....] - ETA: 0s - loss: 0.0245 - mae: 0.1526 - mse: 0.0245
Epoch 13: ReduceLROnPlateau reducing learning rate to 0.04532407969236374.
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0906
Epoch 14/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0453
Epoch 15/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0453
Epoch 16/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0453
Epoch 17/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0453
Epoch 18/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0453
Epoch 19/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0453
Epoch 20/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0453
Epoch 21/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0453
Epoch 22/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0453
Epoch 23/150
85/98 [=========================>....] - ETA: 0s - loss: 0.0244 - mae: 0.1525 - mse: 0.0244
Epoch 23: ReduceLROnPlateau reducing learning rate to 0.02266203984618187.
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0453
Epoch 24/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0227
Epoch 25/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0227
Epoch 26/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0227
Epoch 27/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0227
Epoch 28/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0227
Epoch 29/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0227
Epoch 30/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0227
Epoch 31/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0227
Epoch 32/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0227
Epoch 33/150
74/98 [=====================>........] - ETA: 0s - loss: 0.0244 - mae: 0.1526 - mse: 0.0244
Epoch 33: ReduceLROnPlateau reducing learning rate to 0.011331019923090935.
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0227
Epoch 34/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0113
Epoch 35/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0113
Epoch 36/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0113
Epoch 37/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0113
Epoch 38/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0113
Epoch 39/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0113
Epoch 40/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0113
Epoch 41/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0113
Epoch 42/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0113
Epoch 43/150
77/98 [======================>.......] - ETA: 0s - loss: 0.0244 - mae: 0.1525 - mse: 0.0244
Epoch 43: ReduceLROnPlateau reducing learning rate to 0.005665509961545467.
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0113
Epoch 44/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0057
Epoch 45/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0057
Epoch 46/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0057
Epoch 47/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0057
Epoch 48/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0057
Epoch 49/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0057
Epoch 50/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0057
Epoch 51/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0057
Epoch 52/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0057
Epoch 53/150
77/98 [======================>.......] - ETA: 0s - loss: 0.0244 - mae: 0.1524 - mse: 0.0244
Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0028327549807727337.
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0057
Epoch 54/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0028
Epoch 55/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0028
Epoch 56/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0028
Epoch 57/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0028
Epoch 58/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0028
Epoch 59/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0028
Epoch 60/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0028
Epoch 61/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0028
Epoch 62/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0028
Epoch 63/150
75/98 [=====================>........] - ETA: 0s - loss: 0.0245 - mae: 0.1527 - mse: 0.0245
Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0014163774903863668.
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0028
Epoch 64/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0014
Epoch 65/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0014
Epoch 66/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0014
Epoch 67/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0014
Epoch 68/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0014
Epoch 69/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0014
Epoch 70/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0014
Epoch 71/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0014
Epoch 72/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0014
Epoch 73/150
76/98 [======================>.......] - ETA: 0s - loss: 0.0244 - mae: 0.1525 - mse: 0.0244
Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0007081887451931834.
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0014
Epoch 74/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.0819e-04
Epoch 75/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.0819e-04
Epoch 76/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.0819e-04
Epoch 77/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.0819e-04
Epoch 78/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.0819e-04
Epoch 79/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.0819e-04
Epoch 80/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.0819e-04
Epoch 81/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.0819e-04
Epoch 82/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.0819e-04
Epoch 83/150
71/98 [====================>.........] - ETA: 0s - loss: 0.0244 - mae: 0.1523 - mse: 0.0244
Epoch 83: ReduceLROnPlateau reducing learning rate to 0.0003540943725965917.
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.0819e-04
Epoch 84/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5409e-04
Epoch 85/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5409e-04
Epoch 86/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5409e-04
Epoch 87/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5409e-04
Epoch 88/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5409e-04
Epoch 89/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5409e-04
Epoch 90/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5409e-04
Epoch 91/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5409e-04
Epoch 92/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5409e-04
Epoch 93/150
84/98 [========================>.....] - ETA: 0s - loss: 0.0244 - mae: 0.1526 - mse: 0.0244
Epoch 93: ReduceLROnPlateau reducing learning rate to 0.00017704718629829586.
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.5409e-04
Epoch 94/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7705e-04
Epoch 95/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7705e-04
Epoch 96/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7705e-04
Epoch 97/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7705e-04
Epoch 98/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7705e-04
Epoch 99/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7705e-04
Epoch 100/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7705e-04
Epoch 101/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7705e-04
Epoch 102/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7705e-04
Epoch 103/150
74/98 [=====================>........] - ETA: 0s - loss: 0.0245 - mae: 0.1529 - mse: 0.0245
Epoch 103: ReduceLROnPlateau reducing learning rate to 8.852359314914793e-05.
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.7705e-04
Epoch 104/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 8.8524e-05
Epoch 105/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 8.8524e-05
Epoch 106/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 8.8524e-05
Epoch 107/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 8.8524e-05
Epoch 108/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 8.8524e-05
Epoch 109/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 8.8524e-05
Epoch 110/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 8.8524e-05
Epoch 111/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 8.8524e-05
Epoch 112/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 8.8524e-05
Epoch 113/150
68/98 [===================>..........] - ETA: 0s - loss: 0.0243 - mae: 0.1522 - mse: 0.0243
Epoch 113: ReduceLROnPlateau reducing learning rate to 4.4261796574573964e-05.
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 8.8524e-05
Epoch 114/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 4.4262e-05
Epoch 115/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 4.4262e-05
Epoch 116/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 4.4262e-05
Epoch 117/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 4.4262e-05
Epoch 118/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 4.4262e-05
Epoch 119/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 4.4262e-05
Epoch 120/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 4.4262e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0018s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0020s). Check your callbacks.
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 145/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 146/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 147/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 148/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 149/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 150/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0244 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0246 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.09064815996608284LR_[36]HN_40BS_10P_val_mseM_150epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/150
98/98 [==============================] - 1s 5ms/step - loss: 0.4074 - mae: 0.2046 - mse: 0.0456 - val_loss: 0.1671 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0906
Epoch 2/150
98/98 [==============================] - 0s 2ms/step - loss: 0.1005 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0630 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 3/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0522 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0464 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 4/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0443 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0435 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 5/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0429 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0429 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 6/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 7/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 8/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 9/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 10/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 11/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 12/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 13/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 14/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 15/150
56/98 [================>.............] - ETA: 0s - loss: 0.0421 - mae: 0.2023 - mse: 0.0421
Epoch 15: ReduceLROnPlateau reducing learning rate to 0.04532407969236374.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0906
Epoch 16/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0453
Epoch 17/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0453
Epoch 18/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0453
Epoch 19/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0453
Epoch 20/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0453
Epoch 21/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0453
Epoch 22/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0453
Epoch 23/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0453
Epoch 24/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0453
Epoch 25/150
82/98 [========================>.....] - ETA: 0s - loss: 0.0422 - mae: 0.2026 - mse: 0.0422
Epoch 25: ReduceLROnPlateau reducing learning rate to 0.02266203984618187.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0453
Epoch 26/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0227
Epoch 27/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0227
Epoch 28/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0227
Epoch 29/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0227
Epoch 30/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0227
Epoch 31/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0227
Epoch 32/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0227
Epoch 33/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0227
Epoch 34/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0227
Epoch 35/150
55/98 [===============>..............] - ETA: 0s - loss: 0.0422 - mae: 0.2026 - mse: 0.0422
Epoch 35: ReduceLROnPlateau reducing learning rate to 0.011331019923090935.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0227
Epoch 36/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0113
Epoch 37/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0113
Epoch 38/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0113
Epoch 39/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0113
Epoch 40/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0113
Epoch 41/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0113
Epoch 42/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0113
Epoch 43/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0113
Epoch 44/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0113
Epoch 45/150
86/98 [=========================>....] - ETA: 0s - loss: 0.0422 - mae: 0.2026 - mse: 0.0422
Epoch 45: ReduceLROnPlateau reducing learning rate to 0.005665509961545467.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0113
Epoch 46/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0057
Epoch 47/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0057
Epoch 48/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0057
Epoch 49/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0057
Epoch 50/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0057
Epoch 51/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0057
Epoch 52/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0057
Epoch 53/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0057
Epoch 54/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0057
Epoch 55/150
91/98 [==========================>...] - ETA: 0s - loss: 0.0421 - mae: 0.2023 - mse: 0.0421
Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0028327549807727337.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0057
Epoch 56/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0028
Epoch 57/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0028
Epoch 58/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0028
Epoch 59/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0028
Epoch 60/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0028
Epoch 61/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0028
Epoch 62/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0028
Epoch 63/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0028
Epoch 64/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0028
Epoch 65/150
58/98 [================>.............] - ETA: 0s - loss: 0.0422 - mae: 0.2027 - mse: 0.0422
Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0014163774903863668.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0028
Epoch 66/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0014
Epoch 67/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0014
Epoch 68/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0014
Epoch 69/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0014
Epoch 70/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0014
Epoch 71/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0014
Epoch 72/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0014
Epoch 73/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0014
Epoch 74/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0014
Epoch 75/150
84/98 [========================>.....] - ETA: 0s - loss: 0.0421 - mae: 0.2024 - mse: 0.0421
Epoch 75: ReduceLROnPlateau reducing learning rate to 0.0007081887451931834.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0014
Epoch 76/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.0819e-04
Epoch 77/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.0819e-04
Epoch 78/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.0819e-04
Epoch 79/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.0819e-04
Epoch 80/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.0819e-04
Epoch 81/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.0819e-04
Epoch 82/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.0819e-04
Epoch 83/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.0819e-04
Epoch 84/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.0819e-04
Epoch 85/150
93/98 [===========================>..] - ETA: 0s - loss: 0.0422 - mae: 0.2027 - mse: 0.0422
Epoch 85: ReduceLROnPlateau reducing learning rate to 0.0003540943725965917.
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.0819e-04
Epoch 86/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5409e-04
Epoch 87/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5409e-04
Epoch 88/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5409e-04
Epoch 89/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5409e-04
Epoch 90/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5409e-04
Epoch 91/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5409e-04
Epoch 92/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5409e-04
Epoch 93/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5409e-04
Epoch 94/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5409e-04
Epoch 95/150
69/98 [====================>.........] - ETA: 0s - loss: 0.0421 - mae: 0.2025 - mse: 0.0421
Epoch 95: ReduceLROnPlateau reducing learning rate to 0.00017704718629829586.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.5409e-04
Epoch 96/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7705e-04
Epoch 97/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7705e-04
Epoch 98/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7705e-04
Epoch 99/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7705e-04
Epoch 100/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7705e-04
Epoch 101/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7705e-04
Epoch 102/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7705e-04
Epoch 103/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7705e-04
Epoch 104/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7705e-04
Epoch 105/150
66/98 [===================>..........] - ETA: 0s - loss: 0.0422 - mae: 0.2026 - mse: 0.0422
Epoch 105: ReduceLROnPlateau reducing learning rate to 8.852359314914793e-05.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.7705e-04
Epoch 106/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 8.8524e-05
Epoch 107/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 8.8524e-05
Epoch 108/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 8.8524e-05
Epoch 109/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 8.8524e-05
Epoch 110/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 8.8524e-05
Epoch 111/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 8.8524e-05
Epoch 112/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 8.8524e-05
Epoch 113/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 8.8524e-05
Epoch 114/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 8.8524e-05
Epoch 115/150
67/98 [===================>..........] - ETA: 0s - loss: 0.0421 - mae: 0.2023 - mse: 0.0421
Epoch 115: ReduceLROnPlateau reducing learning rate to 4.4261796574573964e-05.
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 8.8524e-05
Epoch 116/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 4.4262e-05
Epoch 117/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 4.4262e-05
Epoch 118/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 4.4262e-05
Epoch 119/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 4.4262e-05
Epoch 120/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 4.4262e-05
Epoch 121/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 4.4262e-05
Epoch 122/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 4.4262e-05
Epoch 123/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 4.4262e-05
Epoch 124/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 4.4262e-05
Epoch 125/150
80/98 [=======================>......] - ETA: 0s - loss: 0.0423 - mae: 0.2029 - mse: 0.0423
Epoch 125: ReduceLROnPlateau reducing learning rate to 2.2130898287286982e-05.
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 4.4262e-05
Epoch 126/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.2131e-05
Epoch 127/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.2131e-05
Epoch 128/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.2131e-05
Epoch 129/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.2131e-05
Epoch 130/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.2131e-05
Epoch 131/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.2131e-05
Epoch 132/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.2131e-05
Epoch 133/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.2131e-05
Epoch 134/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.2131e-05
Epoch 135/150
70/98 [====================>.........] - ETA: 0s - loss: 0.0421 - mae: 0.2024 - mse: 0.0421
Epoch 135: ReduceLROnPlateau reducing learning rate to 1.1065449143643491e-05.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 2.2131e-05
Epoch 136/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.1065e-05
Epoch 137/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.1065e-05
Epoch 138/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.1065e-05
Epoch 139/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.1065e-05
Epoch 140/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.1065e-05
Epoch 141/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.1065e-05
Epoch 142/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.1065e-05
Epoch 143/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.1065e-05
Epoch 144/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.1065e-05
Epoch 145/150
49/98 [==============>...............] - ETA: 0s - loss: 0.0423 - mae: 0.2029 - mse: 0.0423
Epoch 145: ReduceLROnPlateau reducing learning rate to 1e-05.
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.1065e-05
Epoch 146/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 147/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 148/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 149/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 150/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0422 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0425 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.09064815996608284LR_[36]HN_40BS_10P_val_mseM_150epochs/model_4.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/150
98/98 [==============================] - 1s 4ms/step - loss: 0.4696 - mae: 0.2528 - mse: 0.0676 - val_loss: 0.2032 - val_mae: 0.2532 - val_mse: 0.0654 - lr: 0.0906
Epoch 2/150
98/98 [==============================] - 0s 2ms/step - loss: 0.1291 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0875 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0906
Epoch 3/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0755 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0691 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0906
Epoch 4/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0669 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0661 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0906
Epoch 5/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0654 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0655 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0906
Epoch 6/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0906
Epoch 7/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0906
Epoch 8/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0906
Epoch 9/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0906
Epoch 10/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0906
Epoch 11/150
92/98 [===========================>..] - ETA: 0s - loss: 0.0650 - mae: 0.2527 - mse: 0.0650
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.04532407969236374.
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0906
Epoch 12/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0453
Epoch 13/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0453
Epoch 14/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0453
Epoch 15/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0453
Epoch 16/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0453
Epoch 17/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0453
Epoch 18/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0453
Epoch 19/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0453
Epoch 20/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0453
Epoch 21/150
73/98 [=====================>........] - ETA: 0s - loss: 0.0648 - mae: 0.2523 - mse: 0.0648
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.02266203984618187.
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0453
Epoch 22/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0227
Epoch 23/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0227
Epoch 24/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0227
Epoch 25/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0227
Epoch 26/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0227
Epoch 27/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0227
Epoch 28/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0227
Epoch 29/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0227
Epoch 30/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0227
Epoch 31/150
91/98 [==========================>...] - ETA: 0s - loss: 0.0650 - mae: 0.2526 - mse: 0.0650
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.011331019923090935.
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0227
Epoch 32/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0113
Epoch 33/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0113
Epoch 34/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0113
Epoch 35/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0113
Epoch 36/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0113
Epoch 37/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0113
Epoch 38/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0113
Epoch 39/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0113
Epoch 40/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0113
Epoch 41/150
49/98 [==============>...............] - ETA: 0s - loss: 0.0648 - mae: 0.2523 - mse: 0.0648
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005665509961545467.
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0113
Epoch 42/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0057
Epoch 43/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0057
Epoch 44/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0057
Epoch 45/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0057
Epoch 46/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0057
Epoch 47/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0057
Epoch 48/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0057
Epoch 49/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0057
Epoch 50/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0057
Epoch 51/150
81/98 [=======================>......] - ETA: 0s - loss: 0.0649 - mae: 0.2526 - mse: 0.0649
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0028327549807727337.
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0057
Epoch 52/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0028
Epoch 53/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0028
Epoch 54/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0028
Epoch 55/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0028
Epoch 56/150
98/98 [==============================] - 0s 3ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0028
Epoch 57/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0028
Epoch 58/150
98/98 [==============================] - 0s 2ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0028
Epoch 59/150
 1/98 [..............................] - ETA: 0s - loss: 0.0651 - mae: 0.2524 - mse: 0.0651
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0037s). Check your callbacks.
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222
 1/98 [..............................] - ETA: 0s - loss: 0.0222 - mae: 0.1453 - mse: 0.0222