Epoch 1/200
146/162 [==========================>...] - ETA: 0s - loss: 0.8572 - mae: 0.9248 - mse: 0.8572
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0029s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0029s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
162/162 [==============================] - 2s 11ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 2/200
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 3/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 4/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 5/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 6/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 7/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 8/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 9/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 10/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 11/200
151/162 [==========================>...] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.8580
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.043680667877197266.
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0874
Epoch 12/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0437
Epoch 13/200
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0437
Epoch 14/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0437
Epoch 15/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0437
Epoch 16/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0437
Epoch 17/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0437
Epoch 18/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0437
Epoch 19/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0437
Epoch 20/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0437
Epoch 21/200
133/162 [=======================>......] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.021840333938598633.
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0437
Epoch 22/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0218
Epoch 23/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0218
Epoch 24/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0218
Epoch 25/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0218
Epoch 26/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0218
Epoch 27/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0218
Epoch 28/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0218
Epoch 29/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0218
Epoch 30/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0218
Epoch 31/200
161/162 [============================>.] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.010920166969299316.
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0218
Epoch 32/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0109
Epoch 33/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0109
Epoch 34/200
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0109
Epoch 35/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0109
Epoch 36/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0109
Epoch 37/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0109
Epoch 38/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0109
Epoch 39/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0109
Epoch 40/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0109
Epoch 41/200
143/162 [=========================>....] - ETA: 0s - loss: 0.8589 - mae: 0.9257 - mse: 0.8589
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.005460083484649658.
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0109
Epoch 42/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0055
Epoch 43/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0055
Epoch 44/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0055
Epoch 45/200
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0055
Epoch 46/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0055
Epoch 47/200
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0055
Epoch 48/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0055
Epoch 49/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0055
Epoch 50/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0055
Epoch 51/200
159/162 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002730041742324829.
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0055
Epoch 52/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 53/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 54/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 55/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 56/200
133/162 [=======================>......] - ETA: 0s - loss: 0.8567 - mae: 0.9246 - mse: 0.8567
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 58/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 59/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 60/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 61/200
140/162 [========================>.....] - ETA: 0s - loss: 0.8573 - mae: 0.9248 - mse: 0.8573
 59/162 [=========>....................] - ETA: 0s - loss: 0.8583 - mae: 0.9255 - mse: 0.85838579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0027
Epoch 62/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 63/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 64/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 65/200
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 66/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 67/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 68/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 69/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 70/200
149/162 [==========================>...] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
 92/162 [================>.............] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.85828579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0006825104355812073.
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0014
Epoch 72/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.8251e-04
Epoch 73/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.8251e-04
Epoch 74/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.8251e-04
Epoch 75/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.8251e-04
Epoch 76/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.8251e-04
Epoch 77/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.8251e-04
Epoch 78/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.8251e-04
Epoch 79/200
 64/162 [==========>...................] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.8251e-04
Epoch 81/200
142/162 [=========================>....] - ETA: 0s - loss: 0.8571 - mae: 0.9248 - mse: 0.8571
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.00034125521779060364.
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 6.8251e-04
Epoch 82/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.4126e-04
Epoch 83/200
  1/162 [..............................] - ETA: 0s - loss: 0.8458 - mae: 0.9186 - mse: 0.8458
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.4126e-04
Epoch 85/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.4126e-04
Epoch 86/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.4126e-04
Epoch 87/200
 26/162 [===>..........................] - ETA: 0s - loss: 0.8620 - mae: 0.9275 - mse: 0.8620
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.4126e-04
Epoch 90/200
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.4126e-04
Epoch 91/200
 43/162 [======>.......................] - ETA: 0s - loss: 0.8583 - mae: 0.9255 - mse: 0.8583
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7063e-04
Epoch 94/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7063e-04
Epoch 95/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7063e-04
Epoch 96/200
120/162 [=====================>........] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7063e-04
Epoch 99/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7063e-04
Epoch 100/200
133/162 [=======================>......] - ETA: 0s - loss: 0.8575 - mae: 0.9250 - mse: 0.8575
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.5314e-05
Epoch 103/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.5314e-05
Epoch 104/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.5314e-05
Epoch 105/200
159/162 [============================>.] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.5314e-05
Epoch 108/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.5314e-05
Epoch 109/200
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.5314e-05
Epoch 110/200
  1/162 [..............................] - ETA: 0s - loss: 0.8737 - mae: 0.9337 - mse: 0.8737
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 8.5314e-05
Epoch 112/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2657e-05
Epoch 113/200
121/162 [=====================>........] - ETA: 0s - loss: 0.8591 - mae: 0.9259 - mse: 0.8591
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2657e-05
Epoch 117/200
148/162 [==========================>...] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.8580
127/162 [======================>.......] - ETA: 0s - loss: 0.8566 - mae: 0.9245 - mse: 0.85668579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2657e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 2.1328451111912727e-05.
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 4.2657e-05
Epoch 122/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1328e-05
Epoch 123/200
  1/162 [..............................] - ETA: 0s - loss: 0.8509 - mae: 0.9215 - mse: 0.8509
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1328e-05
Epoch 126/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1328e-05
Epoch 127/200
 88/162 [===============>..............] - ETA: 0s - loss: 0.8591 - mae: 0.9259 - mse: 0.8591
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.1328e-05
Epoch 131/200
161/162 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0664e-05
Epoch 135/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0664e-05
Epoch 136/200
 51/162 [========>.....................] - ETA: 0s - loss: 0.8564 - mae: 0.9244 - mse: 0.8564
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0664e-05
Epoch 140/200
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0664e-05
Epoch 141/200
156/162 [===========================>..] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 144/200
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 145/200
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 146/200
 66/162 [===========>..................] - ETA: 0s - loss: 0.8559 - mae: 0.9241 - mse: 0.8559
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
Epoch 149/200
 70/162 [===========>..................] - ETA: 0s - loss: 0.8568 - mae: 0.9246 - mse: 0.8568
156/162 [===========================>..] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
133/162 [=======================>......] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.85818579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
155/162 [===========================>..] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
  1/162 [..............................] - ETA: 0s - loss: 0.8384 - mae: 0.9147 - mse: 0.83848579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 0s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
129/162 [======================>.......] - ETA: 0s - loss: 0.8588 - mae: 0.9257 - mse: 0.85888579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
162/162 [==============================] - 1s 4ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0874e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0874e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0874e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0437e-05
  1/216 [..............................] - ETA: 0s - loss: 0.6592 - mae: 0.8101 - mse: 0.65926431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0437e-05
202/216 [===========================>..] - ETA: 0s - loss: 0.6435 - mae: 0.8001 - mse: 0.64356431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0437e-05
216/216 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0218e-05
216/216 [==============================] - 1s 4ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0218e-05
216/216 [==============================] - 0s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0218e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0109e-05
 82/216 [==========>...................] - ETA: 0s - loss: 0.6411 - mae: 0.7986 - mse: 0.64116431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0109e-05
  1/216 [..............................] - ETA: 0s - loss: 0.6213 - mae: 0.7867 - mse: 0.62136431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0109e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0055e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0055e-05
195/216 [==========================>...] - ETA: 0s - loss: 0.6434 - mae: 0.8000 - mse: 0.64346431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0055e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0027e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0027e-05
216/216 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0027e-05
216/216 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0027e-05
216/216 [==============================] - 1s 3ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
210/216 [============================>.] - ETA: 0s - loss: 0.6430 - mae: 0.7997 - mse: 0.64306431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
149/216 [===================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0030s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0030s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/200=================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 11/200================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 15/200================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 20/200================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 24/200================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 29/200================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 33/200================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 38/200================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 42/200================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 47/200================>..........] - ETA: 0s - loss: 0.6445 - mae: 0.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 73/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 85/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 93/200duceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 104/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 112/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 117/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 117/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 125/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 134/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 139/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 175/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/20000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 6/20000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 14/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 14/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 14/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 24/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 29/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 44/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 44/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 52/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 52/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 59/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 63/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 63/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 63/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 75/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 75/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 83/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 88/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 88/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 94/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 94/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 102/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 125/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 125/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 133/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/20000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 6/20000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 14/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 14/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 14/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 25/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 25/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 45/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 45/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 53/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 53/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 53/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 64/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 64/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 72/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 77/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 77/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 85/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 85/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 92/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 92/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 92/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 92/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 92/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 92/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 114/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 130/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 134/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 134/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 142/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 158/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 158/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0017s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0017s). Check your callbacks.
Epoch 167/200uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 6/20000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 11/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 15/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 20/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 24/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 29/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 38/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 42/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 42/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 51/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 55/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 55/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 63/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 63/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 63/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 74/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 74/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 82/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 87/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.8006 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00017062760889530182.06 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 96/200duceLROnPlateau reducing learning rate to 0.00017062760889530182.06 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.00017062760889530182.06 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.00017062760889530182.06 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 0.00017062760889530182.06 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 0.00017062760889530182.06 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 110/200uceLROnPlateau reducing learning rate to 0.00017062760889530182.06 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 121: ReduceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 126/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 126/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 134/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 139/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 143/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 148/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 153/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 153/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 153/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 153/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 169/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 169/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 169/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 182/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 182/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 191/200duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_174434-uob4sckp\files\model-best)... Done. 0.0s
Epoch 11/2000duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 15/2000duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 15/2000duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 20/2000duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 24/2000duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 33/2000duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 38/2000duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 42/2000duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 42/2000duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 47/2000duceLROnPlateau reducing learning rate to 2.1328451111912727e-05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 56/200duceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 61/200duceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 65/200duceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 70/200duceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 74/200duceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 79/200duceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 83/200duceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 88/200duceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 92/200duceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 101/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 105/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 113/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 118/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 122/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 127/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 127/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 135/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 140/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 144/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 149/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 165/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 170/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 180/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 190/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 200/200uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_174434-uob4sckp\files\model-best)... Done. 0.0s
Epoch 13/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 22/2000uceLROnPlateau reducing learning rate to 0.002730041742324829.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.010920166969299316.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 41/200duceLROnPlateau reducing learning rate to 0.010920166969299316.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 50/200duceLROnPlateau reducing learning rate to 0.010920166969299316.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 54/200duceLROnPlateau reducing learning rate to 0.010920166969299316.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
Epoch 63/200duceLROnPlateau reducing learning rate to 0.010920166969299316.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 0.0014
Epoch 64/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 0.0014
Epoch 65/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 0.0014
Epoch 66/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 0.0014
Epoch 67/200
126/162 [======================>.......] - ETA: 0s - loss: 0.0326 - mae: 0.1752 - mse: 0.0326
Epoch 72/200duceLROnPlateau reducing learning rate to 0.010920166969299316.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 0.0014
Epoch 69/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 0.0014
Epoch 70/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 0.0014
Epoch 71/200
128/162 [======================>.......] - ETA: 0s - loss: 0.0326 - mae: 0.1752 - mse: 0.0326
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0006825104355812073.
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 0.0014
Epoch 72/200duceLROnPlateau reducing learning rate to 0.010920166969299316.05.6 - mse: 0.64456431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0014e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
Epoch 73/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
Epoch 74/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
Epoch 75/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
Epoch 76/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
Epoch 77/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
Epoch 78/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
Epoch 79/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
Epoch 80/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
Epoch 81/200
150/162 [==========================>...] - ETA: 0s - loss: 0.0326 - mae: 0.1752 - mse: 0.0326
Epoch 81: ReduceLROnPlateau reducing learning rate to 0.00034125521779060364.
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
Epoch 82/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 3.4126e-04
Epoch 83/200
127/162 [======================>.......] - ETA: 0s - loss: 0.0326 - mae: 0.1751 - mse: 0.0326
Epoch 85/200===========================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 6.8251e-04
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 3.4126e-04
Epoch 86/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 3.4126e-04
Epoch 87/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 3.4126e-04
Epoch 88/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 3.4126e-04
Epoch 89/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 3.4126e-04
Epoch 90/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 3.4126e-04
Epoch 91/200
123/162 [=====================>........] - ETA: 0s - loss: 0.0325 - mae: 0.1749 - mse: 0.0325
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00017062760889530182.
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 3.4126e-04
Epoch 92/200
116/162 [====================>.........] - ETA: 0s - loss: 0.0326 - mae: 0.1751 - mse: 0.0326
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
Epoch 94/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
Epoch 95/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
Epoch 96/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
Epoch 97/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
Epoch 98/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
Epoch 99/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
Epoch 100/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
Epoch 101/200
113/162 [===================>..........] - ETA: 0s - loss: 0.0323 - mae: 0.1745 - mse: 0.0323
Epoch 102/200==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 8.5314e-05
Epoch 103/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 8.5314e-05
Epoch 104/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 8.5314e-05
Epoch 105/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 8.5314e-05
Epoch 106/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 8.5314e-05
Epoch 107/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 8.5314e-05
Epoch 108/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 8.5314e-05
Epoch 109/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 8.5314e-05
Epoch 110/200
110/162 [===================>..........] - ETA: 0s - loss: 0.0324 - mae: 0.1747 - mse: 0.0324
101/162 [=================>............] - ETA: 0s - loss: 0.0323 - mae: 0.1744 - mse: 0.03230326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
Epoch 111: ReduceLROnPlateau reducing learning rate to 4.2656902223825455e-05.
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 8.5314e-05
Epoch 112/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 4.2657e-05
Epoch 113/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 4.2657e-05
Epoch 114/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 4.2657e-05
Epoch 115/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 4.2657e-05
Epoch 116/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 4.2657e-05
Epoch 117/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 4.2657e-05
Epoch 118/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 4.2657e-05
Epoch 119/200
122/162 [=====================>........] - ETA: 0s - loss: 0.0326 - mae: 0.1750 - mse: 0.0326
 53/162 [========>.....................] - ETA: 0s - loss: 0.0331 - mae: 0.1767 - mse: 0.03310326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.7063e-04
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 4.2657e-05
Epoch 121/200
148/162 [==========================>...] - ETA: 0s - loss: 0.0325 - mae: 0.1749 - mse: 0.0325
Epoch 121: ReduceLROnPlateau reducing learning rate to 2.1328451111912727e-05.
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 4.2657e-05
Epoch 122/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
Epoch 123/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
Epoch 124/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
Epoch 125/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
Epoch 126/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
Epoch 127/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
Epoch 128/200
Epoch 137/200==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
Epoch 129/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
Epoch 130/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
Epoch 131/200
112/162 [===================>..........] - ETA: 0s - loss: 0.0326 - mae: 0.1752 - mse: 0.0326
Epoch 131: ReduceLROnPlateau reducing learning rate to 1.0664225555956364e-05.
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
Epoch 132/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0664e-05
Epoch 133/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0664e-05
Epoch 134/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0664e-05
Epoch 135/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0664e-05
Epoch 136/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0664e-05
Epoch 137/200==========================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 2.1328e-05
 45/162 [=======>......................] - ETA: 0s - loss: 0.0332 - mae: 0.1767 - mse: 0.03320326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0664e-05
Epoch 138/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0664e-05
Epoch 139/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0664e-05
Epoch 140/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0664e-05
Epoch 141/200
145/162 [=========================>....] - ETA: 0s - loss: 0.0327 - mae: 0.1754 - mse: 0.0327
Epoch 141: ReduceLROnPlateau reducing learning rate to 1e-05.
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0664e-05
Epoch 142/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 143/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 144/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 145/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 146/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 147/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 148/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 149/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 150/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 151/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 152/200
 53/162 [========>.....................] - ETA: 0s - loss: 0.0325 - mae: 0.1746 - mse: 0.0325
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 157/200
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 158/200
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 159/200
137/162 [========================>.....] - ETA: 0s - loss: 0.0326 - mae: 0.1753 - mse: 0.0326
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
Epoch 167/200
 71/162 [============>.................] - ETA: 0s - loss: 0.0324 - mae: 0.1747 - mse: 0.0324
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0326 - mae: 0.1752 - mse: 0.0326 - val_loss: 0.0324 - val_mae: 0.1746 - val_mse: 0.0324 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0437e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0218e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0218e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0109e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0109e-05
145/162 [=========================>....] - ETA: 0s - loss: 0.0076 - mae: 0.0753 - mse: 0.00760076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0109e-05
145/162 [=========================>....] - ETA: 0s - loss: 0.0076 - mae: 0.0753 - mse: 0.00760076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0109e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0027e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 0.0014e-05
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 6.8251e-04
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 6.8251e-04
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 3.4126e-04
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 1.7063e-04
162/162 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 8.5314e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05
162/162 [==============================] - 0s 1ms/step - loss: 0.0076 - mae: 0.0752 - mse: 0.0076 - val_loss: 0.0075 - val_mae: 0.0746 - val_mse: 0.0075 - lr: 4.2657e-05