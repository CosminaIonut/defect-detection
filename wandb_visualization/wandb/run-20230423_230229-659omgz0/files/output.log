Epoch 1/200
211/243 [=========================>....] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.8581
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
243/243 [==============================] - 2s 6ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 2/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 3/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 4/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 5/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 6/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 7/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 8/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 9/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 10/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 11/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 12/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 13/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 14/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 15/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 16/200
225/243 [==========================>...] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.018078282475471497.
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0362
Epoch 17/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 18/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 19/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 20/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 21/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 22/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 23/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 24/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 25/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 26/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 27/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 28/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 29/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 30/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 31/200
235/243 [============================>.] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.8576
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.009039141237735748.
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0181
Epoch 32/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 33/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 34/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 35/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 36/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 37/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 38/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 39/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 40/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 41/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 42/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 43/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 44/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 45/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 46/200
215/243 [=========================>....] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
Epoch 46: ReduceLROnPlateau reducing learning rate to 0.004519570618867874.
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0090
Epoch 47/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 48/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 49/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 50/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 51/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 52/200
 93/243 [==========>...................] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.85818579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 53/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 54/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 55/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 56/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 57/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 58/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 59/200
232/243 [===========================>..] - ETA: 0s - loss: 0.8581 - mae: 0.9253 - mse: 0.8581
235/243 [============================>.] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.85778579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 61/200
228/243 [===========================>..] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.8580
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.002259785309433937.
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0045
Epoch 62/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 63/200
146/243 [=================>............] - ETA: 0s - loss: 0.8576 - mae: 0.9250 - mse: 0.85768579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 64/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 65/200
243/243 [==============================] - 1s 3ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 66/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 67/200
 29/243 [==>...........................] - ETA: 0s - loss: 0.8593 - mae: 0.9260 - mse: 0.85938579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 68/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 69/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 70/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 71/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 72/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 73/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 74/200
222/243 [==========================>...] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
178/243 [====================>.........] - ETA: 0s - loss: 0.8572 - mae: 0.9248 - mse: 0.85728579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 76/200
217/243 [=========================>....] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0011298926547169685.
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0023
Epoch 77/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 78/200
 65/243 [=======>......................] - ETA: 0s - loss: 0.8596 - mae: 0.9261 - mse: 0.85968579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 79/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 80/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 81/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 82/200
  1/243 [..............................] - ETA: 0s - loss: 0.8675 - mae: 0.9305 - mse: 0.86758579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 83/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 84/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 85/200
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 86/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 87/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 88/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 89/200
186/243 [=====================>........] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.8574
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 91/200
218/243 [=========================>....] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.8579
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0005649463273584843.
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 0.0011
Epoch 92/200
 89/243 [=========>....................] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.8580
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.6495e-04
Epoch 94/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.6495e-04
Epoch 95/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.6495e-04
Epoch 96/200
 55/243 [=====>........................] - ETA: 0s - loss: 0.8587 - mae: 0.9257 - mse: 0.8587
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.6495e-04
Epoch 98/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.6495e-04
Epoch 99/200
224/243 [==========================>...] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.8583
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.6495e-04
Epoch 102/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.6495e-04
Epoch 103/200
194/243 [======================>.......] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.6495e-04
Epoch 106/200
239/243 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9252 - mse: 0.8578
Epoch 106: ReduceLROnPlateau reducing learning rate to 0.00028247316367924213.
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 5.6495e-04
Epoch 107/200
 59/243 [======>.......................] - ETA: 0s - loss: 0.8596 - mae: 0.9261 - mse: 0.8596
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.8247e-04
Epoch 109/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.8247e-04
Epoch 110/200
228/243 [===========================>..] - ETA: 0s - loss: 0.8577 - mae: 0.9251 - mse: 0.8577
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.8247e-04
Epoch 113/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.8247e-04
Epoch 114/200
225/243 [==========================>...] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.8585
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.8247e-04
Epoch 117/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.8247e-04
Epoch 118/200
 90/243 [==========>...................] - ETA: 0s - loss: 0.8593 - mae: 0.9260 - mse: 0.8593
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 2.8247e-04
Epoch 121/200
240/243 [============================>.] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.8578
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.4124e-04
Epoch 124/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.4124e-04
Epoch 125/200
139/243 [================>.............] - ETA: 0s - loss: 0.8574 - mae: 0.9249 - mse: 0.8574
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.4124e-04
Epoch 128/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.4124e-04
Epoch 129/200
  1/243 [..............................] - ETA: 1s - loss: 0.8600 - mae: 0.9264 - mse: 0.8600
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.4124e-04
Epoch 132/200
235/243 [============================>.] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.8582
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.4124e-04
Epoch 136/200
182/243 [=====================>........] - ETA: 0s - loss: 0.8580 - mae: 0.9252 - mse: 0.8580
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.0618e-05
Epoch 139/200
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.0618e-05
Epoch 140/200
 60/243 [======>.......................] - ETA: 0s - loss: 0.8586 - mae: 0.9255 - mse: 0.8586
238/243 [============================>.] - ETA: 0s - loss: 0.8579 - mae: 0.9252 - mse: 0.85798579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.0618e-05
200/243 [=======================>......] - ETA: 0s - loss: 0.8573 - mae: 0.9249 - mse: 0.85738579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.0618e-05
 86/243 [=========>....................] - ETA: 0s - loss: 0.8595 - mae: 0.9261 - mse: 0.85958579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 7.0618e-05
243/243 [==============================] - 0s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5309e-05
Epoch 154/200
 61/243 [======>.......................] - ETA: 0s - loss: 0.8548 - mae: 0.9235 - mse: 0.8548
233/243 [===========================>..] - ETA: 0s - loss: 0.8580 - mae: 0.9253 - mse: 0.85808579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5309e-05
124/243 [==============>...............] - ETA: 0s - loss: 0.8585 - mae: 0.9255 - mse: 0.85858579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5309e-05
 27/243 [==>...........................] - ETA: 0s - loss: 0.8575 - mae: 0.9249 - mse: 0.85758579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5309e-05
225/243 [==========================>...] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 3.5309e-05
187/243 [======================>.......] - ETA: 0s - loss: 0.8582 - mae: 0.9254 - mse: 0.85828579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7655e-05
142/243 [================>.............] - ETA: 0s - loss: 0.8578 - mae: 0.9251 - mse: 0.85788579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7655e-05
  1/243 [..............................] - ETA: 0s - loss: 0.8721 - mae: 0.9331 - mse: 0.87218579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7655e-05
191/243 [======================>.......] - ETA: 0s - loss: 0.8583 - mae: 0.9254 - mse: 0.85838579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.7655e-05
 90/243 [==========>...................] - ETA: 0s - loss: 0.8601 - mae: 0.9264 - mse: 0.86018579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
243/243 [==============================] - 1s 2ms/step - loss: 0.8579 - mae: 0.9252 - mse: 0.8579 - val_loss: 0.8568 - val_mae: 0.9246 - val_mse: 0.8568 - lr: 1.0000e-05
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0362e-05
  1/323 [..............................] - ETA: 1s - loss: 0.6381 - mae: 0.7965 - mse: 0.63816431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0362e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0362e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0362e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0181e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0181e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0181e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0181e-05
  1/323 [..............................] - ETA: 1s - loss: 0.6693 - mae: 0.8159 - mse: 0.66936431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0181e-05
 29/323 [=>............................] - ETA: 0s - loss: 0.6426 - mae: 0.7994 - mse: 0.64266431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0181e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0090e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0090e-05
251/323 [======================>.......] - ETA: 0s - loss: 0.6442 - mae: 0.8005 - mse: 0.64426431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0090e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0090e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0090e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0045e-05
315/323 [============================>.] - ETA: 0s - loss: 0.6427 - mae: 0.7996 - mse: 0.64276431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0045e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0045e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0045e-05
139/323 [===========>..................] - ETA: 0s - loss: 0.6412 - mae: 0.7986 - mse: 0.64126431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0045e-05
 86/323 [======>.......................] - ETA: 0s - loss: 0.6398 - mae: 0.7977 - mse: 0.63986431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0045e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0023e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0023e-05
297/323 [==========================>...] - ETA: 0s - loss: 0.6433 - mae: 0.7999 - mse: 0.64336431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0023e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0023e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0023e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0011e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0011e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0011e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0011e-05
310/323 [===========================>..] - ETA: 0s - loss: 0.6429 - mae: 0.7997 - mse: 0.64296431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0011e-05
301/323 [==========================>...] - ETA: 0s - loss: 0.6426 - mae: 0.7995 - mse: 0.64266431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 0.0011e-05
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 5.6495e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 5.6495e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 5.6495e-04
 98/323 [========>.....................] - ETA: 0s - loss: 0.6414 - mae: 0.7987 - mse: 0.64146431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 5.6495e-04
  1/323 [..............................] - ETA: 0s - loss: 0.6381 - mae: 0.7966 - mse: 0.63816431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 5.6495e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
196/323 [=================>............] - ETA: 0s - loss: 0.6431 - mae: 0.7998 - mse: 0.64316431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
323/323 [==============================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 122/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 126/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 128/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 131/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 133/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 136/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 139/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 142/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 145/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 148/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 150/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 152/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 156/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 158/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 161/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 164/200==========================] - 1s 2ms/step - loss: 0.6431 - mae: 0.7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 166: ReduceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 169/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 172/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 175/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 178/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 181/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 182/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 186/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 189/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 192/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 194/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 197/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 200/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 200/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 200/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 7/20000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 11/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 15/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 18/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 22/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 26/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 30/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 33/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 33/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 37/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 41/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 45/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 48/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 52/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 56/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 60/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 63/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 67/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 71/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 75/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 81/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 85/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 85/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 92/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 96/2000duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 100/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 104/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 107/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 111/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 115/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 119/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 122/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 122/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 126/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 130/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 137/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 141/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 141/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 145/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 149/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 152/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 159/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 163/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 163/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 170/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 170/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 174/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 178/200duceLROnPlateau reducing learning rate to 1.7654572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 185/200duceLROnPlateau reducing learning rate to 1e-05.572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 185/200duceLROnPlateau reducing learning rate to 1e-05.572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 189/200duceLROnPlateau reducing learning rate to 1e-05.572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 193/200duceLROnPlateau reducing learning rate to 1e-05.572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 197/200duceLROnPlateau reducing learning rate to 1e-05.572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 197/200duceLROnPlateau reducing learning rate to 1e-05.572729952633e-05..7998 - mse: 0.6431 - val_loss: 0.6442 - val_mae: 0.8005 - val_mse: 0.6442 - lr: 2.8247e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0029s). Check your callbacks.
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 7/200rained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 7/200rained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 11/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 18/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 18/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 22/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 26/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 33/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 33/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 37/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 41/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 45/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 48/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 52/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 56/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 59/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 62/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 66/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 70/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 74/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 77/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 81/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 85/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 89/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 95/200duceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 103/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 106/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 109/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 113/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 119/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 122/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 126/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 130/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 134/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 137/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 141/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 145/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 145/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 152/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 152/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 156/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 160/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 164/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 167/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 171/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 175/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 179/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 182/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 186/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 190/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 194/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 197/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 200/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 200/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_230229-659omgz0\files\model-best)... Done. 0.0s
Epoch 7/20000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 11/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 15/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 18/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 22/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 26/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 30/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 37/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 41/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 45/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 48/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 52/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 56/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 60/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 63/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 67/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 71/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 75/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 78/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 82/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 86/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 90/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 93/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 97/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 97/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 101/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 105/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 112/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 116/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 120/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 123/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 127/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 131/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 138/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 142/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 146/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 146/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 150/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 153/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 157/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 161/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 165/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 168/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 172/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 176/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 183/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 183/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 187/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 191/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 195/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 198/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 1/20000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 1/20000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 7/20000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 11/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 15/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 18/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 22/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 26/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 30/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 33/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 37/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 41/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 45/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 48/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 52/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 56/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 60/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 63/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 67/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 71/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 75/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 78/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 82/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 82/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 86/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 90/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 93/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 97/2000uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 101/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 105/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 108/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 112/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 116/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 120/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 123/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 127/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 131/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 134/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 137/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 141/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 145/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 149/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 152/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 156/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 160/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 164/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 167/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 170/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 174/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 178/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 181: ReduceLROnPlateau reducing learning rate to 1e-05.49463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 185/200duceLROnPlateau reducing learning rate to 1e-05.49463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 189/200duceLROnPlateau reducing learning rate to 1e-05.49463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 193/200duceLROnPlateau reducing learning rate to 1e-05.49463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 197/200duceLROnPlateau reducing learning rate to 1e-05.49463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_3.h5 - val_mse: 0.6442 - lr: 2.8247e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_230229-659omgz0\files\model-best)... Done. 0.0s
Epoch 7/200rained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 11/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 11/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 18/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 22/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 22/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 26/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 30/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 33/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 37/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 41/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 45/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 47/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 51/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 55/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 59/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 62/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 66/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 70/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 74/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0011298926547169685.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 80/200duceLROnPlateau reducing learning rate to 0.0011298926547169685.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 84/200duceLROnPlateau reducing learning rate to 0.0011298926547169685.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 88/200duceLROnPlateau reducing learning rate to 0.0011298926547169685.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 95/200duceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 99/200duceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 103/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 106/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 109/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 113/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 117/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 121/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 124/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 128/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 132/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 135/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 138/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 142/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 146/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 150/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 153/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 157/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 161/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 164/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 167/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 171/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 175/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 179/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 182/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 186/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 190/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 194/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 198/200uceLROnPlateau reducing learning rate to 0.0005649463273584843.2LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_6.h5 - val_mse: 0.6442 - lr: 2.8247e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230423_230229-659omgz0\files\model-best)... Done. 0.0s
Epoch 3/200rained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 7/200rained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 11/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 18/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 18/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 22/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 26/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 33/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 33/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 37/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 41/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 45/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 48/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 52/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 55/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 59/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 62/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 66/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 70/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 74/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 77/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 81/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 85/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 92/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 92/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 96/200ained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 100/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 104/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 107/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 111/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 115/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 119/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 122/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 126/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 130/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 134/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 137/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 141/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 144/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 148/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 151/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 154/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 158/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 162/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 166/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 169/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 173/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 177/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 181/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 184/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 187/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 191/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 195/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 198/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 1/20000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 1/20000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0016s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0016s). Check your callbacks.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
Epoch 7/20000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 11/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 15/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 18/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 22/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 26/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 30/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 33/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 37/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 41/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 45/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 48/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 52/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 56/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 60/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 63/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 63/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 67/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 71/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 75/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 78/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 82/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 86/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 90/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 93/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 97/2000ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 101/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 105/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 107/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 111/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 115/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 118/200ined_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 121: ReduceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 125/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 129/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 133/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 136/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 139/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 143/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 147/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 151/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 154/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 158/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 161/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 165/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 168/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 172/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 176/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 180/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 183/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 187/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 191/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 195/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
Epoch 198/200duceLROnPlateau reducing learning rate to 0.00014123658183962107.R_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_7.h5 - val_mse: 0.6442 - lr: 2.8247e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_9.h5 - val_mse: 0.6442 - lr: 2.8247e-04
>Saved ../trained_models/models_segments_overlap-cnn_adam_0.03615656645923222LR_[31]CHN_64CNNI_16BS_15P_val_lossM_200epochs/model_9.h5 - val_mse: 0.6442 - lr: 2.8247e-04