Epoch 1/200
167/243 [===================>..........] - ETA: 0s - loss: 0.7636 - mae: 0.2126 - mse: 0.0585
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.7002 - mae: 0.1707 - mse: 0.0426 - val_loss: 0.5173 - val_mae: 0.0680 - val_mse: 0.0058 - lr: 0.0101
Epoch 2/200
243/243 [==============================] - 1s 4ms/step - loss: 0.4099 - mae: 0.0573 - mse: 0.0042 - val_loss: 0.3182 - val_mae: 0.0524 - val_mse: 0.0034 - lr: 0.0101
Epoch 3/200
186/243 [=====================>........] - ETA: 0s - loss: 0.2664 - mae: 0.0506 - mse: 0.0032
243/243 [==============================] - 1s 3ms/step - loss: 0.2529 - mae: 0.0503 - mse: 0.0032 - val_loss: 0.1970 - val_mae: 0.0500 - val_mse: 0.0031 - lr: 0.0101
Epoch 4/200
232/243 [===========================>..] - ETA: 0s - loss: 0.1586 - mae: 0.0489 - mse: 0.0030
243/243 [==============================] - 1s 3ms/step - loss: 0.1571 - mae: 0.0490 - mse: 0.0030 - val_loss: 0.1229 - val_mae: 0.0491 - val_mse: 0.0030 - lr: 0.0101
Epoch 5/200
192/243 [======================>.......] - ETA: 0s - loss: 0.1029 - mae: 0.0483 - mse: 0.0029
243/243 [==============================] - 1s 3ms/step - loss: 0.0984 - mae: 0.0483 - mse: 0.0029 - val_loss: 0.0775 - val_mae: 0.0487 - val_mse: 0.0029 - lr: 0.0101
Epoch 6/200
162/243 [===================>..........] - ETA: 0s - loss: 0.0669 - mae: 0.0479 - mse: 0.0029
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0624 - mae: 0.0478 - mse: 0.0028 - val_loss: 0.0496 - val_mae: 0.0482 - val_mse: 0.0029 - lr: 0.0101
Epoch 7/200
161/243 [==================>...........] - ETA: 0s - loss: 0.0431 - mae: 0.0475 - mse: 0.0028
243/243 [==============================] - 1s 3ms/step - loss: 0.0403 - mae: 0.0473 - mse: 0.0028 - val_loss: 0.0325 - val_mae: 0.0477 - val_mse: 0.0028 - lr: 0.0101
Epoch 8/200
157/243 [==================>...........] - ETA: 0s - loss: 0.0285 - mae: 0.0469 - mse: 0.0027
243/243 [==============================] - 1s 4ms/step - loss: 0.0267 - mae: 0.0468 - mse: 0.0027 - val_loss: 0.0219 - val_mae: 0.0470 - val_mse: 0.0027 - lr: 0.0101
Epoch 9/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0183 - mae: 0.0462 - mse: 0.0027 - val_loss: 0.0153 - val_mae: 0.0464 - val_mse: 0.0027 - lr: 0.0101
Epoch 10/200
190/243 [======================>.......] - ETA: 0s - loss: 0.0135 - mae: 0.0456 - mse: 0.0026
243/243 [==============================] - 1s 3ms/step - loss: 0.0130 - mae: 0.0456 - mse: 0.0026 - val_loss: 0.0112 - val_mae: 0.0459 - val_mse: 0.0026 - lr: 0.0101
Epoch 11/200
175/243 [====================>.........] - ETA: 0s - loss: 0.0101 - mae: 0.0452 - mse: 0.0025
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0098 - mae: 0.0452 - mse: 0.0025 - val_loss: 0.0086 - val_mae: 0.0453 - val_mse: 0.0026 - lr: 0.0101
Epoch 12/200
170/243 [===================>..........] - ETA: 0s - loss: 0.0079 - mae: 0.0445 - mse: 0.0025
243/243 [==============================] - 1s 4ms/step - loss: 0.0077 - mae: 0.0446 - mse: 0.0025 - val_loss: 0.0069 - val_mae: 0.0449 - val_mse: 0.0025 - lr: 0.0101
Epoch 13/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0063 - mae: 0.0442 - mse: 0.0024 - val_loss: 0.0059 - val_mae: 0.0444 - val_mse: 0.0025 - lr: 0.0101
Epoch 14/200
186/243 [=====================>........] - ETA: 0s - loss: 0.0055 - mae: 0.0436 - mse: 0.0024
243/243 [==============================] - 1s 3ms/step - loss: 0.0054 - mae: 0.0437 - mse: 0.0024 - val_loss: 0.0051 - val_mae: 0.0441 - val_mse: 0.0024 - lr: 0.0101
Epoch 15/200
166/243 [===================>..........] - ETA: 0s - loss: 0.0049 - mae: 0.0435 - mse: 0.0024
243/243 [==============================] - 1s 3ms/step - loss: 0.0048 - mae: 0.0433 - mse: 0.0024 - val_loss: 0.0046 - val_mae: 0.0437 - val_mse: 0.0024 - lr: 0.0101
Epoch 16/200
182/243 [=====================>........] - ETA: 0s - loss: 0.0044 - mae: 0.0429 - mse: 0.0023
243/243 [==============================] - 1s 3ms/step - loss: 0.0044 - mae: 0.0431 - mse: 0.0023 - val_loss: 0.0043 - val_mae: 0.0434 - val_mse: 0.0024 - lr: 0.0101
Epoch 17/200
181/243 [=====================>........] - ETA: 0s - loss: 0.0041 - mae: 0.0426 - mse: 0.0023
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0041 - mae: 0.0427 - mse: 0.0023 - val_loss: 0.0040 - val_mae: 0.0431 - val_mse: 0.0023 - lr: 0.0101
Epoch 18/200
146/243 [=================>............] - ETA: 0s - loss: 0.0039 - mae: 0.0426 - mse: 0.0023
243/243 [==============================] - 1s 3ms/step - loss: 0.0039 - mae: 0.0424 - mse: 0.0023 - val_loss: 0.0038 - val_mae: 0.0429 - val_mse: 0.0023 - lr: 0.0101
Epoch 19/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0037 - mae: 0.0422 - mse: 0.0022 - val_loss: 0.0037 - val_mae: 0.0427 - val_mse: 0.0023 - lr: 0.0101
Epoch 20/200
148/243 [=================>............] - ETA: 0s - loss: 0.0036 - mae: 0.0424 - mse: 0.0023
243/243 [==============================] - 1s 3ms/step - loss: 0.0036 - mae: 0.0420 - mse: 0.0022 - val_loss: 0.0036 - val_mae: 0.0424 - val_mse: 0.0023 - lr: 0.0101
Epoch 21/200
176/243 [====================>.........] - ETA: 0s - loss: 0.0035 - mae: 0.0420 - mse: 0.0022
243/243 [==============================] - 1s 3ms/step - loss: 0.0035 - mae: 0.0419 - mse: 0.0022 - val_loss: 0.0035 - val_mae: 0.0422 - val_mse: 0.0022 - lr: 0.0101
Epoch 22/200
168/243 [===================>..........] - ETA: 0s - loss: 0.0034 - mae: 0.0417 - mse: 0.0022
243/243 [==============================] - 1s 3ms/step - loss: 0.0034 - mae: 0.0416 - mse: 0.0022 - val_loss: 0.0034 - val_mae: 0.0420 - val_mse: 0.0022 - lr: 0.0101
Epoch 23/200
187/243 [======================>.......] - ETA: 0s - loss: 0.0033 - mae: 0.0413 - mse: 0.0022
243/243 [==============================] - 1s 3ms/step - loss: 0.0033 - mae: 0.0415 - mse: 0.0022 - val_loss: 0.0033 - val_mae: 0.0419 - val_mse: 0.0022 - lr: 0.0101
Epoch 24/200
168/243 [===================>..........] - ETA: 0s - loss: 0.0032 - mae: 0.0416 - mse: 0.0022
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0032 - mae: 0.0413 - mse: 0.0022 - val_loss: 0.0032 - val_mae: 0.0417 - val_mse: 0.0022 - lr: 0.0101
Epoch 25/200
163/243 [===================>..........] - ETA: 0s - loss: 0.0031 - mae: 0.0411 - mse: 0.0021
243/243 [==============================] - 1s 4ms/step - loss: 0.0031 - mae: 0.0411 - mse: 0.0021 - val_loss: 0.0031 - val_mae: 0.0416 - val_mse: 0.0022 - lr: 0.0101
Epoch 26/200
146/243 [=================>............] - ETA: 0s - loss: 0.0031 - mae: 0.0409 - mse: 0.0021
243/243 [==============================] - 1s 4ms/step - loss: 0.0031 - mae: 0.0410 - mse: 0.0021 - val_loss: 0.0031 - val_mae: 0.0415 - val_mse: 0.0022 - lr: 0.0101
Epoch 27/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0030 - mae: 0.0409 - mse: 0.0021 - val_loss: 0.0030 - val_mae: 0.0413 - val_mse: 0.0022 - lr: 0.0101
Epoch 28/200
177/243 [====================>.........] - ETA: 0s - loss: 0.0030 - mae: 0.0407 - mse: 0.0021
243/243 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0407 - mse: 0.0021 - val_loss: 0.0030 - val_mae: 0.0412 - val_mse: 0.0022 - lr: 0.0101
Epoch 29/200
178/243 [====================>.........] - ETA: 0s - loss: 0.0029 - mae: 0.0407 - mse: 0.0021
243/243 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0406 - mse: 0.0021 - val_loss: 0.0029 - val_mae: 0.0411 - val_mse: 0.0022 - lr: 0.0101
Epoch 30/200
174/243 [====================>.........] - ETA: 0s - loss: 0.0029 - mae: 0.0406 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0029 - mae: 0.0405 - mse: 0.0021 - val_loss: 0.0029 - val_mae: 0.0409 - val_mse: 0.0021 - lr: 0.0101
Epoch 31/200
184/243 [=====================>........] - ETA: 0s - loss: 0.0028 - mae: 0.0403 - mse: 0.0021
243/243 [==============================] - 1s 3ms/step - loss: 0.0028 - mae: 0.0404 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0409 - val_mse: 0.0021 - lr: 0.0101
Epoch 32/200
151/243 [=================>............] - ETA: 0s - loss: 0.0028 - mae: 0.0400 - mse: 0.0021
243/243 [==============================] - 1s 3ms/step - loss: 0.0028 - mae: 0.0403 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0408 - val_mse: 0.0021 - lr: 0.0101
Epoch 33/200
176/243 [====================>.........] - ETA: 0s - loss: 0.0027 - mae: 0.0400 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0027 - mae: 0.0402 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0407 - val_mse: 0.0021 - lr: 0.0101
Epoch 34/200
164/243 [===================>..........] - ETA: 0s - loss: 0.0027 - mae: 0.0398 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0027 - mae: 0.0401 - mse: 0.0021 - val_loss: 0.0027 - val_mae: 0.0405 - val_mse: 0.0021 - lr: 0.0101
Epoch 35/200
159/243 [==================>...........] - ETA: 0s - loss: 0.0027 - mae: 0.0400 - mse: 0.0021
Epoch 35: ReduceLROnPlateau reducing learning rate to 0.005041306372731924.
243/243 [==============================] - 1s 3ms/step - loss: 0.0027 - mae: 0.0400 - mse: 0.0021 - val_loss: 0.0027 - val_mae: 0.0405 - val_mse: 0.0021 - lr: 0.0101
Epoch 36/200
167/243 [===================>..........] - ETA: 0s - loss: 0.0026 - mae: 0.0399 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0026 - mae: 0.0400 - mse: 0.0021 - val_loss: 0.0027 - val_mae: 0.0404 - val_mse: 0.0021 - lr: 0.0050
Epoch 37/200
182/243 [=====================>........] - ETA: 0s - loss: 0.0026 - mae: 0.0398 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0026 - mae: 0.0400 - mse: 0.0021 - val_loss: 0.0027 - val_mae: 0.0404 - val_mse: 0.0021 - lr: 0.0050
Epoch 38/200
172/243 [====================>.........] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0026 - mae: 0.0399 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0050
Epoch 39/200
174/243 [====================>.........] - ETA: 0s - loss: 0.0026 - mae: 0.0398 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0026 - mae: 0.0398 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0050
Epoch 40/200
164/243 [===================>..........] - ETA: 0s - loss: 0.0026 - mae: 0.0400 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0026 - mae: 0.0398 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0403 - val_mse: 0.0021 - lr: 0.0050
Epoch 41/200
167/243 [===================>..........] - ETA: 0s - loss: 0.0026 - mae: 0.0397 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0026 - mae: 0.0398 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0050
Epoch 42/200
189/243 [======================>.......] - ETA: 0s - loss: 0.0026 - mae: 0.0396 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0026 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0402 - val_mse: 0.0021 - lr: 0.0050
Epoch 43/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0050
Epoch 44/200
179/243 [=====================>........] - ETA: 0s - loss: 0.0025 - mae: 0.0397 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0397 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0050
Epoch 45/200
144/243 [================>.............] - ETA: 0s - loss: 0.0025 - mae: 0.0396 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0025 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0050
Epoch 46/200
175/243 [====================>.........] - ETA: 0s - loss: 0.0025 - mae: 0.0396 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0401 - val_mse: 0.0021 - lr: 0.0050
Epoch 47/200
182/243 [=====================>........] - ETA: 0s - loss: 0.0025 - mae: 0.0394 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0050
Epoch 48/200
171/243 [====================>.........] - ETA: 0s - loss: 0.0025 - mae: 0.0395 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0396 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0050
Epoch 49/200
168/243 [===================>..........] - ETA: 0s - loss: 0.0025 - mae: 0.0394 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0025 - mae: 0.0395 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0400 - val_mse: 0.0021 - lr: 0.0050
Epoch 50/200
174/243 [====================>.........] - ETA: 0s - loss: 0.0025 - mae: 0.0396 - mse: 0.0020
Epoch 50: ReduceLROnPlateau reducing learning rate to 0.002520653186365962.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0395 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0399 - val_mse: 0.0021 - lr: 0.0050
Epoch 51/200
173/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0395 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0399 - val_mse: 0.0021 - lr: 0.0025
Epoch 52/200
180/243 [=====================>........] - ETA: 0s - loss: 0.0025 - mae: 0.0396 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0395 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0399 - val_mse: 0.0021 - lr: 0.0025
Epoch 53/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0025 - mae: 0.0395 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0399 - val_mse: 0.0021 - lr: 0.0025
Epoch 54/200
184/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0394 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0399 - val_mse: 0.0021 - lr: 0.0025
Epoch 55/200
178/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0394 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0399 - val_mse: 0.0021 - lr: 0.0025
Epoch 56/200
185/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0395 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0394 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0398 - val_mse: 0.0021 - lr: 0.0025
Epoch 57/200
175/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0395 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0394 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0398 - val_mse: 0.0021 - lr: 0.0025
Epoch 58/200
176/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0394 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0398 - val_mse: 0.0021 - lr: 0.0025
Epoch 59/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0398 - val_mse: 0.0021 - lr: 0.0025
Epoch 60/200
182/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
Epoch 60: ReduceLROnPlateau reducing learning rate to 0.001260326593182981.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0398 - val_mse: 0.0021 - lr: 0.0025
Epoch 61/200
179/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0395 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0398 - val_mse: 0.0021 - lr: 0.0013
Epoch 62/200
170/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0398 - val_mse: 0.0021 - lr: 0.0013
Epoch 63/200
175/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0398 - val_mse: 0.0021 - lr: 0.0013
Epoch 64/200
243/243 [==============================] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0398 - val_mse: 0.0021 - lr: 0.0013
Epoch 65/200
190/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 0.0013
Epoch 66/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 0.0013
Epoch 67/200
174/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 0.0013
Epoch 68/200
187/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 0.0013
Epoch 69/200
173/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 0.0013
Epoch 70/200
154/243 [==================>...........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
Epoch 70: ReduceLROnPlateau reducing learning rate to 0.0006301632965914905.
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 0.0013
Epoch 71/200
173/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 6.3016e-04
Epoch 72/200
174/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0393 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 6.3016e-04
Epoch 73/200
166/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 6.3016e-04
Epoch 74/200
171/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 6.3016e-04
Epoch 75/200
169/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 6.3016e-04
Epoch 76/200
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 6.3016e-04
Epoch 77/200
214/243 [=========================>....] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 6.3016e-04
Epoch 78/200
232/243 [===========================>..] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 6.3016e-04
Epoch 79/200
170/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0389 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 6.3016e-04
Epoch 80/200
164/243 [===================>..........] - ETA: 0s - loss: 0.0023 - mae: 0.0389 - mse: 0.0020
Epoch 80: ReduceLROnPlateau reducing learning rate to 0.00031508164829574525.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 6.3016e-04
Epoch 81/200
163/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 3.1508e-04
Epoch 82/200
174/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0395 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 3.1508e-04
Epoch 83/200
238/243 [============================>.] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 3.1508e-04
Epoch 84/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 3.1508e-04
Epoch 85/200
180/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 3.1508e-04
Epoch 86/200
188/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 3.1508e-04
Epoch 87/200
196/243 [=======================>......] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 3.1508e-04
Epoch 88/200
175/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 3.1508e-04
Epoch 89/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 3.1508e-04
Epoch 90/200
170/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
Epoch 90: ReduceLROnPlateau reducing learning rate to 0.00015754082414787263.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 3.1508e-04
Epoch 91/200
178/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.5754e-04
Epoch 92/200
164/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.5754e-04
Epoch 93/200
171/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0389 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.5754e-04
Epoch 94/200
171/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.5754e-04
Epoch 95/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.5754e-04
Epoch 96/200
180/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0390 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.5754e-04
Epoch 97/200
180/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.5754e-04
Epoch 98/200
183/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.5754e-04
Epoch 99/200
173/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.5754e-04
Epoch 100/200
173/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
Epoch 100: ReduceLROnPlateau reducing learning rate to 7.877041207393631e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.5754e-04
Epoch 101/200
168/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0389 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 7.8770e-05
Epoch 102/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 7.8770e-05
Epoch 103/200
174/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 7.8770e-05
Epoch 104/200
165/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0389 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 7.8770e-05
Epoch 105/200
167/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0390 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 7.8770e-05
Epoch 106/200
184/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 7.8770e-05
Epoch 107/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 7.8770e-05
Epoch 108/200
161/243 [==================>...........] - ETA: 0s - loss: 0.0024 - mae: 0.0395 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 7.8770e-05
Epoch 109/200
243/243 [==============================] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 7.8770e-05
Epoch 110/200
179/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
Epoch 110: ReduceLROnPlateau reducing learning rate to 3.938520603696816e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 7.8770e-05
Epoch 111/200
167/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 3.9385e-05
Epoch 112/200
190/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 3.9385e-05
Epoch 113/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 3.9385e-05
Epoch 114/200
182/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0390 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 3.9385e-05
Epoch 115/200
160/243 [==================>...........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 3.9385e-05
Epoch 116/200
167/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 3.9385e-05
Epoch 117/200
167/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0390 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 3.9385e-05
Epoch 118/200
172/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 3.9385e-05
Epoch 119/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 3.9385e-05
Epoch 120/200
185/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
Epoch 120: ReduceLROnPlateau reducing learning rate to 1.969260301848408e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 3.9385e-05
Epoch 121/200
190/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.9693e-05
Epoch 122/200
163/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.9693e-05
Epoch 123/200
177/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
243/243 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.9693e-05
Epoch 124/200
172/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.0020
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.9693e-05
Epoch 125/200
189/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.9693e-05
Epoch 126/200
176/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.9693e-05
Epoch 127/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.9693e-05
Epoch 128/200
191/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0390 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.9693e-05
Epoch 129/200
171/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.9693e-05
Epoch 130/200
186/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
Epoch 130: ReduceLROnPlateau reducing learning rate to 1e-05.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.9693e-05
Epoch 131/200
170/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 132/200
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 133/200
174/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 134/200
243/243 [==============================] - 0s 888us/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 135/200
215/243 [=========================>....] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.0000e-05
Epoch 136/200
184/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.0020
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.0000e-05
186/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.0000e-05
186/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0396 - val_mse: 0.0020 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
169/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
169/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
159/243 [==================>...........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
159/243 [==================>...........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
169/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
169/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
186/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
186/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0020 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
170/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0395 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
170/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0395 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
177/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
177/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
178/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
156/243 [==================>...........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
156/243 [==================>...........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
187/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
187/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
183/243 [=====================>........] - ETA: 0s - loss: 0.0023 - mae: 0.0389 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
176/243 [====================>.........] - ETA: 0s - loss: 0.0023 - mae: 0.0390 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
176/243 [====================>.........] - ETA: 0s - loss: 0.0023 - mae: 0.0390 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
188/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
188/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
171/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
171/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
181/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
181/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
175/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
175/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 2s 8ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 2s 8ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 0s 880us/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
182/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
182/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
179/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0390 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
179/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0390 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
147/243 [=================>............] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
147/243 [=================>............] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 0s 878us/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 0s 878us/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
187/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.00200.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
187/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.00200.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
176/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
176/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
187/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
187/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0394 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
190/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
190/243 [======================>.......] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
169/243 [===================>..........] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
196/243 [=======================>......] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
196/243 [=======================>......] - ETA: 0s - loss: 0.0024 - mae: 0.0393 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 0s 939us/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 0s 939us/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
171/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.00200.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
171/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.00200.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-05
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
173/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
173/243 [====================>.........] - ETA: 0s - loss: 0.0024 - mae: 0.0392 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
243/243 [==============================] - 1s 3ms/step - loss: 0.0024 - mae: 0.0392 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
180/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
180/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
180/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
180/243 [=====================>........] - ETA: 0s - loss: 0.0024 - mae: 0.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_143903-nbjyo447\files\model-best)... Done. 0.0s
Epoch 48/200duceLROnPlateau reducing learning rate to 0.002520653186365962.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
323/323 [==============================] - 0s 818us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 47/200
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 48/200duceLROnPlateau reducing learning rate to 0.002520653186365962.0391 - mse: 0.00200020 - val_loss: 0.0024 - val_mae: 0.0397 - val_mse: 0.0021 - lr: 1.0000e-0505
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 49/200
323/323 [==============================] - 0s 884us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 50/200
323/323 [==============================] - 0s 882us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 51/200
323/323 [==============================] - 0s 870us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 52/200
323/323 [==============================] - 0s 836us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 53/200
323/323 [==============================] - 0s 876us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 54/200
323/323 [==============================] - 0s 836us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 55/200
323/323 [==============================] - 0s 865us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 56/200
272/323 [========================>.....] - ETA: 0s - loss: 0.0101 - mae: 0.0906 - mse: 0.0100
Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0006301632965914905.
323/323 [==============================] - 0s 862us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 0.0013
Epoch 57/200
323/323 [==============================] - 0s 860us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 6.3016e-04
Epoch 58/200
323/323 [==============================] - 0s 880us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 6.3016e-04
Epoch 59/200
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 6.3016e-04
Epoch 60/200
323/323 [==============================] - 0s 871us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 6.3016e-04
Epoch 61/200
323/323 [==============================] - 0s 831us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 6.3016e-04
Epoch 62/200
323/323 [==============================] - 0s 884us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 6.3016e-04
Epoch 63/200
323/323 [==============================] - 0s 884us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 6.3016e-04
Epoch 64/200
323/323 [==============================] - 0s 867us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 6.3016e-04
Epoch 65/200
323/323 [==============================] - 0s 900us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 6.3016e-04
Epoch 66/200
288/323 [=========================>....] - ETA: 0s - loss: 0.0101 - mae: 0.0906 - mse: 0.0100
Epoch 66: ReduceLROnPlateau reducing learning rate to 0.00031508164829574525.
323/323 [==============================] - 0s 857us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 6.3016e-04
Epoch 67/200
323/323 [==============================] - 0s 835us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 3.1508e-04
Epoch 68/200
323/323 [==============================] - 0s 834us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 3.1508e-04
Epoch 69/200
323/323 [==============================] - 0s 884us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 3.1508e-04
Epoch 70/200
323/323 [==============================] - 0s 822us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 3.1508e-04
Epoch 71/200
323/323 [==============================] - 0s 833us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 3.1508e-04
Epoch 72/200
323/323 [==============================] - 0s 894us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 3.1508e-04
Epoch 73/200
323/323 [==============================] - 0s 888us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 3.1508e-04
Epoch 74/200
323/323 [==============================] - 0s 853us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 3.1508e-04
Epoch 75/200
323/323 [==============================] - 0s 819us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 3.1508e-04
Epoch 76/200
282/323 [=========================>....] - ETA: 0s - loss: 0.0101 - mae: 0.0908 - mse: 0.0101
Epoch 76: ReduceLROnPlateau reducing learning rate to 0.00015754082414787263.
323/323 [==============================] - 0s 838us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0907 - val_mse: 0.0100 - lr: 3.1508e-04
Epoch 77/200
323/323 [==============================] - 0s 872us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.5754e-04
Epoch 78/200
323/323 [==============================] - 0s 885us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.5754e-04
Epoch 79/200
323/323 [==============================] - 0s 813us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.5754e-04
Epoch 80/200
323/323 [==============================] - 0s 831us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.5754e-04
Epoch 81/200
323/323 [==============================] - 0s 886us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.5754e-04
Epoch 82/200
323/323 [==============================] - 0s 915us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.5754e-04
Epoch 83/200
323/323 [==============================] - 0s 913us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.5754e-04
Epoch 84/200
323/323 [==============================] - 0s 831us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.5754e-04
Epoch 85/200
323/323 [==============================] - 0s 884us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.5754e-04
Epoch 86/200
266/323 [=======================>......] - ETA: 0s - loss: 0.0101 - mae: 0.0907 - mse: 0.0100
Epoch 86: ReduceLROnPlateau reducing learning rate to 7.877041207393631e-05.
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.5754e-04
Epoch 87/200
323/323 [==============================] - 0s 886us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 7.8770e-05
Epoch 88/200
323/323 [==============================] - 0s 871us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 7.8770e-05
Epoch 89/200
323/323 [==============================] - 0s 865us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 7.8770e-05
Epoch 90/200
323/323 [==============================] - 0s 893us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 7.8770e-05
Epoch 91/200
323/323 [==============================] - 0s 854us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 7.8770e-05
Epoch 92/200
323/323 [==============================] - 0s 836us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 7.8770e-05
Epoch 93/200
323/323 [==============================] - 0s 885us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 7.8770e-05
Epoch 94/200
323/323 [==============================] - 0s 870us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 7.8770e-05
Epoch 95/200
323/323 [==============================] - 0s 831us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 7.8770e-05
Epoch 96/200
266/323 [=======================>......] - ETA: 0s - loss: 0.0101 - mae: 0.0908 - mse: 0.0101
Epoch 96: ReduceLROnPlateau reducing learning rate to 3.938520603696816e-05.
323/323 [==============================] - 0s 880us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 7.8770e-05
Epoch 97/200
323/323 [==============================] - 0s 834us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.9385e-05
Epoch 98/200
323/323 [==============================] - 0s 882us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.9385e-05
Epoch 99/200
323/323 [==============================] - 0s 882us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.9385e-05
Epoch 100/200
323/323 [==============================] - 0s 868us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.9385e-05
Epoch 101/200
323/323 [==============================] - 0s 836us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.9385e-05
Epoch 102/200
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.9385e-05
Epoch 103/200
323/323 [==============================] - 0s 884us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.9385e-05
Epoch 104/200
323/323 [==============================] - 0s 821us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.9385e-05
Epoch 105/200
323/323 [==============================] - 0s 832us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.9385e-05
Epoch 106/200
268/323 [=======================>......] - ETA: 0s - loss: 0.0100 - mae: 0.0904 - mse: 0.0100
Epoch 106: ReduceLROnPlateau reducing learning rate to 1.969260301848408e-05.
323/323 [==============================] - 0s 901us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 3.9385e-05
Epoch 107/200
323/323 [==============================] - 0s 876us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.9693e-05
Epoch 108/200
323/323 [==============================] - 0s 873us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.9693e-05
Epoch 109/200
323/323 [==============================] - 0s 866us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.9693e-05
Epoch 110/200
323/323 [==============================] - 0s 836us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.9693e-05
Epoch 111/200
323/323 [==============================] - 0s 856us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.9693e-05
Epoch 112/200
323/323 [==============================] - 0s 858us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.9693e-05
Epoch 113/200
323/323 [==============================] - 0s 865us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.9693e-05
Epoch 114/200
323/323 [==============================] - 0s 882us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.9693e-05
Epoch 115/200
323/323 [==============================] - 0s 886us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.9693e-05
Epoch 116/200
271/323 [========================>.....] - ETA: 0s - loss: 0.0101 - mae: 0.0907 - mse: 0.0101
Epoch 116: ReduceLROnPlateau reducing learning rate to 1e-05.
323/323 [==============================] - 0s 881us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.9693e-05
Epoch 117/200
323/323 [==============================] - 0s 875us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 118/200
323/323 [==============================] - 0s 837us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 119/200
323/323 [==============================] - 0s 870us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 120/200
323/323 [==============================] - 0s 832us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 121/200
323/323 [==============================] - 0s 891us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 122/200
323/323 [==============================] - 0s 818us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 123/200
323/323 [==============================] - 0s 934us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 124/200
323/323 [==============================] - 0s 879us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 125/200
323/323 [==============================] - 0s 825us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 126/200
323/323 [==============================] - 0s 892us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 127/200
323/323 [==============================] - 0s 829us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 128/200
323/323 [==============================] - 0s 862us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 129/200
323/323 [==============================] - 0s 880us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 130/200
323/323 [==============================] - 0s 833us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 131/200
323/323 [==============================] - 0s 873us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 132/200
323/323 [==============================] - 0s 928us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 133/200
323/323 [==============================] - 0s 831us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 134/200
323/323 [==============================] - 0s 882us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 135/200
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 136/200
323/323 [==============================] - 0s 818us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 137/200
323/323 [==============================] - 0s 839us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 138/200
323/323 [==============================] - 0s 879us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 139/200
323/323 [==============================] - 0s 832us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 140/200
323/323 [==============================] - 0s 931us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 141/200
323/323 [==============================] - 0s 819us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 142/200
323/323 [==============================] - 0s 837us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 143/200
323/323 [==============================] - 0s 877us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 144/200
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 145/200
323/323 [==============================] - 0s 836us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 146/200
323/323 [==============================] - 0s 866us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 147/200
323/323 [==============================] - 0s 885us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 148/200
323/323 [==============================] - 0s 930us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 149/200
323/323 [==============================] - 0s 836us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 150/200
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 151/200
323/323 [==============================] - 0s 885us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 152/200
323/323 [==============================] - 0s 872us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 153/200
323/323 [==============================] - 0s 829us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 154/200
323/323 [==============================] - 0s 881us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 155/200
323/323 [==============================] - 0s 884us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 156/200
323/323 [==============================] - 0s 882us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 157/200
323/323 [==============================] - 0s 825us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 158/200
323/323 [==============================] - 0s 826us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 159/200
323/323 [==============================] - 0s 888us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 160/200
323/323 [==============================] - 0s 878us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 161/200
323/323 [==============================] - 0s 874us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 162/200
323/323 [==============================] - 0s 817us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 163/200
323/323 [==============================] - 0s 889us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 164/200
323/323 [==============================] - 0s 835us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 165/200
323/323 [==============================] - 0s 926us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 166/200
323/323 [==============================] - 0s 885us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 167/200
323/323 [==============================] - 0s 869us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 168/200
323/323 [==============================] - 0s 835us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 169/200
323/323 [==============================] - 0s 889us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 170/200
323/323 [==============================] - 0s 891us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 171/200
323/323 [==============================] - 0s 858us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 172/200
323/323 [==============================] - 0s 902us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 173/200
323/323 [==============================] - 0s 878us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 174/200
323/323 [==============================] - 0s 881us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 175/200
323/323 [==============================] - 0s 865us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 176/200
323/323 [==============================] - 0s 867us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 177/200
323/323 [==============================] - 0s 838us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 178/200
323/323 [==============================] - 0s 878us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 179/200
323/323 [==============================] - 0s 835us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 180/200
323/323 [==============================] - 0s 865us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 181/200
323/323 [==============================] - 0s 866us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 182/200
323/323 [==============================] - 0s 841us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 183/200
323/323 [==============================] - 0s 846us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 184/200
323/323 [==============================] - 0s 835us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 185/200
323/323 [==============================] - 0s 819us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 186/200
323/323 [==============================] - 0s 869us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 187/200
323/323 [==============================] - 0s 848us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 188/200
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 189/200
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 190/200
323/323 [==============================] - 0s 913us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 191/200
323/323 [==============================] - 0s 874us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 192/200
323/323 [==============================] - 0s 846us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 193/200
323/323 [==============================] - 0s 831us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 194/200
323/323 [==============================] - 0s 884us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 195/200
323/323 [==============================] - 0s 870us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 196/200
323/323 [==============================] - 0s 883us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 197/200
323/323 [==============================] - 0s 858us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 198/200
323/323 [==============================] - 0s 854us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 199/200
323/323 [==============================] - 0s 877us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
Epoch 200/200
323/323 [==============================] - 0s 866us/step - loss: 0.0101 - mae: 0.0907 - mse: 0.0100 - val_loss: 0.0101 - val_mae: 0.0906 - val_mse: 0.0100 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.01008261251683028LR_[38]HN_16BS_10P_val_mseM_200epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
243/243 [==============================] - 1s 1ms/step - loss: 0.6506 - mae: 0.1795 - mse: 0.0455 - val_loss: 0.4948 - val_mae: 0.1532 - val_mse: 0.0255 - lr: 0.0101
Epoch 2/200
243/243 [==============================] - 0s 892us/step - loss: 0.3964 - mae: 0.1525 - mse: 0.0248 - val_loss: 0.3126 - val_mae: 0.1532 - val_mse: 0.0249 - lr: 0.0101
Epoch 3/200
243/243 [==============================] - 0s 846us/step - loss: 0.2525 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.2013 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0101
Epoch 4/200
243/243 [==============================] - 0s 921us/step - loss: 0.1645 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.1332 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0101
Epoch 5/200
243/243 [==============================] - 0s 917us/step - loss: 0.1106 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0916 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0101
Epoch 6/200
243/243 [==============================] - 0s 956us/step - loss: 0.0777 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0661 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0101
Epoch 7/200
243/243 [==============================] - 0s 890us/step - loss: 0.0574 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0504 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0101
Epoch 8/200
243/243 [==============================] - 0s 893us/step - loss: 0.0450 - mae: 0.1525 - mse: 0.0246 - val_loss: 0.0408 - val_mae: 0.1532 - val_mse: 0.0248 - lr: 0.0101
Epoch 9/200
243/243 [==============================] - 0s 827us/step - loss: 0.0374 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0348 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0101
Epoch 10/200
243/243 [==============================] - 0s 839us/step - loss: 0.0327 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0312 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0101
Epoch 11/200
243/243 [==============================] - 0s 840us/step - loss: 0.0297 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0289 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0101
Epoch 12/200
243/243 [==============================] - 0s 899us/step - loss: 0.0279 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0275 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0101
Epoch 13/200
243/243 [==============================] - 0s 882us/step - loss: 0.0268 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0266 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0101
Epoch 14/200
243/243 [==============================] - 0s 890us/step - loss: 0.0260 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0260 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0101
Epoch 15/200
243/243 [==============================] - 0s 894us/step - loss: 0.0256 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0256 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0101
Epoch 16/200
243/243 [==============================] - 0s 916us/step - loss: 0.0253 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0254 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0101
Epoch 17/200
243/243 [==============================] - 0s 932us/step - loss: 0.0250 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0252 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0101
Epoch 18/200
169/243 [===================>..........] - ETA: 0s - loss: 0.0251 - mae: 0.1532 - mse: 0.0246
Epoch 18: ReduceLROnPlateau reducing learning rate to 0.005041306372731924.
243/243 [==============================] - 0s 875us/step - loss: 0.0249 - mae: 0.1525 - mse: 0.0245 - val_loss: 0.0251 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0101
Epoch 19/200
243/243 [==============================] - 0s 886us/step - loss: 0.0248 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0250 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0050
Epoch 20/200
243/243 [==============================] - 0s 942us/step - loss: 0.0248 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0250 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0050
Epoch 21/200
243/243 [==============================] - 0s 848us/step - loss: 0.0247 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0050
Epoch 22/200
243/243 [==============================] - 0s 939us/step - loss: 0.0247 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0050
Epoch 23/200
243/243 [==============================] - 0s 838us/step - loss: 0.0247 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0050
Epoch 24/200
243/243 [==============================] - 0s 895us/step - loss: 0.0247 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0050
Epoch 25/200
243/243 [==============================] - 0s 893us/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0249 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0050
Epoch 26/200
243/243 [==============================] - 0s 895us/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0050
Epoch 27/200
243/243 [==============================] - 0s 979us/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0050
Epoch 28/200
243/243 [==============================] - 0s 884us/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0247 - lr: 0.0050
Epoch 29/200
243/243 [==============================] - 0s 871us/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0050
Epoch 30/200
243/243 [==============================] - 0s 828us/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0050
Epoch 31/200
243/243 [==============================] - 0s 892us/step - loss: 0.0246 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0050
Epoch 32/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0050
Epoch 33/200
176/243 [====================>.........] - ETA: 0s - loss: 0.0245 - mae: 0.1525 - mse: 0.0244
Epoch 33: ReduceLROnPlateau reducing learning rate to 0.002520653186365962.
243/243 [==============================] - 0s 917us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0248 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0050
Epoch 34/200
243/243 [==============================] - 0s 910us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0025
Epoch 35/200
243/243 [==============================] - 0s 887us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0025
Epoch 36/200
243/243 [==============================] - 0s 861us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0025
Epoch 37/200
243/243 [==============================] - 0s 925us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0025
Epoch 38/200
243/243 [==============================] - 0s 863us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0025
Epoch 39/200
243/243 [==============================] - 0s 913us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0025
Epoch 40/200
243/243 [==============================] - 0s 893us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0025
Epoch 41/200
243/243 [==============================] - 0s 911us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0025
Epoch 42/200
243/243 [==============================] - 0s 861us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0025
Epoch 43/200
182/243 [=====================>........] - ETA: 0s - loss: 0.0245 - mae: 0.1526 - mse: 0.0244
Epoch 43: ReduceLROnPlateau reducing learning rate to 0.001260326593182981.
243/243 [==============================] - 0s 888us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0025
Epoch 44/200
243/243 [==============================] - 0s 899us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0013
Epoch 45/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0013
Epoch 46/200
243/243 [==============================] - 0s 850us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0013
Epoch 47/200
243/243 [==============================] - 0s 973us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0013
Epoch 48/200
243/243 [==============================] - 0s 866us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0013
Epoch 49/200
243/243 [==============================] - 0s 884us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0013
Epoch 50/200
243/243 [==============================] - 0s 895us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0013
Epoch 51/200
243/243 [==============================] - 0s 900us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0013
Epoch 52/200
243/243 [==============================] - 0s 852us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0013
Epoch 53/200
175/243 [====================>.........] - ETA: 0s - loss: 0.0245 - mae: 0.1525 - mse: 0.0244
Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0006301632965914905.
243/243 [==============================] - 0s 930us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 0.0013
Epoch 54/200
243/243 [==============================] - 0s 887us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.3016e-04
Epoch 55/200
243/243 [==============================] - 0s 834us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.3016e-04
Epoch 56/200
243/243 [==============================] - 0s 825us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.3016e-04
Epoch 57/200
243/243 [==============================] - 0s 841us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.3016e-04
Epoch 58/200
243/243 [==============================] - 0s 979us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.3016e-04
Epoch 59/200
243/243 [==============================] - 0s 926us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.3016e-04
Epoch 60/200
243/243 [==============================] - 0s 908us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.3016e-04
Epoch 61/200
243/243 [==============================] - 0s 902us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.3016e-04
Epoch 62/200
243/243 [==============================] - 0s 833us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.3016e-04
Epoch 63/200
179/243 [=====================>........] - ETA: 0s - loss: 0.0245 - mae: 0.1525 - mse: 0.0244
Epoch 63: ReduceLROnPlateau reducing learning rate to 0.00031508164829574525.
243/243 [==============================] - 0s 851us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 6.3016e-04
Epoch 64/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.1508e-04
Epoch 65/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.1508e-04
Epoch 66/200
243/243 [==============================] - 0s 890us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.1508e-04
Epoch 67/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.1508e-04
Epoch 68/200
243/243 [==============================] - 0s 911us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.1508e-04
Epoch 69/200
243/243 [==============================] - 0s 897us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.1508e-04
Epoch 70/200
243/243 [==============================] - 0s 917us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.1508e-04
Epoch 71/200
243/243 [==============================] - 0s 879us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.1508e-04
Epoch 72/200
243/243 [==============================] - 0s 849us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.1508e-04
Epoch 73/200
158/243 [==================>...........] - ETA: 0s - loss: 0.0244 - mae: 0.1523 - mse: 0.0244
Epoch 73: ReduceLROnPlateau reducing learning rate to 0.00015754082414787263.
243/243 [==============================] - 0s 912us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.1508e-04
Epoch 74/200
243/243 [==============================] - 0s 921us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.5754e-04
Epoch 75/200
243/243 [==============================] - 0s 881us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.5754e-04
Epoch 76/200
243/243 [==============================] - 0s 883us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.5754e-04
Epoch 77/200
243/243 [==============================] - 0s 935us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.5754e-04
Epoch 78/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.5754e-04
Epoch 79/200
243/243 [==============================] - 0s 851us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.5754e-04
Epoch 80/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.5754e-04
Epoch 81/200
243/243 [==============================] - 0s 959us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.5754e-04
Epoch 82/200
243/243 [==============================] - 0s 893us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.5754e-04
Epoch 83/200
184/243 [=====================>........] - ETA: 0s - loss: 0.0244 - mae: 0.1524 - mse: 0.0244
Epoch 83: ReduceLROnPlateau reducing learning rate to 7.877041207393631e-05.
243/243 [==============================] - 0s 921us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.5754e-04
Epoch 84/200
243/243 [==============================] - 0s 885us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.8770e-05
Epoch 85/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.8770e-05
Epoch 86/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.8770e-05
Epoch 87/200
243/243 [==============================] - 0s 828us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.8770e-05
Epoch 88/200
243/243 [==============================] - 0s 827us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.8770e-05
Epoch 89/200
243/243 [==============================] - 0s 834us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.8770e-05
Epoch 90/200
243/243 [==============================] - 0s 884us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.8770e-05
Epoch 91/200
243/243 [==============================] - 0s 852us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.8770e-05
Epoch 92/200
243/243 [==============================] - 0s 956us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.8770e-05
Epoch 93/200
187/243 [======================>.......] - ETA: 0s - loss: 0.0244 - mae: 0.1523 - mse: 0.0243
Epoch 93: ReduceLROnPlateau reducing learning rate to 3.938520603696816e-05.
243/243 [==============================] - 0s 857us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 7.8770e-05
Epoch 94/200
243/243 [==============================] - 0s 872us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.9385e-05
Epoch 95/200
243/243 [==============================] - 0s 898us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.9385e-05
Epoch 96/200
243/243 [==============================] - 0s 908us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.9385e-05
Epoch 97/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.9385e-05
Epoch 98/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.9385e-05
Epoch 99/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.9385e-05
Epoch 100/200
243/243 [==============================] - 0s 890us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.9385e-05
Epoch 101/200
243/243 [==============================] - 0s 893us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.9385e-05
Epoch 102/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.9385e-05
Epoch 103/200
148/243 [=================>............] - ETA: 0s - loss: 0.0244 - mae: 0.1523 - mse: 0.0244
Epoch 103: ReduceLROnPlateau reducing learning rate to 1.969260301848408e-05.
243/243 [==============================] - 0s 917us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 3.9385e-05
Epoch 104/200
243/243 [==============================] - 0s 929us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.9693e-05
Epoch 105/200
243/243 [==============================] - 0s 855us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.9693e-05
Epoch 106/200
243/243 [==============================] - 0s 844us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.9693e-05
Epoch 107/200
243/243 [==============================] - 0s 872us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.9693e-05
Epoch 108/200
243/243 [==============================] - 0s 894us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.9693e-05
Epoch 109/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.9693e-05
Epoch 110/200
243/243 [==============================] - 0s 827us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.9693e-05
Epoch 111/200
243/243 [==============================] - 0s 826us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.9693e-05
Epoch 112/200
243/243 [==============================] - 0s 825us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.9693e-05
Epoch 113/200
171/243 [====================>.........] - ETA: 0s - loss: 0.0244 - mae: 0.1524 - mse: 0.0244
Epoch 113: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 890us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.9693e-05
Epoch 114/200
243/243 [==============================] - 0s 959us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 115/200
243/243 [==============================] - 0s 856us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 116/200
243/243 [==============================] - 0s 888us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 117/200
243/243 [==============================] - 0s 889us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 118/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 119/200
243/243 [==============================] - 0s 898us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 120/200
243/243 [==============================] - 0s 886us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 121/200
243/243 [==============================] - 0s 835us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 122/200
243/243 [==============================] - 0s 909us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 123/200
243/243 [==============================] - 0s 872us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 124/200
243/243 [==============================] - 0s 912us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 125/200
243/243 [==============================] - 0s 960us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 126/200
243/243 [==============================] - 0s 833us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 127/200
243/243 [==============================] - 0s 877us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 128/200
243/243 [==============================] - 0s 854us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 129/200
243/243 [==============================] - 0s 957us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 130/200
243/243 [==============================] - 0s 894us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 131/200
243/243 [==============================] - 0s 889us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 132/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 133/200
243/243 [==============================] - 0s 918us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 134/200
243/243 [==============================] - 0s 866us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 135/200
243/243 [==============================] - 0s 997us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 136/200
243/243 [==============================] - 0s 916us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 137/200
243/243 [==============================] - 0s 868us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 138/200
243/243 [==============================] - 0s 957us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 139/200
243/243 [==============================] - 0s 853us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 140/200
243/243 [==============================] - 0s 893us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 141/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 142/200
243/243 [==============================] - 0s 895us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 143/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 144/200
243/243 [==============================] - 0s 898us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 145/200
243/243 [==============================] - 0s 910us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 146/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 147/200
243/243 [==============================] - 0s 889us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 148/200
243/243 [==============================] - 0s 822us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 149/200
243/243 [==============================] - 0s 832us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 150/200
243/243 [==============================] - 0s 823us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 151/200
243/243 [==============================] - 0s 846us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 152/200
243/243 [==============================] - 0s 918us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 153/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 154/200
243/243 [==============================] - 0s 851us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 155/200
243/243 [==============================] - 0s 894us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 156/200
243/243 [==============================] - 0s 889us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 157/200
243/243 [==============================] - 0s 895us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 158/200
243/243 [==============================] - 0s 936us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 159/200
243/243 [==============================] - 0s 876us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 160/200
243/243 [==============================] - 0s 874us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 161/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 162/200
243/243 [==============================] - 0s 894us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 163/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 164/200
243/243 [==============================] - 0s 847us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 165/200
243/243 [==============================] - 0s 882us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 166/200
243/243 [==============================] - 0s 882us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 167/200
243/243 [==============================] - 0s 835us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 168/200
243/243 [==============================] - 0s 830us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 169/200
243/243 [==============================] - 0s 973us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 170/200
243/243 [==============================] - 0s 918us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 171/200
243/243 [==============================] - 0s 889us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 172/200
243/243 [==============================] - 0s 848us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 173/200
243/243 [==============================] - 0s 895us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 174/200
243/243 [==============================] - 0s 889us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 175/200
243/243 [==============================] - 0s 894us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 176/200
243/243 [==============================] - 0s 895us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 177/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 178/200
243/243 [==============================] - 0s 868us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 179/200
243/243 [==============================] - 0s 893us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 180/200
243/243 [==============================] - 0s 958us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 181/200
243/243 [==============================] - 0s 857us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 182/200
243/243 [==============================] - 0s 937us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 183/200
243/243 [==============================] - 0s 882us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 184/200
243/243 [==============================] - 0s 830us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 185/200
243/243 [==============================] - 0s 888us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 186/200
243/243 [==============================] - 0s 913us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 187/200
243/243 [==============================] - 0s 911us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 188/200
243/243 [==============================] - 0s 921us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 189/200
243/243 [==============================] - 0s 891us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 190/200
243/243 [==============================] - 0s 877us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 191/200
243/243 [==============================] - 0s 951us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 192/200
243/243 [==============================] - 0s 875us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 193/200
243/243 [==============================] - 0s 890us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 194/200
243/243 [==============================] - 0s 890us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 195/200
243/243 [==============================] - 0s 917us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 196/200
243/243 [==============================] - 0s 893us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 197/200
243/243 [==============================] - 0s 892us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 198/200
243/243 [==============================] - 0s 845us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 199/200
243/243 [==============================] - 0s 875us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
Epoch 200/200
243/243 [==============================] - 0s 905us/step - loss: 0.0245 - mae: 0.1525 - mse: 0.0244 - val_loss: 0.0247 - val_mae: 0.1532 - val_mse: 0.0246 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.01008261251683028LR_[38]HN_16BS_10P_val_mseM_200epochs/model_3.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
243/243 [==============================] - 1s 1ms/step - loss: 0.6285 - mae: 0.2078 - mse: 0.0533 - val_loss: 0.4884 - val_mae: 0.2032 - val_mse: 0.0427 - lr: 0.0101
Epoch 2/200
243/243 [==============================] - 0s 827us/step - loss: 0.3951 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.3156 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0101
Epoch 3/200
243/243 [==============================] - 0s 844us/step - loss: 0.2585 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.2100 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0101
Epoch 4/200
243/243 [==============================] - 0s 873us/step - loss: 0.1749 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.1453 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0101
Epoch 5/200
243/243 [==============================] - 0s 892us/step - loss: 0.1238 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.1058 - val_mae: 0.2032 - val_mse: 0.0426 - lr: 0.0101
Epoch 6/200
243/243 [==============================] - 0s 894us/step - loss: 0.0925 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0816 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0101
Epoch 7/200
243/243 [==============================] - 0s 890us/step - loss: 0.0733 - mae: 0.2025 - mse: 0.0423 - val_loss: 0.0667 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0101
Epoch 8/200
243/243 [==============================] - 0s 920us/step - loss: 0.0615 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0576 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0101
Epoch 9/200
243/243 [==============================] - 0s 880us/step - loss: 0.0543 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0520 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0101
Epoch 10/200
243/243 [==============================] - 0s 892us/step - loss: 0.0498 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0485 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0101
Epoch 11/200
243/243 [==============================] - 0s 893us/step - loss: 0.0471 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0464 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0101
Epoch 12/200
189/243 [======================>.......] - ETA: 0s - loss: 0.0455 - mae: 0.2026 - mse: 0.0422
Epoch 12: ReduceLROnPlateau reducing learning rate to 0.005041306372731924.
243/243 [==============================] - 0s 894us/step - loss: 0.0454 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0450 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0101
Epoch 13/200
243/243 [==============================] - 0s 825us/step - loss: 0.0445 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0445 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0050
Epoch 14/200
243/243 [==============================] - 0s 895us/step - loss: 0.0441 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0442 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0050
Epoch 15/200
243/243 [==============================] - 0s 848us/step - loss: 0.0437 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0439 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0050
Epoch 16/200
243/243 [==============================] - 0s 892us/step - loss: 0.0435 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0436 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0050
Epoch 17/200
243/243 [==============================] - 0s 894us/step - loss: 0.0433 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0434 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0050
Epoch 18/200
243/243 [==============================] - 0s 956us/step - loss: 0.0431 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0433 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0050
Epoch 19/200
243/243 [==============================] - 0s 895us/step - loss: 0.0429 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0432 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0050
Epoch 20/200
243/243 [==============================] - 0s 912us/step - loss: 0.0428 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0431 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0050
Epoch 21/200
243/243 [==============================] - 0s 890us/step - loss: 0.0427 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0430 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0050
Epoch 22/200
192/243 [======================>.......] - ETA: 0s - loss: 0.0428 - mae: 0.2029 - mse: 0.0423
Epoch 22: ReduceLROnPlateau reducing learning rate to 0.002520653186365962.
243/243 [==============================] - 0s 891us/step - loss: 0.0427 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0429 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0050
Epoch 23/200
243/243 [==============================] - 0s 870us/step - loss: 0.0426 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0429 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 24/200
243/243 [==============================] - 0s 916us/step - loss: 0.0426 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0429 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 25/200
243/243 [==============================] - 0s 892us/step - loss: 0.0426 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 26/200
243/243 [==============================] - 0s 892us/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 27/200
243/243 [==============================] - 0s 852us/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 28/200
243/243 [==============================] - 0s 958us/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 29/200
243/243 [==============================] - 0s 887us/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0428 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 30/200
243/243 [==============================] - 0s 898us/step - loss: 0.0425 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 31/200
243/243 [==============================] - 0s 844us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 32/200
243/243 [==============================] - 0s 944us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 33/200
243/243 [==============================] - 0s 848us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 34/200
243/243 [==============================] - 0s 891us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 35/200
243/243 [==============================] - 0s 895us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 36/200
243/243 [==============================] - 0s 891us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 37/200
243/243 [==============================] - 0s 897us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0427 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 38/200
175/243 [====================>.........] - ETA: 0s - loss: 0.0424 - mae: 0.2025 - mse: 0.0422
Epoch 38: ReduceLROnPlateau reducing learning rate to 0.001260326593182981.
243/243 [==============================] - 0s 942us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0025
Epoch 39/200
243/243 [==============================] - 0s 922us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0013
Epoch 40/200
243/243 [==============================] - 0s 896us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0013
Epoch 41/200
243/243 [==============================] - 0s 881us/step - loss: 0.0424 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0013
Epoch 42/200
243/243 [==============================] - 0s 836us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0013
Epoch 43/200
243/243 [==============================] - 0s 899us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0013
Epoch 44/200
243/243 [==============================] - 0s 908us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0013
Epoch 45/200
243/243 [==============================] - 0s 954us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0013
Epoch 46/200
243/243 [==============================] - 0s 910us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0013
Epoch 47/200
243/243 [==============================] - 0s 891us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0013
Epoch 48/200
185/243 [=====================>........] - ETA: 0s - loss: 0.0423 - mae: 0.2025 - mse: 0.0421
Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0006301632965914905.
243/243 [==============================] - 0s 827us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 0.0013
Epoch 49/200
243/243 [==============================] - 0s 939us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 6.3016e-04
Epoch 50/200
243/243 [==============================] - 0s 868us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 6.3016e-04
Epoch 51/200
243/243 [==============================] - 0s 892us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 6.3016e-04
Epoch 52/200
243/243 [==============================] - 0s 895us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 6.3016e-04
Epoch 53/200
243/243 [==============================] - 0s 889us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 6.3016e-04
Epoch 54/200
243/243 [==============================] - 0s 892us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 6.3016e-04
Epoch 55/200
243/243 [==============================] - 0s 928us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 6.3016e-04
Epoch 56/200
243/243 [==============================] - 0s 881us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 6.3016e-04
Epoch 57/200
243/243 [==============================] - 0s 891us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 6.3016e-04
Epoch 58/200
185/243 [=====================>........] - ETA: 0s - loss: 0.0424 - mae: 0.2028 - mse: 0.0423
Epoch 58: ReduceLROnPlateau reducing learning rate to 0.00031508164829574525.
243/243 [==============================] - 0s 890us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 6.3016e-04
Epoch 59/200
243/243 [==============================] - 0s 878us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.1508e-04
Epoch 60/200
243/243 [==============================] - 0s 909us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.1508e-04
Epoch 61/200
243/243 [==============================] - 0s 890us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.1508e-04
Epoch 62/200
243/243 [==============================] - 0s 892us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.1508e-04
Epoch 63/200
243/243 [==============================] - 0s 827us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.1508e-04
Epoch 64/200
243/243 [==============================] - 0s 890us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.1508e-04
Epoch 65/200
243/243 [==============================] - 0s 827us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.1508e-04
Epoch 66/200
243/243 [==============================] - 0s 858us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.1508e-04
Epoch 67/200
243/243 [==============================] - 0s 896us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.1508e-04
Epoch 68/200
153/243 [=================>............] - ETA: 0s - loss: 0.0421 - mae: 0.2021 - mse: 0.0420
Epoch 68: ReduceLROnPlateau reducing learning rate to 0.00015754082414787263.
243/243 [==============================] - 0s 858us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.1508e-04
Epoch 69/200
243/243 [==============================] - 0s 850us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.5754e-04
Epoch 70/200
243/243 [==============================] - 0s 943us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.5754e-04
Epoch 71/200
243/243 [==============================] - 0s 906us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.5754e-04
Epoch 72/200
243/243 [==============================] - 0s 891us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.5754e-04
Epoch 73/200
243/243 [==============================] - 0s 891us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.5754e-04
Epoch 74/200
243/243 [==============================] - 0s 916us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.5754e-04
Epoch 75/200
243/243 [==============================] - 0s 831us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.5754e-04
Epoch 76/200
243/243 [==============================] - 0s 822us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.5754e-04
Epoch 77/200
243/243 [==============================] - 0s 827us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.5754e-04
Epoch 78/200
196/243 [=======================>......] - ETA: 0s - loss: 0.0423 - mae: 0.2025 - mse: 0.0422
Epoch 78: ReduceLROnPlateau reducing learning rate to 7.877041207393631e-05.
243/243 [==============================] - 0s 890us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.5754e-04
Epoch 79/200
243/243 [==============================] - 0s 841us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.8770e-05
Epoch 80/200
243/243 [==============================] - 0s 837us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.8770e-05
Epoch 81/200
243/243 [==============================] - 0s 961us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.8770e-05
Epoch 82/200
243/243 [==============================] - 0s 934us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.8770e-05
Epoch 83/200
243/243 [==============================] - 0s 837us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.8770e-05
Epoch 84/200
243/243 [==============================] - 0s 830us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.8770e-05
Epoch 85/200
243/243 [==============================] - 0s 849us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.8770e-05
Epoch 86/200
243/243 [==============================] - 0s 892us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.8770e-05
Epoch 87/200
243/243 [==============================] - 0s 894us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.8770e-05
Epoch 88/200
174/243 [====================>.........] - ETA: 0s - loss: 0.0423 - mae: 0.2025 - mse: 0.0422
Epoch 88: ReduceLROnPlateau reducing learning rate to 3.938520603696816e-05.
243/243 [==============================] - 0s 897us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 7.8770e-05
Epoch 89/200
243/243 [==============================] - 0s 887us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.9385e-05
Epoch 90/200
243/243 [==============================] - 0s 889us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.9385e-05
Epoch 91/200
243/243 [==============================] - 0s 916us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.9385e-05
Epoch 92/200
243/243 [==============================] - 0s 925us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.9385e-05
Epoch 93/200
243/243 [==============================] - 0s 928us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.9385e-05
Epoch 94/200
243/243 [==============================] - 0s 887us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.9385e-05
Epoch 95/200
243/243 [==============================] - 0s 917us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.9385e-05
Epoch 96/200
243/243 [==============================] - 0s 889us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.9385e-05
Epoch 97/200
243/243 [==============================] - 0s 894us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.9385e-05
Epoch 98/200
175/243 [====================>.........] - ETA: 0s - loss: 0.0423 - mae: 0.2026 - mse: 0.0422
Epoch 98: ReduceLROnPlateau reducing learning rate to 1.969260301848408e-05.
243/243 [==============================] - 0s 901us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 3.9385e-05
Epoch 99/200
243/243 [==============================] - 0s 880us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.9693e-05
Epoch 100/200
243/243 [==============================] - 0s 826us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.9693e-05
Epoch 101/200
243/243 [==============================] - 0s 984us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.9693e-05
Epoch 102/200
243/243 [==============================] - 0s 896us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.9693e-05
Epoch 103/200
243/243 [==============================] - 0s 947us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.9693e-05
Epoch 104/200
243/243 [==============================] - 0s 909us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.9693e-05
Epoch 105/200
243/243 [==============================] - 0s 873us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.9693e-05
Epoch 106/200
243/243 [==============================] - 0s 897us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.9693e-05
Epoch 107/200
243/243 [==============================] - 0s 923us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.9693e-05
Epoch 108/200
193/243 [======================>.......] - ETA: 0s - loss: 0.0422 - mae: 0.2023 - mse: 0.0421
Epoch 108: ReduceLROnPlateau reducing learning rate to 1e-05.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0006s vs `on_train_batch_end` time: 0.0020s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0001s). Check your callbacks.
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 148/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 149/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 150/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 151/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 152/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 153/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 154/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 155/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 156/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 157/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 158/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 159/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 160/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 161/200
243/243 [==============================] - 0s 914us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 162/200
243/243 [==============================] - 0s 863us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 163/200
243/243 [==============================] - 0s 894us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 164/200
243/243 [==============================] - 0s 890us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 165/200
243/243 [==============================] - 0s 827us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 166/200
243/243 [==============================] - 0s 826us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 167/200
243/243 [==============================] - 0s 899us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 168/200
243/243 [==============================] - 0s 886us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 169/200
243/243 [==============================] - 0s 893us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 170/200
243/243 [==============================] - 0s 896us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 171/200
243/243 [==============================] - 0s 892us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 172/200
243/243 [==============================] - 0s 892us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 173/200
243/243 [==============================] - 0s 896us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 174/200
243/243 [==============================] - 0s 887us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 175/200
243/243 [==============================] - 0s 892us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 176/200
243/243 [==============================] - 0s 896us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 177/200
243/243 [==============================] - 0s 910us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 178/200
243/243 [==============================] - 0s 894us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 179/200
243/243 [==============================] - 0s 890us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 180/200
243/243 [==============================] - 0s 895us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 181/200
243/243 [==============================] - 0s 903us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 182/200
243/243 [==============================] - 0s 903us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 183/200
243/243 [==============================] - 0s 954us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 184/200
243/243 [==============================] - 0s 885us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 185/200
243/243 [==============================] - 0s 891us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 186/200
243/243 [==============================] - 0s 896us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 187/200
243/243 [==============================] - 0s 911us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 188/200
243/243 [==============================] - 0s 829us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 189/200
243/243 [==============================] - 0s 826us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 190/200
243/243 [==============================] - 0s 894us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 191/200
243/243 [==============================] - 0s 891us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 192/200
243/243 [==============================] - 0s 959us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 193/200
243/243 [==============================] - 0s 913us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 194/200
243/243 [==============================] - 0s 896us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 195/200
243/243 [==============================] - 0s 890us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 196/200
243/243 [==============================] - 0s 890us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 197/200
243/243 [==============================] - 0s 890us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 198/200
243/243 [==============================] - 0s 892us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 199/200
243/243 [==============================] - 0s 894us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
Epoch 200/200
243/243 [==============================] - 0s 898us/step - loss: 0.0423 - mae: 0.2025 - mse: 0.0422 - val_loss: 0.0426 - val_mae: 0.2032 - val_mse: 0.0425 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.01008261251683028LR_[38]HN_16BS_10P_val_mseM_200epochs/model_4.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
243/243 [==============================] - 1s 1ms/step - loss: 0.6785 - mae: 0.2528 - mse: 0.0723 - val_loss: 0.5347 - val_mae: 0.2532 - val_mse: 0.0654 - lr: 0.0101
Epoch 2/200
243/243 [==============================] - 0s 916us/step - loss: 0.4364 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.3526 - val_mae: 0.2532 - val_mse: 0.0654 - lr: 0.0101
Epoch 3/200
243/243 [==============================] - 0s 862us/step - loss: 0.2923 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.2413 - val_mae: 0.2532 - val_mse: 0.0654 - lr: 0.0101
Epoch 4/200
243/243 [==============================] - 0s 894us/step - loss: 0.2043 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.1732 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 5/200
243/243 [==============================] - 0s 850us/step - loss: 0.1504 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.1315 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 6/200
243/243 [==============================] - 0s 916us/step - loss: 0.1175 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.1060 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 7/200
243/243 [==============================] - 0s 868us/step - loss: 0.0973 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0904 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 8/200
243/243 [==============================] - 0s 889us/step - loss: 0.0849 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0808 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 9/200
243/243 [==============================] - 0s 852us/step - loss: 0.0773 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0749 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 10/200
243/243 [==============================] - 0s 959us/step - loss: 0.0726 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0713 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 11/200
243/243 [==============================] - 0s 893us/step - loss: 0.0698 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0691 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 12/200
243/243 [==============================] - 0s 889us/step - loss: 0.0680 - mae: 0.2525 - mse: 0.0650 - val_loss: 0.0677 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 13/200
243/243 [==============================] - 0s 845us/step - loss: 0.0669 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0668 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 14/200
243/243 [==============================] - 0s 873us/step - loss: 0.0662 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0663 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 15/200
243/243 [==============================] - 0s 850us/step - loss: 0.0658 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0660 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 16/200
243/243 [==============================] - 0s 892us/step - loss: 0.0655 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0657 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 17/200
243/243 [==============================] - 0s 892us/step - loss: 0.0653 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0656 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 18/200
198/243 [=======================>......] - ETA: 0s - loss: 0.0651 - mae: 0.2524 - mse: 0.0649
Epoch 18: ReduceLROnPlateau reducing learning rate to 0.005041306372731924.
243/243 [==============================] - 0s 892us/step - loss: 0.0652 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0655 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0101
Epoch 19/200
243/243 [==============================] - 0s 892us/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0655 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0050
Epoch 20/200
243/243 [==============================] - 0s 904us/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0050
Epoch 21/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0050
Epoch 22/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0651 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0050
Epoch 23/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0050
Epoch 24/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0050
Epoch 25/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0050
Epoch 26/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0654 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0050
Epoch 27/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0050
Epoch 28/200
230/243 [===========================>..] - ETA: 0s - loss: 0.0650 - mae: 0.2526 - mse: 0.0650
Epoch 28: ReduceLROnPlateau reducing learning rate to 0.002520653186365962.
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0050
Epoch 29/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0025
Epoch 30/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0025
Epoch 31/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0025
Epoch 32/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0025
Epoch 33/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0025
Epoch 34/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0025
Epoch 35/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0025
Epoch 36/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0025
Epoch 37/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0025
Epoch 38/200
162/243 [===================>..........] - ETA: 0s - loss: 0.0649 - mae: 0.2524 - mse: 0.0649
Epoch 38: ReduceLROnPlateau reducing learning rate to 0.001260326593182981.
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0025
Epoch 39/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0013
Epoch 40/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0013
Epoch 41/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0013
Epoch 42/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0013
Epoch 43/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0013
Epoch 44/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0013
Epoch 45/200
243/243 [==============================] - 0s 912us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0013
Epoch 46/200
243/243 [==============================] - 0s 876us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0013
Epoch 47/200
243/243 [==============================] - 0s 889us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0013
Epoch 48/200
191/243 [======================>.......] - ETA: 0s - loss: 0.0649 - mae: 0.2524 - mse: 0.0649
Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0006301632965914905.
243/243 [==============================] - 0s 918us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 0.0013
Epoch 49/200
243/243 [==============================] - 0s 874us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.3016e-04
Epoch 50/200
243/243 [==============================] - 0s 906us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.3016e-04
Epoch 51/200
243/243 [==============================] - 0s 892us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.3016e-04
Epoch 52/200
243/243 [==============================] - 0s 828us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.3016e-04
Epoch 53/200
243/243 [==============================] - 0s 826us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.3016e-04
Epoch 54/200
243/243 [==============================] - 0s 891us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.3016e-04
Epoch 55/200
243/243 [==============================] - 0s 901us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.3016e-04
Epoch 56/200
243/243 [==============================] - 0s 885us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.3016e-04
Epoch 57/200
243/243 [==============================] - 0s 917us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.3016e-04
Epoch 58/200
185/243 [=====================>........] - ETA: 0s - loss: 0.0651 - mae: 0.2528 - mse: 0.0651
Epoch 58: ReduceLROnPlateau reducing learning rate to 0.00031508164829574525.
243/243 [==============================] - 0s 909us/step - loss: 0.0650 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 6.3016e-04
Epoch 59/200
243/243 [==============================] - 0s 842us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.1508e-04
Epoch 60/200
243/243 [==============================] - 0s 940us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.1508e-04
Epoch 61/200
243/243 [==============================] - 0s 890us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.1508e-04
Epoch 62/200
243/243 [==============================] - 0s 833us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.1508e-04
Epoch 63/200
243/243 [==============================] - 0s 883us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.1508e-04
Epoch 64/200
243/243 [==============================] - 0s 894us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.1508e-04
Epoch 65/200
243/243 [==============================] - 0s 892us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.1508e-04
Epoch 66/200
243/243 [==============================] - 0s 913us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.1508e-04
Epoch 67/200
243/243 [==============================] - 0s 909us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.1508e-04
Epoch 68/200
212/243 [=========================>....] - ETA: 0s - loss: 0.0649 - mae: 0.2525 - mse: 0.0649
Epoch 68: ReduceLROnPlateau reducing learning rate to 0.00015754082414787263.
243/243 [==============================] - 0s 871us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.1508e-04
Epoch 69/200
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.5754e-04
Epoch 70/200
243/243 [==============================] - 0s 891us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.5754e-04
Epoch 71/200
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.5754e-04
Epoch 72/200
243/243 [==============================] - 0s 919us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.5754e-04
Epoch 73/200
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.5754e-04
Epoch 74/200
243/243 [==============================] - 0s 892us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.5754e-04
Epoch 75/200
243/243 [==============================] - 0s 897us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.5754e-04
Epoch 76/200
243/243 [==============================] - 0s 891us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.5754e-04
Epoch 77/200
243/243 [==============================] - 0s 823us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.5754e-04
Epoch 78/200
177/243 [====================>.........] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.0649
Epoch 78: ReduceLROnPlateau reducing learning rate to 7.877041207393631e-05.
243/243 [==============================] - 0s 873us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.5754e-04
Epoch 79/200
243/243 [==============================] - 0s 872us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.8770e-05
Epoch 80/200
243/243 [==============================] - 0s 891us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.8770e-05
Epoch 81/200
243/243 [==============================] - 0s 872us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.8770e-05
Epoch 82/200
243/243 [==============================] - 0s 865us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.8770e-05
Epoch 83/200
243/243 [==============================] - 0s 876us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.8770e-05
Epoch 84/200
243/243 [==============================] - 0s 886us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.8770e-05
Epoch 85/200
243/243 [==============================] - 0s 890us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.8770e-05
Epoch 86/200
243/243 [==============================] - 0s 852us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.8770e-05
Epoch 87/200
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.8770e-05
Epoch 88/200
178/243 [====================>.........] - ETA: 0s - loss: 0.0649 - mae: 0.2525 - mse: 0.0649
Epoch 88: ReduceLROnPlateau reducing learning rate to 3.938520603696816e-05.
243/243 [==============================] - 0s 891us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 7.8770e-05
Epoch 89/200
243/243 [==============================] - 0s 898us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.9385e-05
Epoch 90/200
243/243 [==============================] - 0s 838us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.9385e-05
Epoch 91/200
243/243 [==============================] - 0s 873us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.9385e-05
Epoch 92/200
243/243 [==============================] - 0s 916us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.9385e-05
Epoch 93/200
243/243 [==============================] - 0s 892us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.9385e-05
Epoch 94/200
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.9385e-05
Epoch 95/200
243/243 [==============================] - 0s 827us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.9385e-05
Epoch 96/200
243/243 [==============================] - 0s 891us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.9385e-05
Epoch 97/200
243/243 [==============================] - 0s 892us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.9385e-05
Epoch 98/200
194/243 [======================>.......] - ETA: 0s - loss: 0.0651 - mae: 0.2528 - mse: 0.0650
Epoch 98: ReduceLROnPlateau reducing learning rate to 1.969260301848408e-05.
243/243 [==============================] - 0s 828us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 3.9385e-05
Epoch 99/200
243/243 [==============================] - 0s 824us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.9693e-05
Epoch 100/200
243/243 [==============================] - 0s 852us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.9693e-05
Epoch 101/200
243/243 [==============================] - 0s 892us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.9693e-05
Epoch 102/200
243/243 [==============================] - 0s 895us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.9693e-05
Epoch 103/200
243/243 [==============================] - 0s 892us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.9693e-05
Epoch 104/200
243/243 [==============================] - 0s 889us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.9693e-05
Epoch 105/200
243/243 [==============================] - 0s 914us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.9693e-05
Epoch 106/200
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.9693e-05
Epoch 107/200
243/243 [==============================] - 0s 956us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.9693e-05
Epoch 108/200
228/243 [===========================>..] - ETA: 0s - loss: 0.0650 - mae: 0.2525 - mse: 0.0649
Epoch 108: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.9693e-05
Epoch 109/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 110/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 111/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 112/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 113/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 114/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 115/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 116/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 117/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 118/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 119/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 120/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 121/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 122/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 123/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 124/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 125/200
243/243 [==============================] - 0s 992us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 126/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 127/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 128/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 129/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 130/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 131/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 132/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 133/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 134/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 135/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 136/200
243/243 [==============================] - 0s 980us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 137/200
243/243 [==============================] - 0s 888us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 138/200
243/243 [==============================] - 0s 896us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 139/200
243/243 [==============================] - 0s 886us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 140/200
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 141/200
243/243 [==============================] - 0s 870us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 142/200
243/243 [==============================] - 0s 879us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 143/200
243/243 [==============================] - 0s 887us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 144/200
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 145/200
243/243 [==============================] - 0s 888us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 146/200
243/243 [==============================] - 0s 828us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 147/200
243/243 [==============================] - 0s 892us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 148/200
243/243 [==============================] - 0s 854us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 149/200
243/243 [==============================] - 0s 891us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 150/200
243/243 [==============================] - 0s 896us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 151/200
243/243 [==============================] - 0s 951us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 152/200
243/243 [==============================] - 0s 874us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 153/200
243/243 [==============================] - 0s 844us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 154/200
243/243 [==============================] - 0s 916us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 155/200
243/243 [==============================] - 0s 891us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 156/200
243/243 [==============================] - 0s 894us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 157/200
243/243 [==============================] - 0s 889us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 158/200
243/243 [==============================] - 0s 827us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 159/200
243/243 [==============================] - 0s 888us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 160/200
243/243 [==============================] - 0s 830us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 161/200
243/243 [==============================] - 0s 849us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 162/200
243/243 [==============================] - 0s 899us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 163/200
243/243 [==============================] - 0s 886us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 164/200
243/243 [==============================] - 0s 894us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 165/200
243/243 [==============================] - 0s 891us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 166/200
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 167/200
243/243 [==============================] - 0s 952us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 168/200
243/243 [==============================] - 0s 893us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 169/200
243/243 [==============================] - 0s 858us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 170/200
243/243 [==============================] - 0s 955us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 171/200
243/243 [==============================] - 0s 910us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 172/200
243/243 [==============================] - 0s 959us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 173/200
243/243 [==============================] - 0s 920us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 174/200
243/243 [==============================] - 0s 877us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 175/200
243/243 [==============================] - 0s 857us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 176/200
243/243 [==============================] - 0s 894us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 177/200
243/243 [==============================] - 0s 890us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 178/200
243/243 [==============================] - 0s 846us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 179/200
243/243 [==============================] - 0s 932us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 180/200
243/243 [==============================] - 0s 832us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 181/200
243/243 [==============================] - 0s 852us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 182/200
243/243 [==============================] - 0s 897us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 183/200
243/243 [==============================] - 0s 888us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 184/200
243/243 [==============================] - 0s 958us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 185/200
243/243 [==============================] - 0s 903us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 186/200
243/243 [==============================] - 0s 885us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 187/200
243/243 [==============================] - 0s 851us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 188/200
243/243 [==============================] - 0s 825us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 189/200
243/243 [==============================] - 0s 890us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 190/200
243/243 [==============================] - 0s 923us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 191/200
243/243 [==============================] - 0s 930us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 192/200
243/243 [==============================] - 0s 827us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 193/200
243/243 [==============================] - 0s 911us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 194/200
243/243 [==============================] - 0s 966us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 195/200
243/243 [==============================] - 0s 890us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 196/200
243/243 [==============================] - 0s 910us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 197/200
243/243 [==============================] - 0s 861us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 198/200
243/243 [==============================] - 0s 918us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 199/200
243/243 [==============================] - 0s 889us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
Epoch 200/200
243/243 [==============================] - 0s 892us/step - loss: 0.0649 - mae: 0.2525 - mse: 0.0649 - val_loss: 0.0653 - val_mae: 0.2532 - val_mse: 0.0653 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.01008261251683028LR_[38]HN_16BS_10P_val_mseM_200epochs/model_5.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
243/243 [==============================] - 1s 1ms/step - loss: 0.6292 - mae: 0.3025 - mse: 0.0975 - val_loss: 0.5047 - val_mae: 0.3032 - val_mse: 0.0932 - lr: 0.0101
Epoch 2/200
243/243 [==============================] - 0s 827us/step - loss: 0.4183 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.3449 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0101
Epoch 3/200
243/243 [==============================] - 0s 883us/step - loss: 0.2919 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.2472 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0101
Epoch 4/200
243/243 [==============================] - 0s 897us/step - loss: 0.2147 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1875 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0101
Epoch 5/200
243/243 [==============================] - 0s 893us/step - loss: 0.1674 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1509 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0101
Epoch 6/200
243/243 [==============================] - 0s 894us/step - loss: 0.1385 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1286 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0101
Epoch 7/200
243/243 [==============================] - 0s 893us/step - loss: 0.1208 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1149 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0101
Epoch 8/200
243/243 [==============================] - 0s 827us/step - loss: 0.1100 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1065 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0101
Epoch 9/200
243/243 [==============================] - 0s 892us/step - loss: 0.1033 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.1014 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0101
Epoch 10/200
243/243 [==============================] - 0s 891us/step - loss: 0.0992 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0982 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0101
Epoch 11/200
185/243 [=====================>........] - ETA: 0s - loss: 0.0970 - mae: 0.3025 - mse: 0.0927
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.005041306372731924.
243/243 [==============================] - 0s 896us/step - loss: 0.0967 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0963 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0101
Epoch 12/200
243/243 [==============================] - 0s 882us/step - loss: 0.0955 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0956 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0050
Epoch 13/200
243/243 [==============================] - 0s 925us/step - loss: 0.0949 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0951 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0050
Epoch 14/200
243/243 [==============================] - 0s 977us/step - loss: 0.0944 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0946 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0050
Epoch 15/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0941 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0943 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0050
Epoch 16/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0938 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0941 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0050
Epoch 17/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0936 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0939 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0050
Epoch 18/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0934 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0937 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0050
Epoch 19/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0932 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0936 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0050
Epoch 20/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0931 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0935 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0050
Epoch 21/200
157/243 [==================>...........] - ETA: 0s - loss: 0.0932 - mae: 0.3028 - mse: 0.0928
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.002520653186365962.
243/243 [==============================] - 0s 1ms/step - loss: 0.0930 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0934 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0050
Epoch 22/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0930 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0934 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0025
Epoch 23/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0025
Epoch 24/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0025
Epoch 25/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0025
Epoch 26/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0025
Epoch 27/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0929 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0933 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0025
Epoch 28/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0025
Epoch 29/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0025
Epoch 30/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0025
Epoch 31/200
207/243 [========================>.....] - ETA: 0s - loss: 0.0927 - mae: 0.3024 - mse: 0.0926
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.001260326593182981.
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0025
Epoch 32/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0013
Epoch 33/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0013
Epoch 34/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0013
Epoch 35/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0013
Epoch 36/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0013
Epoch 37/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0013
Epoch 38/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0013
Epoch 39/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0928 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0013
Epoch 40/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0013
Epoch 41/200
180/243 [=====================>........] - ETA: 0s - loss: 0.0928 - mae: 0.3027 - mse: 0.0927
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0006301632965914905.
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 0.0013
Epoch 42/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.3016e-04
Epoch 43/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.3016e-04
Epoch 44/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.3016e-04
Epoch 45/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0932 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.3016e-04
Epoch 46/200
243/243 [==============================] - 0s 980us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.3016e-04
Epoch 47/200
243/243 [==============================] - 0s 896us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.3016e-04
Epoch 48/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.3016e-04
Epoch 49/200
243/243 [==============================] - 0s 939us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.3016e-04
Epoch 50/200
243/243 [==============================] - 0s 912us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.3016e-04
Epoch 51/200
173/243 [====================>.........] - ETA: 0s - loss: 0.0925 - mae: 0.3021 - mse: 0.0924
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00031508164829574525.
243/243 [==============================] - 0s 886us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 6.3016e-04
Epoch 52/200
243/243 [==============================] - 0s 891us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.1508e-04
Epoch 53/200
243/243 [==============================] - 0s 977us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.1508e-04
Epoch 54/200
243/243 [==============================] - 0s 964us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.1508e-04
Epoch 55/200
243/243 [==============================] - 0s 977us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.1508e-04
Epoch 56/200
243/243 [==============================] - 0s 897us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.1508e-04
Epoch 57/200
243/243 [==============================] - 0s 900us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.1508e-04
Epoch 58/200
243/243 [==============================] - 0s 881us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.1508e-04
Epoch 59/200
243/243 [==============================] - 0s 853us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.1508e-04
Epoch 60/200
243/243 [==============================] - 0s 934us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.1508e-04
Epoch 61/200
171/243 [====================>.........] - ETA: 0s - loss: 0.0925 - mae: 0.3022 - mse: 0.0925
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.00015754082414787263.
243/243 [==============================] - 0s 839us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.1508e-04
Epoch 62/200
243/243 [==============================] - 0s 895us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.5754e-04
Epoch 63/200
243/243 [==============================] - 0s 891us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.5754e-04
Epoch 64/200
243/243 [==============================] - 0s 880us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.5754e-04
Epoch 65/200
243/243 [==============================] - 0s 835us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.5754e-04
Epoch 66/200
243/243 [==============================] - 0s 834us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.5754e-04
Epoch 67/200
243/243 [==============================] - 0s 903us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.5754e-04
Epoch 68/200
243/243 [==============================] - 0s 892us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.5754e-04
Epoch 69/200
243/243 [==============================] - 0s 892us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.5754e-04
Epoch 70/200
243/243 [==============================] - 0s 956us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.5754e-04
Epoch 71/200
194/243 [======================>.......] - ETA: 0s - loss: 0.0926 - mae: 0.3023 - mse: 0.0925
Epoch 71: ReduceLROnPlateau reducing learning rate to 7.877041207393631e-05.
243/243 [==============================] - 0s 849us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.5754e-04
Epoch 72/200
243/243 [==============================] - 0s 874us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.8770e-05
Epoch 73/200
243/243 [==============================] - 0s 985us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.8770e-05
Epoch 74/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.8770e-05
Epoch 75/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.8770e-05
Epoch 76/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.8770e-05
Epoch 77/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.8770e-05
Epoch 78/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.8770e-05
Epoch 79/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.8770e-05
Epoch 80/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.8770e-05
Epoch 81/200
183/243 [=====================>........] - ETA: 0s - loss: 0.0926 - mae: 0.3024 - mse: 0.0926
Epoch 81: ReduceLROnPlateau reducing learning rate to 3.938520603696816e-05.
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 7.8770e-05
Epoch 82/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.9385e-05
Epoch 83/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.9385e-05
Epoch 84/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.9385e-05
Epoch 85/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.9385e-05
Epoch 86/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.9385e-05
Epoch 87/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.9385e-05
Epoch 88/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.9385e-05
Epoch 89/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.9385e-05
Epoch 90/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.9385e-05
Epoch 91/200
160/243 [==================>...........] - ETA: 0s - loss: 0.0929 - mae: 0.3028 - mse: 0.0929
Epoch 91: ReduceLROnPlateau reducing learning rate to 1.969260301848408e-05.
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 3.9385e-05
Epoch 92/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.9693e-05
Epoch 93/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.9693e-05
Epoch 94/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.9693e-05
Epoch 95/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.9693e-05
Epoch 96/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.9693e-05
Epoch 97/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.9693e-05
Epoch 98/200
243/243 [==============================] - 0s 976us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.9693e-05
Epoch 99/200
243/243 [==============================] - 0s 869us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.9693e-05
Epoch 100/200
243/243 [==============================] - 0s 865us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.9693e-05
Epoch 101/200
186/243 [=====================>........] - ETA: 0s - loss: 0.0927 - mae: 0.3024 - mse: 0.0926
Epoch 101: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 918us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.9693e-05
Epoch 102/200
243/243 [==============================] - 0s 887us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 103/200
243/243 [==============================] - 0s 850us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 104/200
243/243 [==============================] - 0s 892us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 105/200
243/243 [==============================] - 0s 894us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 106/200
243/243 [==============================] - 0s 842us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 107/200
243/243 [==============================] - 0s 893us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 108/200
243/243 [==============================] - 0s 941us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 109/200
243/243 [==============================] - 0s 841us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 110/200
243/243 [==============================] - 0s 897us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 111/200
243/243 [==============================] - 0s 887us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 112/200
243/243 [==============================] - 0s 917us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 113/200
243/243 [==============================] - 0s 868us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 114/200
243/243 [==============================] - 0s 896us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 115/200
243/243 [==============================] - 0s 913us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 116/200
243/243 [==============================] - 0s 890us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 117/200
243/243 [==============================] - 0s 892us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 118/200
243/243 [==============================] - 0s 913us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 119/200
243/243 [==============================] - 0s 943us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 120/200
243/243 [==============================] - 0s 913us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 121/200
243/243 [==============================] - 0s 909us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 122/200
243/243 [==============================] - 0s 873us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 123/200
243/243 [==============================] - 0s 899us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 124/200
243/243 [==============================] - 0s 888us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 125/200
243/243 [==============================] - 0s 990us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 126/200
243/243 [==============================] - 0s 915us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 127/200
243/243 [==============================] - 0s 854us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 128/200
243/243 [==============================] - 0s 863us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 129/200
243/243 [==============================] - 0s 897us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 130/200
243/243 [==============================] - 0s 911us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 131/200
243/243 [==============================] - 0s 893us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 132/200
243/243 [==============================] - 0s 897us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 133/200
243/243 [==============================] - 0s 887us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 134/200
243/243 [==============================] - 0s 869us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 135/200
243/243 [==============================] - 0s 916us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 136/200
243/243 [==============================] - 0s 914us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 137/200
243/243 [==============================] - 0s 893us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 138/200
243/243 [==============================] - 0s 892us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 139/200
243/243 [==============================] - 0s 891us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 140/200
243/243 [==============================] - 0s 888us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 141/200
243/243 [==============================] - 0s 829us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 142/200
243/243 [==============================] - 0s 848us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 143/200
243/243 [==============================] - 0s 892us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 144/200
243/243 [==============================] - 0s 892us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 145/200
243/243 [==============================] - 0s 892us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 146/200
243/243 [==============================] - 0s 896us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 147/200
243/243 [==============================] - 0s 893us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 148/200
243/243 [==============================] - 0s 913us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 149/200
243/243 [==============================] - 0s 884us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 150/200
243/243 [==============================] - 0s 900us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 151/200
243/243 [==============================] - 0s 894us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 152/200
243/243 [==============================] - 0s 891us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 153/200
243/243 [==============================] - 0s 911us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 154/200
243/243 [==============================] - 0s 897us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 155/200
243/243 [==============================] - 0s 895us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 156/200
243/243 [==============================] - 0s 898us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 157/200
243/243 [==============================] - 0s 882us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 158/200
243/243 [==============================] - 0s 828us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 159/200
243/243 [==============================] - 0s 827us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 160/200
243/243 [==============================] - 0s 959us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 161/200
243/243 [==============================] - 0s 827us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 162/200
243/243 [==============================] - 0s 941us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 163/200
243/243 [==============================] - 0s 904us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 164/200
243/243 [==============================] - 0s 899us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 165/200
243/243 [==============================] - 0s 888us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 166/200
243/243 [==============================] - 0s 833us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 167/200
243/243 [==============================] - 0s 844us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 168/200
243/243 [==============================] - 0s 891us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 169/200
243/243 [==============================] - 0s 960us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 170/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 171/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 172/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 173/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 174/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 175/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 176/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 177/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 178/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 179/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 180/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 181/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 182/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 183/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 184/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 185/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 186/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 187/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 188/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 189/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 190/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 191/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 192/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 193/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 194/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 195/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 196/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 197/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 198/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 199/200
243/243 [==============================] - 0s 1ms/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
Epoch 200/200
243/243 [==============================] - 0s 957us/step - loss: 0.0927 - mae: 0.3025 - mse: 0.0927 - val_loss: 0.0931 - val_mae: 0.3032 - val_mse: 0.0931 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.01008261251683028LR_[38]HN_16BS_10P_val_mseM_200epochs/model_6.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
243/243 [==============================] - 1s 1ms/step - loss: 0.7380 - mae: 0.3525 - mse: 0.1282 - val_loss: 0.5977 - val_mae: 0.3532 - val_mse: 0.1260 - lr: 0.0101
Epoch 2/200
243/243 [==============================] - 0s 907us/step - loss: 0.4985 - mae: 0.3525 - mse: 0.1255 - val_loss: 0.4144 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0101
Epoch 3/200
243/243 [==============================] - 0s 909us/step - loss: 0.3536 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.3023 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0101
Epoch 4/200
243/243 [==============================] - 0s 873us/step - loss: 0.2650 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.2338 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0101
Epoch 5/200
243/243 [==============================] - 0s 893us/step - loss: 0.2108 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1919 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0101
Epoch 6/200
243/243 [==============================] - 0s 889us/step - loss: 0.1777 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1663 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0101
Epoch 7/200
243/243 [==============================] - 0s 893us/step - loss: 0.1574 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1507 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0101
Epoch 8/200
243/243 [==============================] - 0s 921us/step - loss: 0.1450 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1411 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0101
Epoch 9/200
243/243 [==============================] - 0s 897us/step - loss: 0.1374 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1352 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0101
Epoch 10/200
243/243 [==============================] - 0s 883us/step - loss: 0.1328 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1316 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0101
Epoch 11/200
183/243 [=====================>........] - ETA: 0s - loss: 0.1303 - mae: 0.3527 - mse: 0.1255
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.005041306372731924.
243/243 [==============================] - 0s 891us/step - loss: 0.1299 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1294 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0101
Epoch 12/200
243/243 [==============================] - 0s 893us/step - loss: 0.1285 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1286 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0050
Epoch 13/200
243/243 [==============================] - 0s 938us/step - loss: 0.1279 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1281 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0050
Epoch 14/200
243/243 [==============================] - 0s 867us/step - loss: 0.1273 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1276 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0050
Epoch 15/200
243/243 [==============================] - 0s 891us/step - loss: 0.1269 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1272 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0050
Epoch 16/200
243/243 [==============================] - 0s 891us/step - loss: 0.1266 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1269 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0050
Epoch 17/200
243/243 [==============================] - 0s 897us/step - loss: 0.1264 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1267 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0050
Epoch 18/200
243/243 [==============================] - 0s 891us/step - loss: 0.1262 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1266 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0050
Epoch 19/200
243/243 [==============================] - 0s 890us/step - loss: 0.1260 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1264 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0050
Epoch 20/200
243/243 [==============================] - 0s 913us/step - loss: 0.1259 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1263 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0050
Epoch 21/200
187/243 [======================>.......] - ETA: 0s - loss: 0.1261 - mae: 0.3529 - mse: 0.1257
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.002520653186365962.
243/243 [==============================] - 0s 891us/step - loss: 0.1258 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1262 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0050
Epoch 22/200
243/243 [==============================] - 0s 892us/step - loss: 0.1257 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1262 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0025
Epoch 23/200
243/243 [==============================] - 0s 955us/step - loss: 0.1257 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1262 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0025
Epoch 24/200
243/243 [==============================] - 0s 866us/step - loss: 0.1257 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0025
Epoch 25/200
243/243 [==============================] - 0s 942us/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0025
Epoch 26/200
243/243 [==============================] - 0s 900us/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0025
Epoch 27/200
243/243 [==============================] - 0s 884us/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1261 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0025
Epoch 28/200
243/243 [==============================] - 0s 831us/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0025
Epoch 29/200
243/243 [==============================] - 0s 863us/step - loss: 0.1256 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0025
Epoch 30/200
243/243 [==============================] - 0s 899us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0025
Epoch 31/200
162/243 [===================>..........] - ETA: 0s - loss: 0.1256 - mae: 0.3525 - mse: 0.1254
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.001260326593182981.
243/243 [==============================] - 0s 934us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0025
Epoch 32/200
243/243 [==============================] - 0s 840us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0013
Epoch 33/200
243/243 [==============================] - 0s 955us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0013
Epoch 34/200
243/243 [==============================] - 0s 879us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0013
Epoch 35/200
243/243 [==============================] - 0s 838us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0013
Epoch 36/200
243/243 [==============================] - 0s 894us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0013
Epoch 37/200
243/243 [==============================] - 0s 914us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0013
Epoch 38/200
243/243 [==============================] - 0s 894us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0013
Epoch 39/200
243/243 [==============================] - 0s 891us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0013
Epoch 40/200
243/243 [==============================] - 0s 890us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0013
Epoch 41/200
235/243 [============================>.] - ETA: 0s - loss: 0.1254 - mae: 0.3525 - mse: 0.1254
Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0006301632965914905.
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 0.0013
Epoch 42/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.3016e-04
Epoch 43/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.3016e-04
Epoch 44/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.3016e-04
Epoch 45/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.3016e-04
Epoch 46/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.3016e-04
Epoch 47/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.3016e-04
Epoch 48/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.3016e-04
Epoch 49/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.3016e-04
Epoch 50/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.3016e-04
Epoch 51/200
228/243 [===========================>..] - ETA: 0s - loss: 0.1255 - mae: 0.3526 - mse: 0.1255
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00031508164829574525.
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 6.3016e-04
Epoch 52/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.1508e-04
Epoch 53/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1260 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.1508e-04
Epoch 54/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.1508e-04
Epoch 55/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.1508e-04
Epoch 56/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.1508e-04
Epoch 57/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.1508e-04
Epoch 58/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.1508e-04
Epoch 59/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.1508e-04
Epoch 60/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.1508e-04
Epoch 61/200
243/243 [==============================] - ETA: 0s - loss: 0.1255 - mae: 0.3525 - mse: 0.1254
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.00015754082414787263.
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.1508e-04
Epoch 62/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.5754e-04
Epoch 63/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.5754e-04
Epoch 64/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.5754e-04
Epoch 65/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.5754e-04
Epoch 66/200
243/243 [==============================] - 0s 914us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.5754e-04
Epoch 67/200
243/243 [==============================] - 0s 861us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.5754e-04
Epoch 68/200
243/243 [==============================] - 0s 897us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.5754e-04
Epoch 69/200
243/243 [==============================] - 0s 827us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.5754e-04
Epoch 70/200
243/243 [==============================] - 0s 886us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.5754e-04
Epoch 71/200
172/243 [====================>.........] - ETA: 0s - loss: 0.1253 - mae: 0.3524 - mse: 0.1253
Epoch 71: ReduceLROnPlateau reducing learning rate to 7.877041207393631e-05.
243/243 [==============================] - 0s 917us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.5754e-04
Epoch 72/200
243/243 [==============================] - 0s 889us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.8770e-05
Epoch 73/200
243/243 [==============================] - 0s 892us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.8770e-05
Epoch 74/200
243/243 [==============================] - 0s 893us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.8770e-05
Epoch 75/200
243/243 [==============================] - 0s 891us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.8770e-05
Epoch 76/200
243/243 [==============================] - 0s 891us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.8770e-05
Epoch 77/200
243/243 [==============================] - 0s 917us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.8770e-05
Epoch 78/200
243/243 [==============================] - 0s 892us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.8770e-05
Epoch 79/200
243/243 [==============================] - 0s 893us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.8770e-05
Epoch 80/200
243/243 [==============================] - 0s 950us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.8770e-05
Epoch 81/200
191/243 [======================>.......] - ETA: 0s - loss: 0.1254 - mae: 0.3525 - mse: 0.1254
Epoch 81: ReduceLROnPlateau reducing learning rate to 3.938520603696816e-05.
243/243 [==============================] - 0s 889us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 7.8770e-05
Epoch 82/200
243/243 [==============================] - 0s 899us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.9385e-05
Epoch 83/200
243/243 [==============================] - 0s 880us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.9385e-05
Epoch 84/200
243/243 [==============================] - 0s 942us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.9385e-05
Epoch 85/200
243/243 [==============================] - 0s 893us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.9385e-05
Epoch 86/200
243/243 [==============================] - 0s 892us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.9385e-05
Epoch 87/200
243/243 [==============================] - 0s 892us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.9385e-05
Epoch 88/200
243/243 [==============================] - 0s 901us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.9385e-05
Epoch 89/200
243/243 [==============================] - 0s 884us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.9385e-05
Epoch 90/200
243/243 [==============================] - 0s 891us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.9385e-05
Epoch 91/200
174/243 [====================>.........] - ETA: 0s - loss: 0.1256 - mae: 0.3527 - mse: 0.1255
Epoch 91: ReduceLROnPlateau reducing learning rate to 1.969260301848408e-05.
243/243 [==============================] - 0s 924us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 3.9385e-05
Epoch 92/200
243/243 [==============================] - 0s 884us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.9693e-05
Epoch 93/200
243/243 [==============================] - 0s 892us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.9693e-05
Epoch 94/200
243/243 [==============================] - 0s 893us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.9693e-05
Epoch 95/200
243/243 [==============================] - 0s 892us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.9693e-05
Epoch 96/200
243/243 [==============================] - 0s 915us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.9693e-05
Epoch 97/200
243/243 [==============================] - 0s 891us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.9693e-05
Epoch 98/200
243/243 [==============================] - 0s 891us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.9693e-05
Epoch 99/200
243/243 [==============================] - 0s 827us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.9693e-05
Epoch 100/200
243/243 [==============================] - 0s 891us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.9693e-05
Epoch 101/200
188/243 [======================>.......] - ETA: 0s - loss: 0.1255 - mae: 0.3526 - mse: 0.1255
Epoch 101: ReduceLROnPlateau reducing learning rate to 1e-05.
243/243 [==============================] - 0s 888us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.9693e-05
Epoch 102/200
243/243 [==============================] - 0s 915us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 103/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 104/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 105/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 106/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 107/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 108/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 109/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 110/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 111/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 112/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 113/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 114/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 115/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 116/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 117/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 118/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 119/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 120/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 121/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 122/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 123/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 124/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 125/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 126/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 127/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 128/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 129/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 130/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 131/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 132/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 133/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 134/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 135/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 136/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 137/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 138/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 139/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 140/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 141/200
243/243 [==============================] - 0s 822us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 142/200
243/243 [==============================] - 0s 832us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 143/200
243/243 [==============================] - 0s 888us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 144/200
243/243 [==============================] - 0s 847us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 145/200
243/243 [==============================] - 0s 923us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 146/200
243/243 [==============================] - 0s 876us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 147/200
243/243 [==============================] - 0s 868us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 148/200
243/243 [==============================] - 0s 896us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 149/200
243/243 [==============================] - 0s 886us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 150/200
243/243 [==============================] - 0s 917us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 151/200
243/243 [==============================] - 0s 922us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 152/200
243/243 [==============================] - 0s 888us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 153/200
243/243 [==============================] - 0s 860us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 154/200
243/243 [==============================] - 0s 889us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 155/200
243/243 [==============================] - 0s 894us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 156/200
243/243 [==============================] - 0s 826us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 157/200
243/243 [==============================] - 0s 853us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 158/200
243/243 [==============================] - 0s 915us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 159/200
243/243 [==============================] - 0s 872us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 160/200
243/243 [==============================] - 0s 870us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 161/200
243/243 [==============================] - 0s 856us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 162/200
243/243 [==============================] - 0s 917us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 163/200
243/243 [==============================] - 0s 863us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 164/200
243/243 [==============================] - 0s 894us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 165/200
243/243 [==============================] - 0s 828us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 166/200
243/243 [==============================] - 0s 922us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 167/200
243/243 [==============================] - 0s 839us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 168/200
243/243 [==============================] - 0s 903us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 169/200
243/243 [==============================] - 0s 893us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 170/200
243/243 [==============================] - 0s 899us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 171/200
243/243 [==============================] - 0s 898us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 172/200
243/243 [==============================] - 0s 887us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 173/200
243/243 [==============================] - 0s 893us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 174/200
243/243 [==============================] - 0s 853us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 175/200
243/243 [==============================] - 0s 867us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 176/200
243/243 [==============================] - 0s 897us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 177/200
243/243 [==============================] - 0s 909us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 178/200
243/243 [==============================] - 0s 891us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 179/200
243/243 [==============================] - 0s 892us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 180/200
243/243 [==============================] - 0s 892us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 181/200
243/243 [==============================] - 0s 897us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 182/200
243/243 [==============================] - 0s 888us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 183/200
243/243 [==============================] - 0s 914us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 184/200
243/243 [==============================] - 0s 828us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 185/200
243/243 [==============================] - 0s 960us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 186/200
243/243 [==============================] - 0s 842us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 187/200
243/243 [==============================] - 0s 890us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 188/200
243/243 [==============================] - 0s 875us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 189/200
243/243 [==============================] - 0s 851us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 190/200
243/243 [==============================] - 0s 918us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 191/200
243/243 [==============================] - 0s 907us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 192/200
243/243 [==============================] - 0s 906us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 193/200
243/243 [==============================] - 0s 873us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 194/200
243/243 [==============================] - 0s 884us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 195/200
243/243 [==============================] - 0s 895us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 196/200
243/243 [==============================] - 0s 886us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 197/200
243/243 [==============================] - 0s 828us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 198/200
243/243 [==============================] - 0s 851us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 199/200
243/243 [==============================] - 0s 909us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
Epoch 200/200
243/243 [==============================] - 0s 932us/step - loss: 0.1255 - mae: 0.3525 - mse: 0.1254 - val_loss: 0.1259 - val_mae: 0.3532 - val_mse: 0.1259 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__sgd_0.01008261251683028LR_[38]HN_16BS_10P_val_mseM_200epochs/model_7.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/200
243/243 [==============================] - 1s 1ms/step - loss: 0.7952 - mae: 0.4025 - mse: 0.1639 - val_loss: 0.6520 - val_mae: 0.4032 - val_mse: 0.1638 - lr: 0.0101
Epoch 2/200
243/243 [==============================] - 0s 966us/step - loss: 0.5493 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.4622 - val_mae: 0.4032 - val_mse: 0.1638 - lr: 0.0101
Epoch 3/200
243/243 [==============================] - 0s 896us/step - loss: 0.3992 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.3462 - val_mae: 0.4032 - val_mse: 0.1638 - lr: 0.0101
Epoch 4/200
243/243 [==============================] - 0s 953us/step - loss: 0.3075 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.2753 - val_mae: 0.4032 - val_mse: 0.1638 - lr: 0.0101
Epoch 5/200
243/243 [==============================] - 0s 921us/step - loss: 0.2514 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.2320 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0101
Epoch 6/200
243/243 [==============================] - 0s 948us/step - loss: 0.2172 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.2055 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0101
Epoch 7/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1962 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1893 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0101
Epoch 8/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1834 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1794 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0101
Epoch 9/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1756 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1733 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0101
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.005041306372731924.
243/243 [==============================] - 0s 1ms/step - loss: 0.1678 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1673 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0101
Epoch 12/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1664 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1665 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0050
Epoch 13/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1657 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1659 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0050
Epoch 14/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1651 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1655 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0050
Epoch 15/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1647 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1651 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0050
Epoch 16/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1644 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1648 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0050
Epoch 17/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1641 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1646 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0050
Epoch 18/200
243/243 [==============================] - 0s 1ms/step - loss: 0.1639 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1644 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0050
Epoch 19/200
233/243 [===========================>..] - ETA: 0s - loss: 0.1638 - mae: 0.4025 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632
205/243 [========================>.....] - ETA: 0s - loss: 0.1680 - mae: 0.4026 - mse: 0.1632