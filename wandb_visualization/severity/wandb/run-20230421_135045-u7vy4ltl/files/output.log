Epoch 1/150
121/243 [=============>................] - ETA: 0s - loss: 0.0446 - mae: 0.0591 - mse: 0.0094

227/243 [===========================>..] - ETA: 0s - loss: 0.0250 - mae: 0.0493 - mse: 0.0060
243/243 [==============================] - 5s 15ms/step - loss: 0.0235 - mae: 0.0485 - mse: 0.0057 - val_loss: 0.0024 - val_mae: 0.0391 - val_mse: 0.0020 - lr: 0.0298
Epoch 2/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0025 - mae: 0.0381 - mse: 0.0021 - val_loss: 0.0024 - val_mae: 0.0418 - val_mse: 0.0022 - lr: 0.0298
Epoch 3/150
238/243 [============================>.] - ETA: 0s - loss: 0.0024 - mae: 0.0378 - mse: 0.0021

243/243 [==============================] - 3s 13ms/step - loss: 0.0024 - mae: 0.0379 - mse: 0.0021 - val_loss: 0.0022 - val_mae: 0.0395 - val_mse: 0.0020 - lr: 0.0298
Epoch 4/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0374 - mse: 0.0020 - val_loss: 0.0030 - val_mae: 0.0370 - val_mse: 0.0026 - lr: 0.0298
Epoch 5/150
224/243 [==========================>...] - ETA: 0s - loss: 0.0022 - mae: 0.0368 - mse: 0.0020
243/243 [==============================] - 3s 13ms/step - loss: 0.0022 - mae: 0.0368 - mse: 0.0020 - val_loss: 0.0021 - val_mae: 0.0358 - val_mse: 0.0019 - lr: 0.0298
Epoch 6/150
229/243 [===========================>..] - ETA: 0s - loss: 0.0021 - mae: 0.0366 - mse: 0.0019
243/243 [==============================] - 3s 13ms/step - loss: 0.0021 - mae: 0.0365 - mse: 0.0019 - val_loss: 0.0020 - val_mae: 0.0358 - val_mse: 0.0018 - lr: 0.0298
Epoch 7/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0359 - mse: 0.0019 - val_loss: 0.0021 - val_mae: 0.0343 - val_mse: 0.0019 - lr: 0.0298
Epoch 8/150
240/243 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0356 - mse: 0.0018
243/243 [==============================] - 3s 14ms/step - loss: 0.0019 - mae: 0.0356 - mse: 0.0018 - val_loss: 0.0019 - val_mae: 0.0368 - val_mse: 0.0018 - lr: 0.0298
Epoch 9/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0352 - mse: 0.0018 - val_loss: 0.0024 - val_mae: 0.0432 - val_mse: 0.0024 - lr: 0.0298
Epoch 10/150
243/243 [==============================] - ETA: 0s - loss: 0.0019 - mae: 0.0349 - mse: 0.0017
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_135045-u7vy4ltl\files\model-best)... Done. 0.0s
243/243 [==============================] - 3s 13ms/step - loss: 0.0019 - mae: 0.0349 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0346 - val_mse: 0.0017 - lr: 0.0298
Epoch 11/150
243/243 [==============================] - 3s 13ms/step - loss: 0.0018 - mae: 0.0348 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0361 - val_mse: 0.0017 - lr: 0.0298
Epoch 12/150
242/243 [============================>.] - ETA: 0s - loss: 0.0018 - mae: 0.0347 - mse: 0.0017
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_135045-u7vy4ltl\files\model-best)... Done. 0.0s
243/243 [==============================] - 3s 14ms/step - loss: 0.0018 - mae: 0.0347 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0348 - val_mse: 0.0017 - lr: 0.0298
Epoch 13/150
243/243 [==============================] - 4s 15ms/step - loss: 0.0018 - mae: 0.0345 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0354 - val_mse: 0.0017 - lr: 0.0298
Epoch 14/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0347 - mse: 0.0017 - val_loss: 0.0019 - val_mae: 0.0341 - val_mse: 0.0017 - lr: 0.0298
Epoch 15/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0345 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0341 - val_mse: 0.0017 - lr: 0.0298
Epoch 16/150
242/243 [============================>.] - ETA: 0s - loss: 0.0018 - mae: 0.0344 - mse: 0.0017
243/243 [==============================] - 3s 14ms/step - loss: 0.0018 - mae: 0.0344 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0343 - val_mse: 0.0017 - lr: 0.0298
Epoch 17/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0345 - mse: 0.0017 - val_loss: 0.0020 - val_mae: 0.0337 - val_mse: 0.0018 - lr: 0.0298
Epoch 18/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0342 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0353 - val_mse: 0.0017 - lr: 0.0298
Epoch 19/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0343 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0338 - val_mse: 0.0021 - lr: 0.0298
Epoch 20/150
236/243 [============================>.] - ETA: 0s - loss: 0.0018 - mae: 0.0341 - mse: 0.0017
243/243 [==============================] - 3s 13ms/step - loss: 0.0018 - mae: 0.0341 - mse: 0.0017 - val_loss: 0.0017 - val_mae: 0.0356 - val_mse: 0.0017 - lr: 0.0298
Epoch 21/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0341 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0353 - val_mse: 0.0017 - lr: 0.0298
Epoch 22/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0339 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0361 - val_mse: 0.0017 - lr: 0.0298
Epoch 23/150
231/243 [===========================>..] - ETA: 0s - loss: 0.0018 - mae: 0.0340 - mse: 0.0017
Epoch 23: ReduceLROnPlateau reducing learning rate to 0.014897994697093964.
243/243 [==============================] - 1s 4ms/step - loss: 0.0018 - mae: 0.0340 - mse: 0.0017 - val_loss: 0.0018 - val_mae: 0.0338 - val_mse: 0.0017 - lr: 0.0298
Epoch 24/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0338 - mse: 0.0016 - val_loss: 0.0019 - val_mae: 0.0334 - val_mse: 0.0018 - lr: 0.0149
Epoch 25/150

239/243 [============================>.] - ETA: 0s - loss: 0.0017 - mae: 0.0338 - mse: 0.0016
243/243 [==============================] - 3s 13ms/step - loss: 0.0017 - mae: 0.0338 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0339 - val_mse: 0.0017 - lr: 0.0149
Epoch 26/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0338 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0353 - val_mse: 0.0017 - lr: 0.0149
Epoch 27/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0337 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0335 - val_mse: 0.0017 - lr: 0.0149
Epoch 28/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0337 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0355 - val_mse: 0.0017 - lr: 0.0149
Epoch 29/150
241/243 [============================>.] - ETA: 0s - loss: 0.0017 - mae: 0.0336 - mse: 0.0016
243/243 [==============================] - 3s 13ms/step - loss: 0.0017 - mae: 0.0336 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0343 - val_mse: 0.0017 - lr: 0.0149
Epoch 30/150
243/243 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0336 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0347 - val_mse: 0.0017 - lr: 0.0149
Epoch 31/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0336 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0017 - lr: 0.0149
Epoch 32/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0335 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0017 - lr: 0.0149
Epoch 33/150
230/243 [===========================>..] - ETA: 0s - loss: 0.0017 - mae: 0.0337 - mse: 0.0016
Epoch 33: ReduceLROnPlateau reducing learning rate to 0.007448997348546982.
243/243 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0335 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0017 - lr: 0.0149
Epoch 34/150
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0017 - lr: 0.0074
Epoch 35/150
243/243 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0334 - mse: 0.0016
243/243 [==============================] - 3s 13ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0342 - val_mse: 0.0016 - lr: 0.0074
Epoch 36/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0335 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0017 - lr: 0.0074
Epoch 37/150
238/243 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0334 - mse: 0.0016
243/243 [==============================] - 3s 13ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0342 - val_mse: 0.0016 - lr: 0.0074
Epoch 38/150
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0334 - val_mse: 0.0017 - lr: 0.0074
Epoch 39/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 0.0074
Epoch 40/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0018 - val_mae: 0.0332 - val_mse: 0.0017 - lr: 0.0074
Epoch 41/150
233/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0334 - mse: 0.0016
243/243 [==============================] - 3s 13ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0341 - val_mse: 0.0016 - lr: 0.0074
Epoch 42/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0334 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0334 - val_mse: 0.0017 - lr: 0.0074
Epoch 43/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.0074
Epoch 44/150

229/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0333 - mse: 0.0016
243/243 [==============================] - 3s 13ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 0.0074
Epoch 45/150
227/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0333 - mse: 0.0016
Epoch 45: ReduceLROnPlateau reducing learning rate to 0.003724498674273491.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_135045-u7vy4ltl\files\model-best)... Done. 0.0s
243/243 [==============================] - 3s 13ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0342 - val_mse: 0.0016 - lr: 0.0074
Epoch 46/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 0.0037
Epoch 47/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.0037
Epoch 48/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0332 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0348 - val_mse: 0.0017 - lr: 0.0037
Epoch 49/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0332 - val_mse: 0.0017 - lr: 0.0037
Epoch 50/150
242/243 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.0016
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0333 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0332 - val_mse: 0.0017 - lr: 0.0037
243/243 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.0016        al_loss: 0.0017 - val_mae: 0.0332 - val_mse: 0.0017 - lr: 0.0037
243/243 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.0016        al_loss: 0.0017 - val_mae: 0.0332 - val_mse: 0.0017 - lr: 0.0037
234/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.0016        val_loss: 0.0017 - val_mae: 0.0342 - val_mse: 0.0016 - lr: 0.0037
234/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.0016        val_loss: 0.0017 - val_mae: 0.0342 - val_mse: 0.0016 - lr: 0.0037
231/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.0016.0016 - val_loss: 0.0017 - val_mae: 0.0343 - val_mse: 0.0016 - lr: 0.0037
Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0018622493371367455.
231/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.0016.0016 - val_loss: 0.0017 - val_mae: 0.0343 - val_mse: 0.0016 - lr: 0.0037
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 0.00197
Epoch 57/150
232/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.0016
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 0.00197
226/243 [==========================>...] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.00160016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 0.00197
Epoch 59/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.0019
Epoch 60/150
226/243 [==========================>...] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.00160016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 0.00197
243/243 [==============================] - 3s 13ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 0.0019
Epoch 61/150
237/243 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.0016
243/243 [==============================] - 3s 13ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 0.0019
180/243 [=====================>........] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.00160016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.00199
Epoch 63/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0334 - val_mse: 0.0016 - lr: 0.0019
Epoch 64/150
234/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.00150016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.00199
234/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.00150016 - val_loss: 0.0017 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 0.00199
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 9.3112e-04
Epoch 66/150
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0342 - val_mse: 0.0016 - lr: 9.3112e-04
Epoch 67/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 9.3112e-04
Epoch 68/150
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 9.3112e-04
Epoch 69/150
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0346 - val_mse: 0.0016 - lr: 9.3112e-04
Epoch 70/150
 85/243 [=========>....................] - ETA: 0s - loss: 0.0016 - mae: 0.0339 - mse: 0.0016
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 9.3112e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0016 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 9.3112e-04
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0017 - val_mae: 0.0344 - val_mse: 0.0016 - lr: 9.3112e-04
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0331 - mse: 0.0015 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 9.3112e-04
231/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.00160015 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 9.3112e-04
231/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.00160015 - val_loss: 0.0017 - val_mae: 0.0335 - val_mse: 0.0016 - lr: 9.3112e-04
235/243 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0329 - mse: 0.0015.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 4.6556e-04
235/243 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0329 - mse: 0.0015.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 4.6556e-04
243/243 [==============================] - 4s 17ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 4.6556e-04
243/243 [==============================] - 4s 17ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 4.6556e-04
232/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.0015.0015 - val_loss: 0.0016 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 4.6556e-04
232/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.0015.0015 - val_loss: 0.0016 - val_mae: 0.0340 - val_mse: 0.0016 - lr: 4.6556e-04
160/243 [==================>...........] - ETA: 0s - loss: 0.0016 - mae: 0.0333 - mse: 0.0016.0015 - val_loss: 0.0016 - val_mae: 0.0339 - val_mse: 0.0016 - lr: 4.6556e-04
 66/243 [=======>......................] - ETA: 0s - loss: 0.0016 - mae: 0.0326 - mse: 0.0015.0015 - val_loss: 0.0016 - val_mae: 0.0339 - val_mse: 0.0016 - lr: 4.6556e-04
234/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.0015.0015 - val_loss: 0.0016 - val_mae: 0.0339 - val_mse: 0.0016 - lr: 4.6556e-04
234/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.0015.0015 - val_loss: 0.0016 - val_mae: 0.0339 - val_mse: 0.0016 - lr: 4.6556e-04
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_135045-u7vy4ltl\files\model-best)... Done. 0.0s
243/243 [==============================] - 4s 15ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 2.3278e-04
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 2.3278e-044
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 2.3278e-044
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 2.3278e-044
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 2.3278e-044
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 2.3278e-044
242/243 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.00150015 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 2.3278e-044
242/243 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.00150015 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 2.3278e-044
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_135045-u7vy4ltl\files\model-best)... Done. 0.0s
243/243 [==============================] - 3s 13ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.1639e-04
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 1.1639e-044
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 1.1639e-044
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0017 - val_mae: 0.0336 - val_mse: 0.0016 - lr: 1.1639e-044
128/243 [==============>...............] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.00160015 - val_loss: 0.0016 - val_mae: 0.0339 - val_mse: 0.0016 - lr: 1.1639e-044
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 5.8195e-054
226/243 [==========================>...] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.00160015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 5.8195e-054
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 5.8195e-054
237/243 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0329 - mse: 0.00150015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 5.8195e-054
237/243 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0329 - mse: 0.00150015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 5.8195e-054
 84/243 [=========>....................] - ETA: 0s - loss: 0.0016 - mae: 0.0333 - mse: 0.0016.0015 - val_loss: 0.0016 - val_mae: 0.0339 - val_mse: 0.0016 - lr: 5.8195e-05
 49/243 [=====>........................] - ETA: 0s - loss: 0.0016 - mae: 0.0331 - mse: 0.0016.0015 - val_loss: 0.0016 - val_mae: 0.0339 - val_mse: 0.0016 - lr: 5.8195e-05
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 2.9098e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 2.9098e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 2.9098e-055
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\severity\wandb\run-20230421_135045-u7vy4ltl\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 2.9098e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 2.9098e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 2.9098e-055
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 1.4549e-055
100/243 [===========>..................] - ETA: 0s - loss: 0.0015 - mae: 0.0327 - mse: 0.00150015 - val_loss: 0.0016 - val_mae: 0.0337 - val_mse: 0.0016 - lr: 1.4549e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.4549e-055
233/243 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.00150015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.4549e-055
220/243 [==========================>...] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.00150015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.4549e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
243/243 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
105/243 [===========>..................] - ETA: 0s - loss: 0.0016 - mae: 0.0332 - mse: 0.00160015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
238/243 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0330 - mse: 0.00150015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
243/243 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 4/150'loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 4/150'loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 4/150'loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 8/150'loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 8/150'loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 8/150'loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 12/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 12/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 15/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 15/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 18/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 18/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 18/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 18/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 23/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 24/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 26/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 26/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 29/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 29/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 29/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 29/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 34/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 34/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 37/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 37/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 37/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 40/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 40/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 40/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 45/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 45/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 48/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 48/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 50/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 50/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 50/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 55/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 55/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 58/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 58/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 60/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 60/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 64/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 64/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 64/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 68/150loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])ae: 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0009311246685683727. 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0009311246685683727. 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0009311246685683727. 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 74/150duceLROnPlateau reducing learning rate to 0.0009311246685683727. 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 74/150duceLROnPlateau reducing learning rate to 0.0009311246685683727. 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 77/150duceLROnPlateau reducing learning rate to 0.0009311246685683727. 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 77/150duceLROnPlateau reducing learning rate to 0.0009311246685683727. 0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 79: ReduceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 79: ReduceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 79: ReduceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 84/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 84/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 84/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 88/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 88/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 90/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 92/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 92/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 92/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 92/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 97/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 97/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 99: ReduceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 99: ReduceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 99: ReduceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 104/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 104/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 104/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 108/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 108/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 110/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 110/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 110/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 115/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 115/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 115/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 119/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 120/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 122/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 122/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 125/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 125/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 125/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 125/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.0.0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 129: ReduceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 129: ReduceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 133/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 133/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 136/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 136/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 136/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 136/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 140/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 140/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 144/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 144/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 147/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 147/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 147/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 147/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 147/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 147/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 147/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0057s). Check your callbacks.
Epoch 147/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 5/15050duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 7/15050duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 9/15050duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 9/15050duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 12/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 14/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 16/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 17/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 19/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 21/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 23/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 23/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 26/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 27/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 30/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 32/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 34/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 36/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 37/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 39/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 41/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 41/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 44/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 46/1500duceLROnPlateau reducing learning rate to 1.4548822946380824e-05..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 50/150duceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 52/150duceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 54/150duceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 54/150duceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 57/150duceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 58/150duceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 61/150duceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 61/150duceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 64/150duceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 66/150duceLROnPlateau reducing learning rate to 0.0018622493371367455.5..0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 67: ReduceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 70/150duceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 72/150duceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 72/150duceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 75/150duceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 77/150duceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 78/150duceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 81/150duceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 81/150duceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 84/150duceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 86/150duceLROnPlateau reducing learning rate to 0.00046556233428418636...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 87: ReduceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 87: ReduceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 91/150duceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 93/150duceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 95/150duceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 97/150duceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 98/150duceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 98/150duceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 102/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 104/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 104/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 107/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 108/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 111/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 113/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 115/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 117/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 118/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 120/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 122/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 124/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 124/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 127/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 128/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 131/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 133/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 133/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 136/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 138/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 140/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 142/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 144/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 144/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 147/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 149/150uceLROnPlateau reducing learning rate to 0.00011639058357104659...0330 - mse: 0.0015 - val_loss: 0.0016 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 3/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 5/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 7/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 7/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 10/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 12/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 14/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 16/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 17/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 19/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 21/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 23/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 25/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 27/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 29/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 31/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 32/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 34/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 36/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 38/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 38/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 41/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 42/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 45/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 47/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 47/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 50/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 54/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 56/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 58/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 60/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0009311246685683727._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0009311246685683727._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 65/150duceLROnPlateau reducing learning rate to 0.0009311246685683727._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 67/150duceLROnPlateau reducing learning rate to 0.0009311246685683727._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 69/150duceLROnPlateau reducing learning rate to 0.0009311246685683727._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 69/150duceLROnPlateau reducing learning rate to 0.0009311246685683727._16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 71: ReduceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 74/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 76/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 78/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 80/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 80/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 82/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 85/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 87/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 89/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 91/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 92/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 92/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 96/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 96/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 101: ReduceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 101: ReduceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 104/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 107/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 109/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 111/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 112/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 114/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 116/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 118/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 120/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 121: ReduceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 121: ReduceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 125/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 127/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 127/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 130/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 131: ReduceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 131: ReduceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 135/150duceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 135/150duceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 138/150duceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 140/150duceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 142/150duceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 144/150duceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 144/150duceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 147/150duceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 149/150duceLROnPlateau reducing learning rate to 1e-05.822946380824e-05.6BS_10P_val_lossM_150epochs/model_3.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 3/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 5/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 7/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 9/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 11/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 13/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 13/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 16/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 18/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 20/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 22/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 22/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 25/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 26/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 29/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 31/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 31/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 34/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 35: ReduceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 38/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 40/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 42/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 44/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 44/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 46/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 49/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 51/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 53/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 55/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 56/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 58/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 60/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 62/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 62/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 65/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 66/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 69/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 71/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 73/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 75/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 75/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 78/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 80/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 82/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 83/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 86/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 86/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 89/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 91/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 92: ReduceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 95/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 95/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 98/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 100/150uceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 102/150uceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 103/150uceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 103/150uceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 107/150uceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 109/150uceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 111/150uceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 112: ReduceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 115/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 117/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 119/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 119/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 122/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 123/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 126/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 126/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 129/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 131/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 132: ReduceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 132: ReduceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 136/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 138/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 140/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 142/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 143/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 146/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 148/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 148/150duceLROnPlateau reducing learning rate to 1.4548822946380824e-05.6BS_10P_val_lossM_150epochs/model_4.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0001s vs `on_train_batch_end` time: 0.0030s). Check your callbacks.
Epoch 3/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 5/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 7/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 9/150rained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 11/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 12/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 14/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 16/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 18/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 20/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 22/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 24/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 26/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 26/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 29/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 31/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 33/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 35/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 37/150ained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 38: ReduceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 38: ReduceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 42/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 44/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 46/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 48/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 49/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 51/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 53/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 55/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 57/150duceLROnPlateau reducing learning rate to 0.007448997348546982.N_16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 62/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 64/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 66/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 68/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 69/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 72/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 74/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 76/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 76/150duceLROnPlateau reducing learning rate to 0.0018622493371367455._16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 78: ReduceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 81/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 83/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 83/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 86/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 88/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 89/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 89/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 93/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 95/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 97/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 98: ReduceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 101/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 103/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 105/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 105/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 108/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 109/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 109/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 113/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 115/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 115/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 118/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 119/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 122/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 124/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 126/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 128/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 129/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 131/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 133/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 135/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 135/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 138/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 139/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 139/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 143/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 145/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 147/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 149/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 149/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 149/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 149/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 149/150uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.
Epoch 4/15050uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 6/15050uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 6/15050uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 9/15050uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 11/1500uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 12/1500uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 15/1500uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 15/1500uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 18/1500uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 20/1500uceLROnPlateau reducing learning rate to 0.00011639058357104659.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 24/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 26/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 28/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 28/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 31/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 33/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 35/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 37/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 39/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 41/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 43/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 43/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 45/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 48/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 50/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 52/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 52/150duceLROnPlateau reducing learning rate to 0.007448997348546982.9.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 57/150duceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 59/150duceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 61/150duceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 61/150duceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 64/150duceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 65/150duceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 68/150duceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 70/150duceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 72/150duceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 72/150duceLROnPlateau reducing learning rate to 0.0018622493371367455..16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 74: ReduceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 77/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 79/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 81/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 83/150duceLROnPlateau reducing learning rate to 0.00046556233428418636.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 84: ReduceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 87/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 87/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 90/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 92/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 94/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 95/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 97/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 99/150duceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 101/150uceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 103/150uceLROnPlateau reducing learning rate to 0.00023278116714209318.16BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 104: ReduceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 107/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 115/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 121/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 124/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 129/150duceLROnPlateau reducing learning rate to 5.8195291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 134: ReduceLROnPlateau reducing learning rate to 1e-05.291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 143/150duceLROnPlateau reducing learning rate to 1e-05.291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 1/15050duceLROnPlateau reducing learning rate to 1e-05.291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 7/15050duceLROnPlateau reducing learning rate to 1e-05.291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 7/15050duceLROnPlateau reducing learning rate to 1e-05.291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 18/1500duceLROnPlateau reducing learning rate to 1e-05.291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 26/1500duceLROnPlateau reducing learning rate to 1e-05.291785523295e-05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
Epoch 31: ReduceLROnPlateau reducing learning rate to 0.003724498674273491.05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
243/243 [==============================] - 0s 935us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1638 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0074
Epoch 32/150
243/243 [==============================] - 0s 1ms/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0037
Epoch 33/150
225/243 [==========================>...] - ETA: 0s - loss: 0.1632 - mae: 0.4025 - mse: 0.1631
Epoch 37/150duceLROnPlateau reducing learning rate to 0.003724498674273491.05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
243/243 [==============================] - 0s 913us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1632 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1637 - lr: 0.0037
Epoch 35/150
243/243 [==============================] - 0s 890us/step - loss: 0.1632 - mae: 0.4025 - mse: 0.1631 - val_loss: 0.1637 - val_mae: 0.4032 - val_mse: 0.1636 - lr: 0.0037
Epoch 36/150
243/243 [==============================] - 0s 892us/step - loss: 0.1631 - mae: 0.4025 - mse: 0.1630 - val_loss: 0.1636 - val_mae: 0.4032 - val_mse: 0.1635 - lr: 0.0037
Epoch 37/150duceLROnPlateau reducing learning rate to 0.003724498674273491.05.6BS_10P_val_lossM_150epochs/model_5.h5 - val_mae: 0.0338 - val_mse: 0.0016 - lr: 1.0000e-055
243/243 [==============================] - 0s 888us/step - loss: 0.1630 - mae: 0.4025 - mse: 0.1629 - val_loss: 0.1636 - val_mae: 0.4032 - val_mse: 0.1635 - lr: 0.0037
Epoch 38/150
243/243 [==============================] - 0s 916us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1629 - val_loss: 0.1635 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 39/150
243/243 [==============================] - 0s 891us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1629 - val_loss: 0.1635 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 40/150
243/243 [==============================] - 0s 898us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1629 - val_loss: 0.1635 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 41/150
243/243 [==============================] - 0s 952us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1635 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 42/150
243/243 [==============================] - 0s 890us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 43/150
Epoch 52/150===========================] - 0s 916us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 44/150
243/243 [==============================] - 0s 890us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1635 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 45/150
243/243 [==============================] - 0s 892us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 46/150
243/243 [==============================] - 0s 922us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 47/150
243/243 [==============================] - 0s 929us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1635 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 48/150
169/243 [===================>..........] - ETA: 0s - loss: 0.1632 - mae: 0.4029 - mse: 0.1631
Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0018622493371367455.
243/243 [==============================] - 0s 850us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
Epoch 49/150
243/243 [==============================] - 0s 892us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 50/150
243/243 [==============================] - 0s 893us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 51/150
243/243 [==============================] - 0s 893us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 52/150===========================] - 0s 916us/step - loss: 0.1629 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0037
243/243 [==============================] - 0s 957us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 53/150
243/243 [==============================] - 0s 916us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 54/150
243/243 [==============================] - 0s 893us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 55/150
243/243 [==============================] - 0s 890us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 56/150
243/243 [==============================] - 0s 902us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 57/150
243/243 [==============================] - 0s 907us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 58/150
243/243 [==============================] - 0s 957us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 59/150
243/243 [==============================] - 0s 899us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 60/150
243/243 [==============================] - 0s 954us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 0.0019
Epoch 61/150
185/243 [=====================>........] - ETA: 0s - loss: 0.1627 - mae: 0.4024 - mse: 0.1627
  1/243 [..............................] - ETA: 0s - loss: 0.1619 - mae: 0.4014 - mse: 0.16190.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1634 - lr: 0.0019
Epoch 63/150
243/243 [==============================] - 0s 904us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 0.0019
Epoch 64/150
170/243 [===================>..........] - ETA: 0s - loss: 0.1626 - mae: 0.4023 - mse: 0.1626
Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0009311246685683727.
243/243 [==============================] - 0s 891us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 0.0019
Epoch 65/150
243/243 [==============================] - 0s 890us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 9.3112e-04
Epoch 66/150
243/243 [==============================] - 0s 903us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 9.3112e-04
Epoch 67/150
243/243 [==============================] - 0s 969us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 9.3112e-04
Epoch 68/150
243/243 [==============================] - 0s 895us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 9.3112e-04
Epoch 69/150
243/243 [==============================] - 0s 890us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 9.3112e-04
Epoch 70/150
243/243 [==============================] - 0s 892us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 9.3112e-04
Epoch 71/150
 72/243 [=======>......................] - ETA: 0s - loss: 0.1631 - mae: 0.4029 - mse: 0.16310.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 9.3112e-04
Epoch 72/150
243/243 [==============================] - 0s 954us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 9.3112e-04
Epoch 73/150
243/243 [==============================] - 0s 847us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 9.3112e-04
Epoch 74/150
190/243 [======================>.......] - ETA: 0s - loss: 0.1628 - mae: 0.4025 - mse: 0.1628
Epoch 74: ReduceLROnPlateau reducing learning rate to 0.00046556233428418636.
243/243 [==============================] - 0s 893us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 9.3112e-04
Epoch 75/150
243/243 [==============================] - 0s 923us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 4.6556e-04
Epoch 76/150
243/243 [==============================] - 0s 925us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 4.6556e-04
Epoch 77/150
243/243 [==============================] - 0s 920us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 4.6556e-04
Epoch 78/150
243/243 [==============================] - 0s 892us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 4.6556e-04
Epoch 79/150
243/243 [==============================] - 0s 957us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 4.6556e-04
Epoch 80/150
144/243 [================>.............] - ETA: 0s - loss: 0.1628 - mae: 0.4026 - mse: 0.16280.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 4.6556e-04
Epoch 81/150
243/243 [==============================] - 0s 954us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 4.6556e-04
Epoch 82/150
243/243 [==============================] - 0s 892us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 4.6556e-04
Epoch 83/150
243/243 [==============================] - 0s 891us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 4.6556e-04
Epoch 84/150
187/243 [======================>.......] - ETA: 0s - loss: 0.1630 - mae: 0.4028 - mse: 0.1629
Epoch 84: ReduceLROnPlateau reducing learning rate to 0.00023278116714209318.
243/243 [==============================] - 0s 892us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 4.6556e-04
Epoch 85/150
243/243 [==============================] - 0s 897us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.3278e-04
Epoch 86/150
243/243 [==============================] - 0s 848us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.3278e-04
Epoch 87/150
243/243 [==============================] - 0s 893us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.3278e-04
Epoch 88/150
243/243 [==============================] - 0s 890us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.3278e-04
Epoch 89/150
182/243 [=====================>........] - ETA: 0s - loss: 0.1631 - mae: 0.4029 - mse: 0.16300.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.3278e-04
Epoch 90/150
243/243 [==============================] - 0s 894us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.3278e-04
Epoch 91/150
243/243 [==============================] - 0s 996us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.3278e-04
Epoch 92/150
243/243 [==============================] - 0s 908us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.3278e-04
Epoch 93/150
243/243 [==============================] - 0s 908us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.3278e-04
Epoch 94/150
164/243 [===================>..........] - ETA: 0s - loss: 0.1630 - mae: 0.4028 - mse: 0.1629
Epoch 94: ReduceLROnPlateau reducing learning rate to 0.00011639058357104659.
243/243 [==============================] - 0s 1ms/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.3278e-04
Epoch 95/150
243/243 [==============================] - 0s 885us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.1639e-04
Epoch 96/150
243/243 [==============================] - 0s 901us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.1639e-04
Epoch 97/150
243/243 [==============================] - 0s 910us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.1639e-04
Epoch 98/150
186/243 [=====================>........] - ETA: 0s - loss: 0.1625 - mae: 0.4022 - mse: 0.16250.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.1639e-04
Epoch 99/150
243/243 [==============================] - 0s 954us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.1639e-04
Epoch 100/150
243/243 [==============================] - 0s 906us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.1639e-04
Epoch 101/150
243/243 [==============================] - 0s 900us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.1639e-04
Epoch 102/150
243/243 [==============================] - 0s 890us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.1639e-04
Epoch 103/150
243/243 [==============================] - 0s 909us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.1639e-04
Epoch 104/150
184/243 [=====================>........] - ETA: 0s - loss: 0.1627 - mae: 0.4025 - mse: 0.1627
Epoch 104: ReduceLROnPlateau reducing learning rate to 5.8195291785523295e-05.
243/243 [==============================] - 0s 939us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.1639e-04
Epoch 105/150
243/243 [==============================] - 0s 915us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 5.8195e-05
Epoch 106/150
243/243 [==============================] - 0s 892us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 5.8195e-05
Epoch 107/150
243/243 [==============================] - 0s 892us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.9098e-05
Epoch 108/150
243/243 [==============================] - 0s 885us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 5.8195e-05
Epoch 109/150
243/243 [==============================] - 0s 888us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 5.8195e-05
Epoch 110/150
243/243 [==============================] - 0s 916us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 5.8195e-05
Epoch 111/150
243/243 [==============================] - 0s 892us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 5.8195e-05
Epoch 112/150
243/243 [==============================] - 0s 910us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 5.8195e-05
Epoch 113/150
243/243 [==============================] - 0s 936us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 5.8195e-05
Epoch 114/150
161/243 [==================>...........] - ETA: 0s - loss: 0.1630 - mae: 0.4028 - mse: 0.1630
Epoch 114: ReduceLROnPlateau reducing learning rate to 2.9097645892761648e-05.
243/243 [==============================] - 0s 918us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 5.8195e-05
Epoch 115/150
243/243 [==============================] - 0s 890us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.9098e-05
Epoch 116/150
243/243 [==============================] - 0s 892us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.9098e-05
Epoch 117/150
243/243 [==============================] - 0s 890us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.9098e-05
Epoch 118/150
243/243 [==============================] - 0s 963us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.9098e-05
Epoch 119/150
243/243 [==============================] - 0s 896us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.9098e-05
Epoch 120/150
243/243 [==============================] - 0s 914us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.9098e-05
Epoch 121/150
243/243 [==============================] - 0s 891us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.9098e-05
Epoch 122/150
243/243 [==============================] - 0s 891us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.9098e-05
Epoch 123/150
243/243 [==============================] - 0s 966us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 2.9098e-05
Epoch 124/150
  1/243 [..............................] - ETA: 0s - loss: 0.1556 - mae: 0.3938 - mse: 0.1556
243/243 [==============================] - 0s 891us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.4549e-05
Epoch 128/150
243/243 [==============================] - 0s 960us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.4549e-05
Epoch 129/150
243/243 [==============================] - 0s 890us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.4549e-05
Epoch 130/150
243/243 [==============================] - 0s 893us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.4549e-05
Epoch 131/150
243/243 [==============================] - 0s 890us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.4549e-05
Epoch 132/150
243/243 [==============================] - 0s 899us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.4549e-05
Epoch 133/150
 83/243 [=========>....................] - ETA: 0s - loss: 0.1631 - mae: 0.4029 - mse: 0.1631
243/243 [==============================] - 0s 893us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.0000e-05
Epoch 138/150
243/243 [==============================] - 0s 889us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.0000e-05
Epoch 139/150
243/243 [==============================] - 0s 982us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.0000e-05
Epoch 140/150
243/243 [==============================] - 0s 892us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.0000e-05
Epoch 141/150
243/243 [==============================] - 0s 901us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.0000e-05
Epoch 142/150
164/243 [===================>..........] - ETA: 0s - loss: 0.1632 - mae: 0.4031 - mse: 0.1632
243/243 [==============================] - 0s 852us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.0000e-05
Epoch 149/150
243/243 [==============================] - 0s 890us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.0000e-05
Epoch 150/150
243/243 [==============================] - 0s 900us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.0000e-05
>Saved ../trained_models/severity/models__adam_0.029795988706281577LR_[21]HN_16BS_10P_val_lossM_150epochs/model_8.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
243/243 [==============================] - 0s 852us/step - loss: 0.1628 - mae: 0.4025 - mse: 0.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.0000e-05
 89/243 [=========>....................] - ETA: 0s - loss: 0.2062 - mae: 0.4527 - mse: 0.20610.1628 - val_loss: 0.1634 - val_mae: 0.4032 - val_mse: 0.1633 - lr: 1.0000e-05
243/243 [==============================] - 0s 893us/step - loss: 0.2060 - mae: 0.4525 - mse: 0.2060 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0298e-05
243/243 [==============================] - 0s 887us/step - loss: 0.2060 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0149e-05
243/243 [==============================] - 0s 892us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 0.0074e-05
243/243 [==============================] - 0s 891us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0037e-05
243/243 [==============================] - 0s 852us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 0.0019e-05
243/243 [==============================] - 0s 852us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2066 - val_mae: 0.4532 - val_mse: 0.2066 - lr: 9.3112e-04
243/243 [==============================] - 0s 915us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 4.6556e-04
243/243 [==============================] - 0s 954us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 2.3278e-04
243/243 [==============================] - 0s 893us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.1639e-04
243/243 [==============================] - 0s 915us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 5.8195e-05
173/243 [====================>.........] - ETA: 0s - loss: 0.2061 - mae: 0.4527 - mse: 0.20610.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 5.8195e-05
  1/243 [..............................] - ETA: 0s - loss: 0.1898 - mae: 0.4341 - mse: 0.18980.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 2.9098e-05
243/243 [==============================] - 0s 887us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.4549e-05
243/243 [==============================] - 0s 930us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
243/243 [==============================] - 0s 890us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
243/243 [==============================] - 0s 930us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05
243/243 [==============================] - 0s 930us/step - loss: 0.2059 - mae: 0.4525 - mse: 0.2059 - val_loss: 0.2065 - val_mae: 0.4532 - val_mse: 0.2065 - lr: 1.0000e-05