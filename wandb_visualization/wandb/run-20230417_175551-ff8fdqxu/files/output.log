wandb: WARNING The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.
Epoch 1/20
225/243 [==========================>...] - ETA: 0s - loss: 1.1110 - mae: 0.1592 - mse: 0.0383
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230417_175551-ff8fdqxu\files\model-best)... Done. 0.0s
243/243 [==============================] - 3s 9ms/step - loss: 1.0880 - mae: 0.1518 - mse: 0.0358 - val_loss: 0.7763 - val_mae: 0.0553 - val_mse: 0.0046 - lr: 0.0122
Epoch 2/20
222/243 [==========================>...] - ETA: 0s - loss: 0.6006 - mae: 0.0479 - mse: 0.0034
243/243 [==============================] - 2s 8ms/step - loss: 0.5871 - mae: 0.0475 - mse: 0.0034 - val_loss: 0.4304 - val_mae: 0.0442 - val_mse: 0.0028 - lr: 0.0122
Epoch 3/20
224/243 [==========================>...] - ETA: 0s - loss: 0.3329 - mae: 0.0431 - mse: 0.0027
243/243 [==============================] - 2s 8ms/step - loss: 0.3262 - mae: 0.0433 - mse: 0.0027 - val_loss: 0.2398 - val_mae: 0.0427 - val_mse: 0.0026 - lr: 0.0122
Epoch 4/20
227/243 [===========================>..] - ETA: 0s - loss: 0.1853 - mae: 0.0421 - mse: 0.0025
243/243 [==============================] - 2s 7ms/step - loss: 0.1823 - mae: 0.0423 - mse: 0.0026 - val_loss: 0.1345 - val_mae: 0.0425 - val_mse: 0.0026 - lr: 0.0122
Epoch 5/20
222/243 [==========================>...] - ETA: 0s - loss: 0.1050 - mae: 0.0421 - mse: 0.0025
243/243 [==============================] - 2s 6ms/step - loss: 0.1027 - mae: 0.0420 - mse: 0.0025 - val_loss: 0.0763 - val_mae: 0.0421 - val_mse: 0.0025 - lr: 0.0122
Epoch 6/20
219/243 [==========================>...] - ETA: 0s - loss: 0.0602 - mae: 0.0417 - mse: 0.0025
243/243 [==============================] - 2s 6ms/step - loss: 0.0587 - mae: 0.0416 - mse: 0.0025 - val_loss: 0.0441 - val_mae: 0.0417 - val_mse: 0.0025 - lr: 0.0122
Epoch 7/20
236/243 [============================>.] - ETA: 0s - loss: 0.0345 - mae: 0.0412 - mse: 0.0024
243/243 [==============================] - 1s 5ms/step - loss: 0.0343 - mae: 0.0413 - mse: 0.0024 - val_loss: 0.0262 - val_mae: 0.0413 - val_mse: 0.0024 - lr: 0.0122
Epoch 8/20
237/243 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0409 - mse: 0.0024
243/243 [==============================] - 2s 7ms/step - loss: 0.0208 - mae: 0.0409 - mse: 0.0024 - val_loss: 0.0162 - val_mae: 0.0410 - val_mse: 0.0023 - lr: 0.0122
Epoch 9/20
238/243 [============================>.] - ETA: 0s - loss: 0.0132 - mae: 0.0405 - mse: 0.0023
243/243 [==============================] - 2s 6ms/step - loss: 0.0132 - mae: 0.0405 - mse: 0.0023 - val_loss: 0.0106 - val_mae: 0.0407 - val_mse: 0.0023 - lr: 0.0122
Epoch 10/20
208/243 [========================>.....] - ETA: 0s - loss: 0.0091 - mae: 0.0402 - mse: 0.0023
243/243 [==============================] - 2s 7ms/step - loss: 0.0089 - mae: 0.0403 - mse: 0.0023 - val_loss: 0.0074 - val_mae: 0.0404 - val_mse: 0.0023 - lr: 0.0122
Epoch 11/20
223/243 [==========================>...] - ETA: 0s - loss: 0.0065 - mae: 0.0400 - mse: 0.0022
243/243 [==============================] - 2s 6ms/step - loss: 0.0064 - mae: 0.0399 - mse: 0.0022 - val_loss: 0.0056 - val_mae: 0.0402 - val_mse: 0.0022 - lr: 0.0122
Epoch 12/20
205/243 [========================>.....] - ETA: 0s - loss: 0.0051 - mae: 0.0399 - mse: 0.0022
243/243 [==============================] - 1s 6ms/step - loss: 0.0050 - mae: 0.0398 - mse: 0.0022 - val_loss: 0.0045 - val_mae: 0.0400 - val_mse: 0.0022 - lr: 0.0122
Epoch 13/20
224/243 [==========================>...] - ETA: 0s - loss: 0.0042 - mae: 0.0397 - mse: 0.0022
243/243 [==============================] - 1s 6ms/step - loss: 0.0042 - mae: 0.0395 - mse: 0.0022 - val_loss: 0.0039 - val_mae: 0.0398 - val_mse: 0.0022 - lr: 0.0122
Epoch 14/20
229/243 [===========================>..] - ETA: 0s - loss: 0.0037 - mae: 0.0395 - mse: 0.0021
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.006087197456508875.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230417_175551-ff8fdqxu\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 6ms/step - loss: 0.0037 - mae: 0.0394 - mse: 0.0021 - val_loss: 0.0035 - val_mae: 0.0396 - val_mse: 0.0021 - lr: 0.0122
Epoch 15/20
243/243 [==============================] - 1s 6ms/step - loss: 0.0034 - mae: 0.0392 - mse: 0.0021 - val_loss: 0.0033 - val_mae: 0.0396 - val_mse: 0.0021 - lr: 0.0061
Epoch 16/20
225/243 [==========================>...] - ETA: 0s - loss: 0.0032 - mae: 0.0390 - mse: 0.0021
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230417_175551-ff8fdqxu\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 6ms/step - loss: 0.0033 - mae: 0.0392 - mse: 0.0021 - val_loss: 0.0032 - val_mae: 0.0395 - val_mse: 0.0021 - lr: 0.0061
Epoch 17/20
228/243 [===========================>..] - ETA: 0s - loss: 0.0032 - mae: 0.0391 - mse: 0.0021
243/243 [==============================] - 1s 6ms/step - loss: 0.0031 - mae: 0.0391 - mse: 0.0021 - val_loss: 0.0031 - val_mae: 0.0395 - val_mse: 0.0021 - lr: 0.0061
Epoch 18/20
199/243 [=======================>......] - ETA: 0s - loss: 0.0030 - mae: 0.0388 - mse: 0.0021
243/243 [==============================] - 2s 7ms/step - loss: 0.0030 - mae: 0.0391 - mse: 0.0021 - val_loss: 0.0030 - val_mae: 0.0394 - val_mse: 0.0021 - lr: 0.0061
Epoch 19/20
224/243 [==========================>...] - ETA: 0s - loss: 0.0030 - mae: 0.0391 - mse: 0.0021
Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0030435987282544374.
wandb: Adding directory to artifact (C:\MyDocuments\Disertatie\segments\wandb_visualization\wandb\run-20230417_175551-ff8fdqxu\files\model-best)... Done. 0.0s
243/243 [==============================] - 1s 6ms/step - loss: 0.0030 - mae: 0.0390 - mse: 0.0021 - val_loss: 0.0030 - val_mae: 0.0394 - val_mse: 0.0021 - lr: 0.0061
Epoch 20/20
205/243 [========================>.....] - ETA: 0s - loss: 0.0029 - mae: 0.0391 - mse: 0.0021
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0021s). Check your callbacks.
243/243 [==============================] - 1s 6ms/step - loss: 0.0029 - mae: 0.0390 - mse: 0.0021 - val_loss: 0.0029 - val_mae: 0.0393 - val_mse: 0.0021 - lr: 0.0030
>Saved ../trained_models/models_segments_overlap-new-network-sigmoid-adam-8IN-20HN_sgd_0.012174394596637751_70_16_20epochs/model_1.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
323/323 [==============================] - 1s 3ms/step - loss: 0.9910 - mae: 0.0930 - mse: 0.0148 - val_loss: 0.6447 - val_mae: 0.0523 - val_mse: 0.0038 - lr: 0.0122
Epoch 2/20
323/323 [==============================] - 1s 3ms/step - loss: 0.4477 - mae: 0.0513 - mse: 0.0036 - val_loss: 0.2950 - val_mae: 0.0510 - val_mse: 0.0035 - lr: 0.0122
Epoch 3/20
323/323 [==============================] - 1s 3ms/step - loss: 0.2056 - mae: 0.0510 - mse: 0.0035 - val_loss: 0.1362 - val_mae: 0.0511 - val_mse: 0.0035 - lr: 0.0122
Epoch 4/20
323/323 [==============================] - 1s 3ms/step - loss: 0.0956 - mae: 0.0509 - mse: 0.0035 - val_loss: 0.0641 - val_mae: 0.0509 - val_mse: 0.0035 - lr: 0.0122
Epoch 5/20
305/323 [===========================>..] - ETA: 0s - loss: 0.0464 - mae: 0.0508 - mse: 0.0035
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.006087197456508875.
323/323 [==============================] - 1s 3ms/step - loss: 0.0456 - mae: 0.0508 - mse: 0.0035 - val_loss: 0.0313 - val_mae: 0.0508 - val_mse: 0.0035 - lr: 0.0122
Epoch 6/20
323/323 [==============================] - 1s 2ms/step - loss: 0.0264 - mae: 0.0508 - mse: 0.0035 - val_loss: 0.0223 - val_mae: 0.0507 - val_mse: 0.0035 - lr: 0.0061
Epoch 7/20
323/323 [==============================] - 1s 3ms/step - loss: 0.0191 - mae: 0.0508 - mse: 0.0035 - val_loss: 0.0163 - val_mae: 0.0506 - val_mse: 0.0035 - lr: 0.0061
Epoch 8/20
304/323 [===========================>..] - ETA: 0s - loss: 0.0142 - mae: 0.0507 - mse: 0.0034
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0030435987282544374.
323/323 [==============================] - 1s 3ms/step - loss: 0.0141 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0122 - val_mae: 0.0506 - val_mse: 0.0035 - lr: 0.0061
Epoch 9/20
323/323 [==============================] - 1s 3ms/step - loss: 0.0114 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0107 - val_mae: 0.0506 - val_mse: 0.0035 - lr: 0.0030
Epoch 10/20
323/323 [==============================] - 1s 3ms/step - loss: 0.0100 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0095 - val_mae: 0.0506 - val_mse: 0.0035 - lr: 0.0030
Epoch 11/20
297/323 [==========================>...] - ETA: 0s - loss: 0.0089 - mae: 0.0505 - mse: 0.0034
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0015217993641272187.
323/323 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0084 - val_mae: 0.0506 - val_mse: 0.0035 - lr: 0.0030
Epoch 12/20
323/323 [==============================] - 1s 3ms/step - loss: 0.0082 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0080 - val_mae: 0.0506 - val_mse: 0.0035 - lr: 0.0015
Epoch 13/20
323/323 [==============================] - 1s 2ms/step - loss: 0.0078 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0076 - val_mae: 0.0506 - val_mse: 0.0034 - lr: 0.0015
Epoch 14/20
302/323 [===========================>..] - ETA: 0s - loss: 0.0074 - mae: 0.0505 - mse: 0.0034
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0007608996820636094.
323/323 [==============================] - 1s 3ms/step - loss: 0.0074 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0072 - val_mae: 0.0506 - val_mse: 0.0034 - lr: 0.0015
Epoch 15/20
323/323 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0070 - val_mae: 0.0506 - val_mse: 0.0034 - lr: 7.6090e-04
Epoch 16/20
323/323 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0069 - val_mae: 0.0506 - val_mse: 0.0034 - lr: 7.6090e-04
Epoch 17/20
302/323 [===========================>..] - ETA: 0s - loss: 0.0068 - mae: 0.0507 - mse: 0.0034
Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0003804498410318047.
323/323 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0067 - val_mae: 0.0506 - val_mse: 0.0034 - lr: 7.6090e-04
Epoch 18/20
323/323 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0067 - val_mae: 0.0506 - val_mse: 0.0034 - lr: 3.8045e-04
Epoch 19/20
323/323 [==============================] - 1s 3ms/step - loss: 0.0066 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0066 - val_mae: 0.0506 - val_mse: 0.0034 - lr: 3.8045e-04
Epoch 20/20
287/323 [=========================>....] - ETA: 0s - loss: 0.0065 - mae: 0.0506 - mse: 0.0034
Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00019022492051590234.
323/323 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0507 - mse: 0.0034 - val_loss: 0.0065 - val_mae: 0.0505 - val_mse: 0.0034 - lr: 3.8045e-04
>Saved ../trained_models/models_segments_overlap-new-network-sigmoid-adam-8IN-20HN_sgd_0.012174394596637751_70_16_20epochs/model_2.h5
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse', 'lr'])
Epoch 1/20
243/243 [==============================] - 1s 2ms/step - loss: 1.1045 - mae: 0.0533 - mse: 0.0048 - val_loss: 0.8061 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0122
Epoch 2/20
243/243 [==============================] - 0s 1ms/step - loss: 0.6091 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.4458 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0122
Epoch 3/20
243/243 [==============================] - 0s 1ms/step - loss: 0.3371 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.2470 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0122
Epoch 4/20
239/243 [============================>.] - ETA: 0s - loss: 0.1878 - mae: 0.0382 - mse: 0.0020
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.006087197456508875.
243/243 [==============================] - 0s 1ms/step - loss: 0.1871 - mae: 0.0381 - mse: 0.0020 - val_loss: 0.1374 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0122
Epoch 5/20
243/243 [==============================] - 1s 2ms/step - loss: 0.1187 - mae: 0.0381 - mse: 0.0020 - val_loss: 0.1022 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0061
Epoch 6/20
243/243 [==============================] - 1s 2ms/step - loss: 0.0888 - mae: 0.0381 - mse: 0.0020 - val_loss: 0.0766 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0061
Epoch 7/20
202/243 [=======================>......] - ETA: 0s - loss: 0.0682 - mae: 0.0378 - mse: 0.0019
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0030435987282544374.
243/243 [==============================] - 0s 2ms/step - loss: 0.0666 - mae: 0.0381 - mse: 0.0019 - val_loss: 0.0576 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0061
Epoch 8/20
243/243 [==============================] - 0s 2ms/step - loss: 0.0535 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0498 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0030
Epoch 9/20
243/243 [==============================] - 0s 1ms/step - loss: 0.0465 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0433 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0030
Epoch 10/20
207/243 [========================>.....] - ETA: 0s - loss: 0.0408 - mae: 0.0382 - mse: 0.0020
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0015217993641272187.
243/243 [==============================] - 0s 2ms/step - loss: 0.0404 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0377 - val_mae: 0.0385 - val_mse: 0.0020 - lr: 0.0030
Epoch 11/20
243/243 [==============================] - 1s 3ms/step - loss: 0.0363 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0351 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0015
Epoch 12/20
243/243 [==============================] - 0s 2ms/step - loss: 0.0339 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0327 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0015
Epoch 13/20
223/243 [==========================>...] - ETA: 0s - loss: 0.0317 - mae: 0.0380 - mse: 0.0019
Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0007608996820636094.
243/243 [==============================] - 1s 2ms/step - loss: 0.0316 - mae: 0.0380 - mse: 0.0019 - val_loss: 0.0306 - val_mae: 0.0384 - val_mse: 0.0020 - lr: 0.0015
Epoch 14/20
124/243 [==============>...............] - ETA: 0s - loss: 0.0302 - mae: 0.0375 - mse: 0.0019
207/243 [========================>.....] - ETA: 0s - loss: 0.0408 - mae: 0.0382 - mse: 0.0020
235/243 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0382 - mse: 0.0019
235/243 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0382 - mse: 0.0019
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0021s). Check your callbacks.
243/243 [==============================] - 0s 1ms/step - loss: 0.0675 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0583 - val_mae: 0.0378 - val_mse: 0.0019 - lr: 0.0061
243/243 [==============================] - 0s 1ms/step - loss: 0.0675 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0583 - val_mae: 0.0378 - val_mse: 0.0019 - lr: 0.0061
243/243 [==============================] - 0s 1ms/step - loss: 0.0675 - mae: 0.0373 - mse: 0.0019 - val_loss: 0.0583 - val_mae: 0.0378 - val_mse: 0.0019 - lr: 0.0061
243/243 [==============================] - 1s 2ms/step - loss: 0.0282 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
243/243 [==============================] - 1s 2ms/step - loss: 0.0282 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 3/20=============================] - 1s 2ms/step - loss: 0.0282 - mae: 0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0015217993641272187. 0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0015217993641272187. 0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00019022492051590234.0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 4/20ReduceLROnPlateau reducing learning rate to 0.00019022492051590234.0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 13/20educeLROnPlateau reducing learning rate to 0.00019022492051590234.0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 13/20educeLROnPlateau reducing learning rate to 0.00019022492051590234.0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 1/200educeLROnPlateau reducing learning rate to 0.00019022492051590234.0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0030435987282544374.4.0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0030435987282544374.4.0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0003804498410318047..0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 3/20ReduceLROnPlateau reducing learning rate to 0.0003804498410318047..0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0015217993641272187..0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0015217993641272187..0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00019022492051590234.0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 5: ReduceLROnPlateau reducing learning rate to 0.006087197456508875.34.0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0007608996820636094..0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04
Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0007608996820636094..0.0375 - mse: 0.0019 - val_loss: 0.0277 - val_mae: 0.0380 - val_mse: 0.0019 - lr: 7.6090e-04